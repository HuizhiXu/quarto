{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`log_softmax`函数是softmax函数的一个变体，它直接计算softmax输出的对数形式。这种形式的softmax函数在深度学习中非常有用，尤其是在涉及到对数似然函数的优化问题时。`log_softmax`函数的数学表达式如下：\n",
    "\n",
    "给定一个实数向量 \\( \\mathbf{x} \\)，其元素为 \\( x_i \\)，`log_softmax`函数计算得到的向量 \\( \\mathbf{y} \\) 的第 \\( i \\) 个元素 \\( y_i \\) 可以通过以下公式得到：\n",
    "\n",
    "$ y_i = x_i - \\log\\left(\\sum_{j=1}^{K} e^{x_j}\\right) $\n",
    "\n",
    "其中，\\( K \\) 是向量 \\( \\mathbf{x} \\) 的维度，即类别的数量，\\( e \\) 是自然对数的底数。\n",
    "\n",
    "这个公式的含义是：对于向量 \\( \\mathbf{x} \\) 中的每个元素 \\( x_i \\)，我们首先计算所有元素的指数和的对数，然后从 \\( x_i \\) 中减去这个值。这样做的结果是，每个 \\( y_i \\) 都表示为 \\( x_i \\) 相对于其他元素 \\( x_j \\) 的对数概率。\n",
    "\n",
    "在PyTorch中，`log_softmax`函数的实现利用了广播和自动求导的特性，可以自动计算出每个元素的对数概率，而不需要显式地计算指数和。这使得计算更加高效，并且避免了数值溢出的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NUMPY\n",
    "\n",
    "根据您提供的前向传播代码，我们可以看到网络的结构是这样的：\n",
    "\n",
    "- 输入 `X` 的形状是 (4, 2)。\n",
    "- 第一层权重 `W1` 的形状是 (2, 2)，偏置 `b1` 的形状是 (2, 1)。\n",
    "- 第二层权重 `W2` 的形状是 (1, 2)，偏置 `b2` 的形状是 (1, 1)。\n",
    "\n",
    "在前向传播过程中，我们首先计算第一层的激活 `A1`，然后计算第二层的激活 `A2`。这里是详细的步骤：\n",
    "\n",
    "1. 计算 `Z1`：将输入 `X` 转置（变为 (2, 4)），然后与权重 `W1` 点积（结果为 (2, 2)），最后加上偏置 `b1`（结果不变，仍然是 (2, 2)），得到 `Z1` 的形状是 (2, 4)。\n",
    "2. 应用激活函数 `sigmoid` 得到 `A1`，形状仍然是 (2, 4)。\n",
    "3. 计算 `Z2`：将 `A1` 与权重 `W2` 点积（结果为 (2, 1) 与 (1, 2) 点积，结果为 (2, 1)），然后加上偏置 `b2`（形状为 (1, 1)），得到 `Z2` 的形状是 (1, 4)。\n",
    "4. 应用激活函数 `sigmoid` 得到 `A2`，形状是 (1, 4)。\n",
    "\n",
    "现在，我们已经了解了网络的结构和每一层的输出形状。接下来，我们需要确保反向传播过程中的计算与这些形状相匹配。特别是在计算 `dA1` 时，我们需要使用 `W2.T` 来考虑 `A2` 到 `A1` 的影响。由于 `W2` 的形状是 (1, 2)，`W2.T` 的形状将是 (2, 1)。然而，由于 `dZ2` 的形状是 (4, 4)，我们需要确保 `W2.T` 能够与 `dZ2` 进行点积。\n",
    "\n",
    "为了解决这个问题，我们需要重新考虑网络结构，特别是权重 `W2` 的设计。如果 `A1` 的形状是 (2, 4) 并且我们需要将这些激活传递到一个形状为 (1, 4) 的 `A2`，那么 `W2` 应该是一个形状为 (4, 1) 的矩阵，以便能够将 (2, 4) 的 `A1` 映射到 (1, 4) 的 `A2`。这样，`W2.T` 将是 (1, 4)，可以与 (4, 4) 的 `dZ2` 进行点积。\n",
    "\n",
    "请检查您的网络设计和权重初始化，确保它们与前向传播和反向传播的计算相匹配。如果需要进一步的帮助，请提供更多的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (2,1) doesn't match the broadcast shape (2,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     88\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m---> 89\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 72\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(X, Y, W1, b1, W2, b2, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     70\u001b[0m loss \u001b[38;5;241m=\u001b[39m compute_loss(A2, Y)\n\u001b[1;32m     71\u001b[0m dW1, db1, dW2, db2 \u001b[38;5;241m=\u001b[39m backward_propagation(X, Y, W1, W2, A1, A2, Z1, Z2)\n\u001b[0;32m---> 72\u001b[0m W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[41], line 61\u001b[0m, in \u001b[0;36mupdate_parameters\u001b[0;34m(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\u001b[0m\n\u001b[1;32m     58\u001b[0m db1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(db1) \n\u001b[1;32m     60\u001b[0m W1 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m dW1\n\u001b[0;32m---> 61\u001b[0m b1 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m db1\n\u001b[1;32m     62\u001b[0m W2 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m dW2\n\u001b[1;32m     63\u001b[0m b2 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m db2\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (2,1) doesn't match the broadcast shape (2,4)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 激活函数及其导数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # 避免数值溢出\n",
    "    return exp_x / exp_x.sum(axis=0, keepdims=True)\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    softmax_x = exp_x / exp_x.sum(axis=0, keepdims=True)\n",
    "    return softmax_x * (1 - softmax_x)\n",
    "\n",
    "# 初始化权重和偏置\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    # 隐藏层权重 W1 的形状是（5, 2），偏置 b1 的形状是（5, 1）\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    # 输出层权重 W2 的形状是（5, 5），偏置 b2 的形状是（5, 1）\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# 前向传播\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(W1, X.T) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = softmax(Z2)  # 使用 softmax 函数\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "# 计算损失（交叉熵损失）\n",
    "def compute_loss(A2, Y):\n",
    "    m = Y.shape[0]\n",
    "    loss = -np.sum(Y * np.log(A2.T)) / m\n",
    "    return loss\n",
    "\n",
    "# 反向传播\n",
    "def backward_propagation(X, Y, W1, W2, A1, A2, Z1, Z2):\n",
    "    m = X.shape[0]\n",
    "    Y = Y.reshape(-1, 4) # 由于 Y 的形状是（4, 5），需要转置为（5, 4）\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "    dA1 = np.dot(W2.T, dZ2) * sigmoid_derivative(Z1)\n",
    "    dW1 = (1/m) * np.dot(dA1, X)\n",
    "    db1 = (1/m) * np.sum(dA1, axis=0, keepdims=True)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# 更新参数\n",
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "\n",
    "\n",
    "\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# 训练模型\n",
    "def train(X, Y, W1, b1, W2, b2, learning_rate, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        Z1, A1, Z2, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "        loss = compute_loss(A2, Y)\n",
    "        dW1, db1, dW2, db2 = backward_propagation(X, Y, W1, W2, A1, A2, Z1, Z2)\n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# 示例数据\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]]) # X的形状是（4,2）\n",
    "Y = np.array([[1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0]]) # Y的形状是（4,5），one-hot 编码\n",
    "\n",
    "# 初始化参数: 输入特征的数量为2，隐藏层神经元数量为2，输出层神经元数量为5\n",
    "n_x = X.shape[1]\n",
    "n_h = 2\n",
    "n_y = 5\n",
    "W1, b1, W2, b2 = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "# 训练模型\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "train(X, Y, W1, b1, W2, b2, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(n_h, n_x) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00041989, -0.00443666],\n",
       "       [-0.0009294 ,  0.00248696]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x2 and 4x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     73\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m---> 74\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 54\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(X, Y, W1, b1, W2, b2, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(X, Y, W1, b1, W2, b2, learning_rate, epochs):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 54\u001b[0m         Z1, A1, Z2, A2 \u001b[38;5;241m=\u001b[39m \u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m         loss \u001b[38;5;241m=\u001b[39m compute_loss(A2, Y)\n\u001b[1;32m     56\u001b[0m         dW1, db1, dW2, db2 \u001b[38;5;241m=\u001b[39m backward_propagation(X, Y, W1, W2, A1, A2, Z1, Z2)\n",
      "Cell \u001b[0;32mIn[42], line 21\u001b[0m, in \u001b[0;36mforward_propagation\u001b[0;34m(X, W1, b1, W2, b2)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_propagation\u001b[39m(X, W1, b1, W2, b2):\n\u001b[0;32m---> 21\u001b[0m     Z1 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1\n\u001b[1;32m     22\u001b[0m     A1 \u001b[38;5;241m=\u001b[39m sigmoid(Z1)\n\u001b[1;32m     23\u001b[0m     Z2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(W2, A1) \u001b[38;5;241m+\u001b[39m b2\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x2 and 4x2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 激活函数及其导数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# 初始化权重和偏置\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    np.random.seed(42)  # 为了可重复性\n",
    "    W1 = torch.randn(n_h, n_x) * 0.01\n",
    "    b1 = torch.zeros(n_h, 1)\n",
    "    W2 = torch.randn(n_y, n_h) * 0.01\n",
    "    b2 = torch.zeros(n_y, 1)\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# 前向传播\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    Z1 = torch.mm(W1, X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = torch.mm(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "# 计算损失\n",
    "def compute_loss(A2, Y):\n",
    "    loss = torch.mean(-torch.sum(Y * torch.log(A2) + (1 - Y) * torch.log(1 - A2), dim=1))\n",
    "    return loss\n",
    "\n",
    "# 反向传播\n",
    "def backward_propagation(X, Y, W1, W2, A1, A2, Z1, Z2):\n",
    "    m = X.shape[0]\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = torch.mm(dZ2, A1.T) / m\n",
    "    db2 = torch.sum(dZ2, dim=0, keepdim=True) / m\n",
    "    dA1 = torch.mm(W2.T, dZ2) * sigmoid_derivative(Z1)\n",
    "    dW1 = torch.mm(dA1, X.T) / m\n",
    "    db1 = torch.sum(dA1, dim=0, keepdim=True) / m\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# 更新参数\n",
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# 训练模型\n",
    "def train(X, Y, W1, b1, W2, b2, learning_rate, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        Z1, A1, Z2, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "        loss = compute_loss(A2, Y)\n",
    "        dW1, db1, dW2, db2 = backward_propagation(X, Y, W1, W2, A1, A2, Z1, Z2)\n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# 示例数据\n",
    "X = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float32)\n",
    "Y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# 初始化参数\n",
    "n_x = X.shape[1]\n",
    "n_h = 2\n",
    "n_y = 1\n",
    "W1, b1, W2, b2 = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "# 训练模型\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "train(X, Y, W1, b1, W2, b2, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.functional \n",
    "在这个改写的版本中，我们使用了以下torch.nn.functional模块的函数：\n",
    "\n",
    "F.binary_cross_entropy：计算二元交叉熵损失，这是二分类问题的常用损失函数。\n",
    "torch.sigmoid：应用Sigmoid激活函数。\n",
    "请注意，backward_propagation函数中的dA1计算使用了Sigmoid函数的导数，这是因为F.binary_cross_entropy返回的梯度是关于输出的，而不是关于输入的。因此，我们需要手动计算关于输入的梯度。\n",
    "\n",
    "使用torch.nn.functional模块可以使代码更加简洁和易于理解，同时也能够利用PyTorch的一些内部优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x2 and 4x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     64\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m---> 65\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[43], line 45\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(X, Y, W1, b1, W2, b2, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(X, Y, W1, b1, W2, b2, learning_rate, epochs):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 45\u001b[0m         A1, A2 \u001b[38;5;241m=\u001b[39m \u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m         loss \u001b[38;5;241m=\u001b[39m compute_loss(A2, Y)\n\u001b[1;32m     47\u001b[0m         dW1, dA1 \u001b[38;5;241m=\u001b[39m backward_propagation(X, Y, A1, A2, W1, W2)\n",
      "Cell \u001b[0;32mIn[43], line 14\u001b[0m, in \u001b[0;36mforward_propagation\u001b[0;34m(X, W1, b1, W2, b2)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_propagation\u001b[39m(X, W1, b1, W2, b2):\n\u001b[0;32m---> 14\u001b[0m     Z1 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1\n\u001b[1;32m     15\u001b[0m     A1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(Z1)\n\u001b[1;32m     16\u001b[0m     Z2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(W2, A1) \u001b[38;5;241m+\u001b[39m b2\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x2 and 4x2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 初始化权重和偏置\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    W1 = torch.randn(n_h, n_x) * 0.01\n",
    "    b1 = torch.zeros(n_h, 1)\n",
    "    W2 = torch.randn(n_y, n_h) * 0.01\n",
    "    b2 = torch.zeros(n_y, 1)\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# 前向传播\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    Z1 = torch.mm(W1, X) + b1\n",
    "    A1 = torch.sigmoid(Z1)\n",
    "    Z2 = torch.mm(W2, A1) + b2\n",
    "    A2 = torch.sigmoid(Z2)\n",
    "    return A1, A2\n",
    "\n",
    "# 计算损失\n",
    "def compute_loss(A2, Y):\n",
    "    loss = F.binary_cross_entropy(A2, Y)\n",
    "    return loss\n",
    "\n",
    "# 反向传播\n",
    "def backward_propagation(X, Y, A1, A2, W1, W2):\n",
    "    m = X.shape[0]\n",
    "    dA2 = A2 - Y\n",
    "    dW2 = torch.mm(dA2, A1.T) / m\n",
    "    dA1 = torch.mm(W2.T, dA2) * (A1 * (1 - A1))\n",
    "    dW1 = torch.mm(dA1, X.T) / m\n",
    "    return dW1, dA1\n",
    "\n",
    "# 更新参数\n",
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# 训练模型\n",
    "def train(X, Y, W1, b1, W2, b2, learning_rate, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        A1, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "        loss = compute_loss(A2, Y)\n",
    "        dW1, dA1 = backward_propagation(X, Y, A1, A2, W1, W2)\n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, dA1, dW2, db1, db2, learning_rate)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# 示例数据\n",
    "X = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float32)\n",
    "Y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# 初始化参数\n",
    "n_x = X.shape[1]\n",
    "n_h = 2\n",
    "n_y = 1\n",
    "W1, b1, W2, b2 = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "# 训练模型\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "train(X, Y, W1, b1, W2, b2, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn module\n",
    "\n",
    "在这个代码中，我们定义了一个名为SimpleNN的类，它继承自torch.nn.Module。在这个类中，我们定义了两个线性层（self.layer1和self.layer2），并在forward方法中实现了前向传播逻辑。我们还定义了一个initialize_model函数来创建模型的实例。\n",
    "\n",
    "train函数中，我们使用了torch.optim.SGD优化器来更新模型的权重。在每次迭代中，我们首先清空梯度，然后执行前向传播，计算损失，执行反向传播，并更新模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.736793041229248\n",
      "Epoch 100, Loss: 0.6935387253761292\n",
      "Epoch 200, Loss: 0.6934777498245239\n",
      "Epoch 300, Loss: 0.6934468746185303\n",
      "Epoch 400, Loss: 0.6934185028076172\n",
      "Epoch 500, Loss: 0.6933924555778503\n",
      "Epoch 600, Loss: 0.6933683156967163\n",
      "Epoch 700, Loss: 0.6933459639549255\n",
      "Epoch 800, Loss: 0.6933250427246094\n",
      "Epoch 900, Loss: 0.6933053731918335\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义神经网络模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.n_h = n_h\n",
    "        self.layer1 = nn.Linear(n_x, n_h)  # 输入层到隐藏层的线性变换\n",
    "        self.layer2 = nn.Linear(n_h, n_y)  # 隐藏层到输出层的线性变换\n",
    "\n",
    "    def forward(self, X):\n",
    "        A1 = torch.sigmoid(self.layer1(X))  # 应用Sigmoid激活函数\n",
    "        A2 = torch.sigmoid(self.layer2(A1))  # 应用Sigmoid激活函数\n",
    "        return A1, A2\n",
    "\n",
    "# 初始化模型\n",
    "def initialize_model(n_x, n_h, n_y):\n",
    "    model = SimpleNN(n_x, n_h, n_y)\n",
    "    return model\n",
    "\n",
    "# 计算损失\n",
    "def compute_loss(A2, Y):\n",
    "    loss = F.binary_cross_entropy(A2, Y)\n",
    "    return loss\n",
    "\n",
    "# 训练模型\n",
    "def train(model, X, Y, learning_rate, epochs):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  # 定义优化器\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()  # 清空之前的梯度\n",
    "        A1, A2 = model(X)  # 前向传播\n",
    "        loss = compute_loss(A2, Y)  # 计算损失\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新参数\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# 示例数据\n",
    "X = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float32)\n",
    "Y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# 初始化模型\n",
    "n_x = X.shape[1]\n",
    "n_h = 2\n",
    "n_y = 1\n",
    "model = initialize_model(n_x, n_h, n_y)\n",
    "\n",
    "# 训练模型\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "train(model, X, Y, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactor using Dataset and DataLoader\n",
    "\n",
    "在这个重构的代码中，我们首先定义了一个SimpleDataset类，它继承自torch.utils.data.Dataset。这个类负责存储数据（X和Y），并在__getitem__方法中返回单个样本，在__len__方法中返回数据集的大小。\n",
    "\n",
    "然后，我们创建了一个DataLoader实例，它将为我们提供批处理的数据。DataLoader可以自动地将数据集分割成小批量，并且可以在每个epoch开始时打乱数据。\n",
    "\n",
    "在train函数中，我们遍历dataloader来获取每个批次的数据，并执行前向传播、损失计算、反向传播和参数更新。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 0.6954602599143982\n",
      "Epoch 1, Batch 0, Loss: 0.7854986190795898\n",
      "Epoch 2, Batch 0, Loss: 0.613932728767395\n",
      "Epoch 3, Batch 0, Loss: 0.6937421560287476\n",
      "Epoch 4, Batch 0, Loss: 0.6267933249473572\n",
      "Epoch 5, Batch 0, Loss: 0.6976216435432434\n",
      "Epoch 6, Batch 0, Loss: 0.7042776346206665\n",
      "Epoch 7, Batch 0, Loss: 0.6854599714279175\n",
      "Epoch 8, Batch 0, Loss: 0.6852806806564331\n",
      "Epoch 9, Batch 0, Loss: 0.6920747756958008\n",
      "Epoch 10, Batch 0, Loss: 0.696416974067688\n",
      "Epoch 11, Batch 0, Loss: 0.6849043369293213\n",
      "Epoch 12, Batch 0, Loss: 0.7288597226142883\n",
      "Epoch 13, Batch 0, Loss: 0.6602154970169067\n",
      "Epoch 14, Batch 0, Loss: 0.7231813669204712\n",
      "Epoch 15, Batch 0, Loss: 0.7223381996154785\n",
      "Epoch 16, Batch 0, Loss: 0.6958688497543335\n",
      "Epoch 17, Batch 0, Loss: 0.6957916617393494\n",
      "Epoch 18, Batch 0, Loss: 0.6957243084907532\n",
      "Epoch 19, Batch 0, Loss: 0.6914406418800354\n",
      "Epoch 20, Batch 0, Loss: 0.7024766206741333\n",
      "Epoch 21, Batch 0, Loss: 0.6913979649543762\n",
      "Epoch 22, Batch 0, Loss: 0.6913831233978271\n",
      "Epoch 23, Batch 0, Loss: 0.7023471593856812\n",
      "Epoch 24, Batch 0, Loss: 0.7077240347862244\n",
      "Epoch 25, Batch 0, Loss: 0.6954671144485474\n",
      "Epoch 26, Batch 0, Loss: 0.7068822383880615\n",
      "Epoch 27, Batch 0, Loss: 0.7073018550872803\n",
      "Epoch 28, Batch 0, Loss: 0.6954517364501953\n",
      "Epoch 29, Batch 0, Loss: 0.6954237222671509\n",
      "Epoch 30, Batch 0, Loss: 0.6953985691070557\n",
      "Epoch 31, Batch 0, Loss: 0.6822764873504639\n",
      "Epoch 32, Batch 0, Loss: 0.7021549940109253\n",
      "Epoch 33, Batch 0, Loss: 0.7012213468551636\n",
      "Epoch 34, Batch 0, Loss: 0.6846083998680115\n",
      "Epoch 35, Batch 0, Loss: 0.7021015286445618\n",
      "Epoch 36, Batch 0, Loss: 0.7020895481109619\n",
      "Epoch 37, Batch 0, Loss: 0.698699951171875\n",
      "Epoch 38, Batch 0, Loss: 0.6952860951423645\n",
      "Epoch 39, Batch 0, Loss: 0.6845589280128479\n",
      "Epoch 40, Batch 0, Loss: 0.687952995300293\n",
      "Epoch 41, Batch 0, Loss: 0.6966115236282349\n",
      "Epoch 42, Batch 0, Loss: 0.7020378112792969\n",
      "Epoch 43, Batch 0, Loss: 0.6952418684959412\n",
      "Epoch 44, Batch 0, Loss: 0.6845870018005371\n",
      "Epoch 45, Batch 0, Loss: 0.695224404335022\n",
      "Epoch 46, Batch 0, Loss: 0.6913843154907227\n",
      "Epoch 47, Batch 0, Loss: 0.7019975185394287\n",
      "Epoch 48, Batch 0, Loss: 0.684605062007904\n",
      "Epoch 49, Batch 0, Loss: 0.6951941847801208\n",
      "Epoch 50, Batch 0, Loss: 0.684613823890686\n",
      "Epoch 51, Batch 0, Loss: 0.6914725303649902\n",
      "Epoch 52, Batch 0, Loss: 0.6951660513877869\n",
      "Epoch 53, Batch 0, Loss: 0.6846396923065186\n",
      "Epoch 54, Batch 0, Loss: 0.6846434473991394\n",
      "Epoch 55, Batch 0, Loss: 0.6951500177383423\n",
      "Epoch 56, Batch 0, Loss: 0.6951448917388916\n",
      "Epoch 57, Batch 0, Loss: 0.6846537590026855\n",
      "Epoch 58, Batch 0, Loss: 0.6930398941040039\n",
      "Epoch 59, Batch 0, Loss: 0.6945128440856934\n",
      "Epoch 60, Batch 0, Loss: 0.6846518516540527\n",
      "Epoch 61, Batch 0, Loss: 0.6846563816070557\n",
      "Epoch 62, Batch 0, Loss: 0.6846609115600586\n",
      "Epoch 63, Batch 0, Loss: 0.6914503574371338\n",
      "Epoch 64, Batch 0, Loss: 0.6846694946289062\n",
      "Epoch 65, Batch 0, Loss: 0.6947363018989563\n",
      "Epoch 66, Batch 0, Loss: 0.6914568543434143\n",
      "Epoch 67, Batch 0, Loss: 0.7019076347351074\n",
      "Epoch 68, Batch 0, Loss: 0.6910163164138794\n",
      "Epoch 69, Batch 0, Loss: 0.6951006054878235\n",
      "Epoch 70, Batch 0, Loss: 0.6847018003463745\n",
      "Epoch 71, Batch 0, Loss: 0.691483736038208\n",
      "Epoch 72, Batch 0, Loss: 0.6950842142105103\n",
      "Epoch 73, Batch 0, Loss: 0.6914930939674377\n",
      "Epoch 74, Batch 0, Loss: 0.6914977431297302\n",
      "Epoch 75, Batch 0, Loss: 0.6915024518966675\n",
      "Epoch 76, Batch 0, Loss: 0.6950629353523254\n",
      "Epoch 77, Batch 0, Loss: 0.6933838129043579\n",
      "Epoch 78, Batch 0, Loss: 0.6847453117370605\n",
      "Epoch 79, Batch 0, Loss: 0.6949400901794434\n",
      "Epoch 80, Batch 0, Loss: 0.6847686767578125\n",
      "Epoch 81, Batch 0, Loss: 0.6903054714202881\n",
      "Epoch 82, Batch 0, Loss: 0.6847595572471619\n",
      "Epoch 83, Batch 0, Loss: 0.6847623586654663\n",
      "Epoch 84, Batch 0, Loss: 0.6945161819458008\n",
      "Epoch 85, Batch 0, Loss: 0.6905168890953064\n",
      "Epoch 86, Batch 0, Loss: 0.6847757697105408\n",
      "Epoch 87, Batch 0, Loss: 0.6915413737297058\n",
      "Epoch 88, Batch 0, Loss: 0.6943447589874268\n",
      "Epoch 89, Batch 0, Loss: 0.6958929300308228\n",
      "Epoch 90, Batch 0, Loss: 0.6892505884170532\n",
      "Epoch 91, Batch 0, Loss: 0.6915556192398071\n",
      "Epoch 92, Batch 0, Loss: 0.6911181211471558\n",
      "Epoch 93, Batch 0, Loss: 0.6950116157531738\n",
      "Epoch 94, Batch 0, Loss: 0.6848057508468628\n",
      "Epoch 95, Batch 0, Loss: 0.6938525438308716\n",
      "Epoch 96, Batch 0, Loss: 0.6911256313323975\n",
      "Epoch 97, Batch 0, Loss: 0.695000171661377\n",
      "Epoch 98, Batch 0, Loss: 0.6927354335784912\n",
      "Epoch 99, Batch 0, Loss: 0.6942257881164551\n",
      "Epoch 100, Batch 0, Loss: 0.6915514469146729\n",
      "Epoch 101, Batch 0, Loss: 0.6953621506690979\n",
      "Epoch 102, Batch 0, Loss: 0.6966531872749329\n",
      "Epoch 103, Batch 0, Loss: 0.6848217248916626\n",
      "Epoch 104, Batch 0, Loss: 0.7017520666122437\n",
      "Epoch 105, Batch 0, Loss: 0.6950119733810425\n",
      "Epoch 106, Batch 0, Loss: 0.6967328190803528\n",
      "Epoch 107, Batch 0, Loss: 0.691565990447998\n",
      "Epoch 108, Batch 0, Loss: 0.6848391890525818\n",
      "Epoch 109, Batch 0, Loss: 0.6971096992492676\n",
      "Epoch 110, Batch 0, Loss: 0.6982674598693848\n",
      "Epoch 111, Batch 0, Loss: 0.684849739074707\n",
      "Epoch 112, Batch 0, Loss: 0.6877837777137756\n",
      "Epoch 113, Batch 0, Loss: 0.6848644614219666\n",
      "Epoch 114, Batch 0, Loss: 0.6949779987335205\n",
      "Epoch 115, Batch 0, Loss: 0.6961306929588318\n",
      "Epoch 116, Batch 0, Loss: 0.6848733425140381\n",
      "Epoch 117, Batch 0, Loss: 0.6895848512649536\n",
      "Epoch 118, Batch 0, Loss: 0.6914836168289185\n",
      "Epoch 119, Batch 0, Loss: 0.6932388544082642\n",
      "Epoch 120, Batch 0, Loss: 0.6916178464889526\n",
      "Epoch 121, Batch 0, Loss: 0.6849286556243896\n",
      "Epoch 122, Batch 0, Loss: 0.6947462558746338\n",
      "Epoch 123, Batch 0, Loss: 0.69029700756073\n",
      "Epoch 124, Batch 0, Loss: 0.6849404573440552\n",
      "Epoch 125, Batch 0, Loss: 0.694915771484375\n",
      "Epoch 126, Batch 0, Loss: 0.6916329860687256\n",
      "Epoch 127, Batch 0, Loss: 0.6920881271362305\n",
      "Epoch 128, Batch 0, Loss: 0.7016031742095947\n",
      "Epoch 129, Batch 0, Loss: 0.6929885149002075\n",
      "Epoch 130, Batch 0, Loss: 0.6919153928756714\n",
      "Epoch 131, Batch 0, Loss: 0.6849555969238281\n",
      "Epoch 132, Batch 0, Loss: 0.6931591033935547\n",
      "Epoch 133, Batch 0, Loss: 0.6948919296264648\n",
      "Epoch 134, Batch 0, Loss: 0.7015620470046997\n",
      "Epoch 135, Batch 0, Loss: 0.6919013261795044\n",
      "Epoch 136, Batch 0, Loss: 0.6934469938278198\n",
      "Epoch 137, Batch 0, Loss: 0.6949014067649841\n",
      "Epoch 138, Batch 0, Loss: 0.6948953866958618\n",
      "Epoch 139, Batch 0, Loss: 0.6849769353866577\n",
      "Epoch 140, Batch 0, Loss: 0.6916565895080566\n",
      "Epoch 141, Batch 0, Loss: 0.6922925710678101\n",
      "Epoch 142, Batch 0, Loss: 0.6925555467605591\n",
      "Epoch 143, Batch 0, Loss: 0.69487464427948\n",
      "Epoch 144, Batch 0, Loss: 0.684998631477356\n",
      "Epoch 145, Batch 0, Loss: 0.6916724443435669\n",
      "Epoch 146, Batch 0, Loss: 0.6850048303604126\n",
      "Epoch 147, Batch 0, Loss: 0.6850079298019409\n",
      "Epoch 148, Batch 0, Loss: 0.6916861534118652\n",
      "Epoch 149, Batch 0, Loss: 0.693457841873169\n",
      "Epoch 150, Batch 0, Loss: 0.6916524171829224\n",
      "Epoch 151, Batch 0, Loss: 0.7015055418014526\n",
      "Epoch 152, Batch 0, Loss: 0.7015023231506348\n",
      "Epoch 153, Batch 0, Loss: 0.6917017698287964\n",
      "Epoch 154, Batch 0, Loss: 0.6948246359825134\n",
      "Epoch 155, Batch 0, Loss: 0.6930160522460938\n",
      "Epoch 156, Batch 0, Loss: 0.6850325465202332\n",
      "Epoch 157, Batch 0, Loss: 0.6850359439849854\n",
      "Epoch 158, Batch 0, Loss: 0.7014918327331543\n",
      "Epoch 159, Batch 0, Loss: 0.6850422620773315\n",
      "Epoch 160, Batch 0, Loss: 0.6925859451293945\n",
      "Epoch 161, Batch 0, Loss: 0.6850628852844238\n",
      "Epoch 162, Batch 0, Loss: 0.6947910189628601\n",
      "Epoch 163, Batch 0, Loss: 0.6942020654678345\n",
      "Epoch 164, Batch 0, Loss: 0.6947782039642334\n",
      "Epoch 165, Batch 0, Loss: 0.694774329662323\n",
      "Epoch 166, Batch 0, Loss: 0.7014402151107788\n",
      "Epoch 167, Batch 0, Loss: 0.6953263282775879\n",
      "Epoch 168, Batch 0, Loss: 0.6897516250610352\n",
      "Epoch 169, Batch 0, Loss: 0.6917592287063599\n",
      "Epoch 170, Batch 0, Loss: 0.6947616338729858\n",
      "Epoch 171, Batch 0, Loss: 0.6916239857673645\n",
      "Epoch 172, Batch 0, Loss: 0.6850918531417847\n",
      "Epoch 173, Batch 0, Loss: 0.6850945949554443\n",
      "Epoch 174, Batch 0, Loss: 0.6917656660079956\n",
      "Epoch 175, Batch 0, Loss: 0.6930326223373413\n",
      "Epoch 176, Batch 0, Loss: 0.6944889426231384\n",
      "Epoch 177, Batch 0, Loss: 0.6958352327346802\n",
      "Epoch 178, Batch 0, Loss: 0.6917572021484375\n",
      "Epoch 179, Batch 0, Loss: 0.6947739124298096\n",
      "Epoch 180, Batch 0, Loss: 0.6851004362106323\n",
      "Epoch 181, Batch 0, Loss: 0.6904150247573853\n",
      "Epoch 182, Batch 0, Loss: 0.6942846775054932\n",
      "Epoch 183, Batch 0, Loss: 0.685116171836853\n",
      "Epoch 184, Batch 0, Loss: 0.6954004764556885\n",
      "Epoch 185, Batch 0, Loss: 0.6898561716079712\n",
      "Epoch 186, Batch 0, Loss: 0.701387882232666\n",
      "Epoch 187, Batch 0, Loss: 0.6947340965270996\n",
      "Epoch 188, Batch 0, Loss: 0.7013800740242004\n",
      "Epoch 189, Batch 0, Loss: 0.70137619972229\n",
      "Epoch 190, Batch 0, Loss: 0.6922745108604431\n",
      "Epoch 191, Batch 0, Loss: 0.6918084025382996\n",
      "Epoch 192, Batch 0, Loss: 0.6947009563446045\n",
      "Epoch 193, Batch 0, Loss: 0.6939457654953003\n",
      "Epoch 194, Batch 0, Loss: 0.6946877241134644\n",
      "Epoch 195, Batch 0, Loss: 0.694683849811554\n",
      "Epoch 196, Batch 0, Loss: 0.7013293504714966\n",
      "Epoch 197, Batch 0, Loss: 0.7013275623321533\n",
      "Epoch 198, Batch 0, Loss: 0.6946721076965332\n",
      "Epoch 199, Batch 0, Loss: 0.694668173789978\n",
      "Epoch 200, Batch 0, Loss: 0.6916675567626953\n",
      "Epoch 201, Batch 0, Loss: 0.6932869553565979\n",
      "Epoch 202, Batch 0, Loss: 0.6948875188827515\n",
      "Epoch 203, Batch 0, Loss: 0.6963672637939453\n",
      "Epoch 204, Batch 0, Loss: 0.6852372884750366\n",
      "Epoch 205, Batch 0, Loss: 0.7012845873832703\n",
      "Epoch 206, Batch 0, Loss: 0.6852350234985352\n",
      "Epoch 207, Batch 0, Loss: 0.6895866394042969\n",
      "Epoch 208, Batch 0, Loss: 0.6952124834060669\n",
      "Epoch 209, Batch 0, Loss: 0.7012724876403809\n",
      "Epoch 210, Batch 0, Loss: 0.6918777227401733\n",
      "Epoch 211, Batch 0, Loss: 0.6962569952011108\n",
      "Epoch 212, Batch 0, Loss: 0.685260534286499\n",
      "Epoch 213, Batch 0, Loss: 0.7012571096420288\n",
      "Epoch 214, Batch 0, Loss: 0.6852585077285767\n",
      "Epoch 215, Batch 0, Loss: 0.6896650195121765\n",
      "Epoch 216, Batch 0, Loss: 0.6918841004371643\n",
      "Epoch 217, Batch 0, Loss: 0.6946166753768921\n",
      "Epoch 218, Batch 0, Loss: 0.6852481365203857\n",
      "Epoch 219, Batch 0, Loss: 0.6948841214179993\n",
      "Epoch 220, Batch 0, Loss: 0.6919060945510864\n",
      "Epoch 221, Batch 0, Loss: 0.6945979595184326\n",
      "Epoch 222, Batch 0, Loss: 0.6852679252624512\n",
      "Epoch 223, Batch 0, Loss: 0.6945904493331909\n",
      "Epoch 224, Batch 0, Loss: 0.6956881284713745\n",
      "Epoch 225, Batch 0, Loss: 0.6852876543998718\n",
      "Epoch 226, Batch 0, Loss: 0.6919306516647339\n",
      "Epoch 227, Batch 0, Loss: 0.6945737600326538\n",
      "Epoch 228, Batch 0, Loss: 0.6945701241493225\n",
      "Epoch 229, Batch 0, Loss: 0.6962260007858276\n",
      "Epoch 230, Batch 0, Loss: 0.6976000070571899\n",
      "Epoch 231, Batch 0, Loss: 0.6853246092796326\n",
      "Epoch 232, Batch 0, Loss: 0.6880183219909668\n",
      "Epoch 233, Batch 0, Loss: 0.6898413896560669\n",
      "Epoch 234, Batch 0, Loss: 0.6949691772460938\n",
      "Epoch 235, Batch 0, Loss: 0.7011889219284058\n",
      "Epoch 236, Batch 0, Loss: 0.6919471025466919\n",
      "Epoch 237, Batch 0, Loss: 0.6960593461990356\n",
      "Epoch 238, Batch 0, Loss: 0.6853318214416504\n",
      "Epoch 239, Batch 0, Loss: 0.691963791847229\n",
      "Epoch 240, Batch 0, Loss: 0.685329794883728\n",
      "Epoch 241, Batch 0, Loss: 0.689784586429596\n",
      "Epoch 242, Batch 0, Loss: 0.6919541954994202\n",
      "Epoch 243, Batch 0, Loss: 0.6915513873100281\n",
      "Epoch 244, Batch 0, Loss: 0.6919480562210083\n",
      "Epoch 245, Batch 0, Loss: 0.6945390701293945\n",
      "Epoch 246, Batch 0, Loss: 0.6934556365013123\n",
      "Epoch 247, Batch 0, Loss: 0.6945250034332275\n",
      "Epoch 248, Batch 0, Loss: 0.7011585831642151\n",
      "Epoch 249, Batch 0, Loss: 0.6853336095809937\n",
      "Epoch 250, Batch 0, Loss: 0.6917086839675903\n",
      "Epoch 251, Batch 0, Loss: 0.6932361125946045\n",
      "Epoch 252, Batch 0, Loss: 0.6853445768356323\n",
      "Epoch 253, Batch 0, Loss: 0.6853457689285278\n",
      "Epoch 254, Batch 0, Loss: 0.6919848918914795\n",
      "Epoch 255, Batch 0, Loss: 0.6918323040008545\n",
      "Epoch 256, Batch 0, Loss: 0.701146125793457\n",
      "Epoch 257, Batch 0, Loss: 0.6945032477378845\n",
      "Epoch 258, Batch 0, Loss: 0.6853449940681458\n",
      "Epoch 259, Batch 0, Loss: 0.6919921636581421\n",
      "Epoch 260, Batch 0, Loss: 0.6931580901145935\n",
      "Epoch 261, Batch 0, Loss: 0.6944986581802368\n",
      "Epoch 262, Batch 0, Loss: 0.6853469610214233\n",
      "Epoch 263, Batch 0, Loss: 0.6942991018295288\n",
      "Epoch 264, Batch 0, Loss: 0.6919945478439331\n",
      "Epoch 265, Batch 0, Loss: 0.6919991970062256\n",
      "Epoch 266, Batch 0, Loss: 0.6913045644760132\n",
      "Epoch 267, Batch 0, Loss: 0.6944717168807983\n",
      "Epoch 268, Batch 0, Loss: 0.6853682994842529\n",
      "Epoch 269, Batch 0, Loss: 0.6944621801376343\n",
      "Epoch 270, Batch 0, Loss: 0.6920241117477417\n",
      "Epoch 271, Batch 0, Loss: 0.701107382774353\n",
      "Epoch 272, Batch 0, Loss: 0.6920323371887207\n",
      "Epoch 273, Batch 0, Loss: 0.6853774189949036\n",
      "Epoch 274, Batch 0, Loss: 0.6934033036231995\n",
      "Epoch 275, Batch 0, Loss: 0.691500186920166\n",
      "Epoch 276, Batch 0, Loss: 0.6930575966835022\n",
      "Epoch 277, Batch 0, Loss: 0.6919839382171631\n",
      "Epoch 278, Batch 0, Loss: 0.6920437812805176\n",
      "Epoch 279, Batch 0, Loss: 0.6927880048751831\n",
      "Epoch 280, Batch 0, Loss: 0.6920408010482788\n",
      "Epoch 281, Batch 0, Loss: 0.7010815143585205\n",
      "Epoch 282, Batch 0, Loss: 0.6920493841171265\n",
      "Epoch 283, Batch 0, Loss: 0.6938949823379517\n",
      "Epoch 284, Batch 0, Loss: 0.701083779335022\n",
      "Epoch 285, Batch 0, Loss: 0.6914016008377075\n",
      "Epoch 286, Batch 0, Loss: 0.6944146156311035\n",
      "Epoch 287, Batch 0, Loss: 0.6933027505874634\n",
      "Epoch 288, Batch 0, Loss: 0.6917559504508972\n",
      "Epoch 289, Batch 0, Loss: 0.6930218935012817\n",
      "Epoch 290, Batch 0, Loss: 0.6944161057472229\n",
      "Epoch 291, Batch 0, Loss: 0.6920653581619263\n",
      "Epoch 292, Batch 0, Loss: 0.6944055557250977\n",
      "Epoch 293, Batch 0, Loss: 0.692396342754364\n",
      "Epoch 294, Batch 0, Loss: 0.6920835971832275\n",
      "Epoch 295, Batch 0, Loss: 0.6924334168434143\n",
      "Epoch 296, Batch 0, Loss: 0.7010331153869629\n",
      "Epoch 297, Batch 0, Loss: 0.6854419708251953\n",
      "Epoch 298, Batch 0, Loss: 0.6927469968795776\n",
      "Epoch 299, Batch 0, Loss: 0.7010111808776855\n",
      "Epoch 300, Batch 0, Loss: 0.6943172216415405\n",
      "Epoch 301, Batch 0, Loss: 0.6943612098693848\n",
      "Epoch 302, Batch 0, Loss: 0.6943576335906982\n",
      "Epoch 303, Batch 0, Loss: 0.6854771375656128\n",
      "Epoch 304, Batch 0, Loss: 0.6943502426147461\n",
      "Epoch 305, Batch 0, Loss: 0.7009928226470947\n",
      "Epoch 306, Batch 0, Loss: 0.6943429708480835\n",
      "Epoch 307, Batch 0, Loss: 0.695045530796051\n",
      "Epoch 308, Batch 0, Loss: 0.6854956150054932\n",
      "Epoch 309, Batch 0, Loss: 0.6854952573776245\n",
      "Epoch 310, Batch 0, Loss: 0.6903460025787354\n",
      "Epoch 311, Batch 0, Loss: 0.6944830417633057\n",
      "Epoch 312, Batch 0, Loss: 0.6959707736968994\n",
      "Epoch 313, Batch 0, Loss: 0.6973468661308289\n",
      "Epoch 314, Batch 0, Loss: 0.7009496688842773\n",
      "Epoch 315, Batch 0, Loss: 0.7009490728378296\n",
      "Epoch 316, Batch 0, Loss: 0.6943092346191406\n",
      "Epoch 317, Batch 0, Loss: 0.6888955235481262\n",
      "Epoch 318, Batch 0, Loss: 0.6855167746543884\n",
      "Epoch 319, Batch 0, Loss: 0.6956884860992432\n",
      "Epoch 320, Batch 0, Loss: 0.6921721696853638\n",
      "Epoch 321, Batch 0, Loss: 0.6921738386154175\n",
      "Epoch 322, Batch 0, Loss: 0.6921756267547607\n",
      "Epoch 323, Batch 0, Loss: 0.685530960559845\n",
      "Epoch 324, Batch 0, Loss: 0.6942874193191528\n",
      "Epoch 325, Batch 0, Loss: 0.6855297088623047\n",
      "Epoch 326, Batch 0, Loss: 0.6958913207054138\n",
      "Epoch 327, Batch 0, Loss: 0.6921972036361694\n",
      "Epoch 328, Batch 0, Loss: 0.6970192193984985\n",
      "Epoch 329, Batch 0, Loss: 0.6983139514923096\n",
      "Epoch 330, Batch 0, Loss: 0.6995111107826233\n",
      "Epoch 331, Batch 0, Loss: 0.7006180286407471\n",
      "Epoch 332, Batch 0, Loss: 0.6942658424377441\n",
      "Epoch 333, Batch 0, Loss: 0.6922493577003479\n",
      "Epoch 334, Batch 0, Loss: 0.6942563056945801\n",
      "Epoch 335, Batch 0, Loss: 0.7008957862854004\n",
      "Epoch 336, Batch 0, Loss: 0.6855946779251099\n",
      "Epoch 337, Batch 0, Loss: 0.6855906248092651\n",
      "Epoch 338, Batch 0, Loss: 0.7008931040763855\n",
      "Epoch 339, Batch 0, Loss: 0.6983163356781006\n",
      "Epoch 340, Batch 0, Loss: 0.6942350268363953\n",
      "Epoch 341, Batch 0, Loss: 0.6855977773666382\n",
      "Epoch 342, Batch 0, Loss: 0.6922513246536255\n",
      "Epoch 343, Batch 0, Loss: 0.6855910420417786\n",
      "Epoch 344, Batch 0, Loss: 0.6884785294532776\n",
      "Epoch 345, Batch 0, Loss: 0.6962100267410278\n",
      "Epoch 346, Batch 0, Loss: 0.7008761167526245\n",
      "Epoch 347, Batch 0, Loss: 0.6922489404678345\n",
      "Epoch 348, Batch 0, Loss: 0.6922504901885986\n",
      "Epoch 349, Batch 0, Loss: 0.6967837810516357\n",
      "Epoch 350, Batch 0, Loss: 0.6883782148361206\n",
      "Epoch 351, Batch 0, Loss: 0.6942094564437866\n",
      "Epoch 352, Batch 0, Loss: 0.6922508478164673\n",
      "Epoch 353, Batch 0, Loss: 0.6959431171417236\n",
      "Epoch 354, Batch 0, Loss: 0.697313666343689\n",
      "Epoch 355, Batch 0, Loss: 0.6941941380500793\n",
      "Epoch 356, Batch 0, Loss: 0.6856200098991394\n",
      "Epoch 357, Batch 0, Loss: 0.6885772943496704\n",
      "Epoch 358, Batch 0, Loss: 0.6922634840011597\n",
      "Epoch 359, Batch 0, Loss: 0.6959523558616638\n",
      "Epoch 360, Batch 0, Loss: 0.6941827535629272\n",
      "Epoch 361, Batch 0, Loss: 0.6894118785858154\n",
      "Epoch 362, Batch 0, Loss: 0.6856058835983276\n",
      "Epoch 363, Batch 0, Loss: 0.6912136077880859\n",
      "Epoch 364, Batch 0, Loss: 0.7008494138717651\n",
      "Epoch 365, Batch 0, Loss: 0.6922622323036194\n",
      "Epoch 366, Batch 0, Loss: 0.6941805481910706\n",
      "Epoch 367, Batch 0, Loss: 0.6936944127082825\n",
      "Epoch 368, Batch 0, Loss: 0.692280650138855\n",
      "Epoch 369, Batch 0, Loss: 0.6922836303710938\n",
      "Epoch 370, Batch 0, Loss: 0.6914041042327881\n",
      "Epoch 371, Batch 0, Loss: 0.6941680312156677\n",
      "Epoch 372, Batch 0, Loss: 0.6856095790863037\n",
      "Epoch 373, Batch 0, Loss: 0.7008335590362549\n",
      "Epoch 374, Batch 0, Loss: 0.6928896903991699\n",
      "Epoch 375, Batch 0, Loss: 0.7008392810821533\n",
      "Epoch 376, Batch 0, Loss: 0.6922860741615295\n",
      "Epoch 377, Batch 0, Loss: 0.6923667192459106\n",
      "Epoch 378, Batch 0, Loss: 0.6922996044158936\n",
      "Epoch 379, Batch 0, Loss: 0.6939950585365295\n",
      "Epoch 380, Batch 0, Loss: 0.6941307187080383\n",
      "Epoch 381, Batch 0, Loss: 0.6941273212432861\n",
      "Epoch 382, Batch 0, Loss: 0.6856397986412048\n",
      "Epoch 383, Batch 0, Loss: 0.6941201686859131\n",
      "Epoch 384, Batch 0, Loss: 0.7008024454116821\n",
      "Epoch 385, Batch 0, Loss: 0.6915063858032227\n",
      "Epoch 386, Batch 0, Loss: 0.6933939456939697\n",
      "Epoch 387, Batch 0, Loss: 0.6914926767349243\n",
      "Epoch 388, Batch 0, Loss: 0.693406879901886\n",
      "Epoch 389, Batch 0, Loss: 0.7007853984832764\n",
      "Epoch 390, Batch 0, Loss: 0.6948630809783936\n",
      "Epoch 391, Batch 0, Loss: 0.6963067650794983\n",
      "Epoch 392, Batch 0, Loss: 0.6923578977584839\n",
      "Epoch 393, Batch 0, Loss: 0.689088761806488\n",
      "Epoch 394, Batch 0, Loss: 0.6923441886901855\n",
      "Epoch 395, Batch 0, Loss: 0.6923469305038452\n",
      "Epoch 396, Batch 0, Loss: 0.695388913154602\n",
      "Epoch 397, Batch 0, Loss: 0.7007566690444946\n",
      "Epoch 398, Batch 0, Loss: 0.6856873035430908\n",
      "Epoch 399, Batch 0, Loss: 0.690082848072052\n",
      "Epoch 400, Batch 0, Loss: 0.6947115659713745\n",
      "Epoch 401, Batch 0, Loss: 0.7007489800453186\n",
      "Epoch 402, Batch 0, Loss: 0.6923667192459106\n",
      "Epoch 403, Batch 0, Loss: 0.6958222985267639\n",
      "Epoch 404, Batch 0, Loss: 0.6857067346572876\n",
      "Epoch 405, Batch 0, Loss: 0.6923830509185791\n",
      "Epoch 406, Batch 0, Loss: 0.7007386684417725\n",
      "Epoch 407, Batch 0, Loss: 0.6923863887786865\n",
      "Epoch 408, Batch 0, Loss: 0.7007385492324829\n",
      "Epoch 409, Batch 0, Loss: 0.6903400421142578\n",
      "Epoch 410, Batch 0, Loss: 0.6944711208343506\n",
      "Epoch 411, Batch 0, Loss: 0.6959397792816162\n",
      "Epoch 412, Batch 0, Loss: 0.6857218742370605\n",
      "Epoch 413, Batch 0, Loss: 0.697045087814331\n",
      "Epoch 414, Batch 0, Loss: 0.6924169063568115\n",
      "Epoch 415, Batch 0, Loss: 0.700714111328125\n",
      "Epoch 416, Batch 0, Loss: 0.6857312917709351\n",
      "Epoch 417, Batch 0, Loss: 0.697381854057312\n",
      "Epoch 418, Batch 0, Loss: 0.6924317479133606\n",
      "Epoch 419, Batch 0, Loss: 0.694017767906189\n",
      "Epoch 420, Batch 0, Loss: 0.6979299783706665\n",
      "Epoch 421, Batch 0, Loss: 0.6873183846473694\n",
      "Epoch 422, Batch 0, Loss: 0.6940144300460815\n",
      "Epoch 423, Batch 0, Loss: 0.692427396774292\n",
      "Epoch 424, Batch 0, Loss: 0.6940076947212219\n",
      "Epoch 425, Batch 0, Loss: 0.6857336759567261\n",
      "Epoch 426, Batch 0, Loss: 0.6963472366333008\n",
      "Epoch 427, Batch 0, Loss: 0.6887686848640442\n",
      "Epoch 428, Batch 0, Loss: 0.6857357025146484\n",
      "Epoch 429, Batch 0, Loss: 0.6957783102989197\n",
      "Epoch 430, Batch 0, Loss: 0.6939924955368042\n",
      "Epoch 431, Batch 0, Loss: 0.6924458742141724\n",
      "Epoch 432, Batch 0, Loss: 0.6939858198165894\n",
      "Epoch 433, Batch 0, Loss: 0.6939825415611267\n",
      "Epoch 434, Batch 0, Loss: 0.6857441663742065\n",
      "Epoch 435, Batch 0, Loss: 0.6939759254455566\n",
      "Epoch 436, Batch 0, Loss: 0.6924556493759155\n",
      "Epoch 437, Batch 0, Loss: 0.6939691305160522\n",
      "Epoch 438, Batch 0, Loss: 0.6924606561660767\n",
      "Epoch 439, Batch 0, Loss: 0.6857396364212036\n",
      "Epoch 440, Batch 0, Loss: 0.6939588785171509\n",
      "Epoch 441, Batch 0, Loss: 0.6857386827468872\n",
      "Epoch 442, Batch 0, Loss: 0.6924719214439392\n",
      "Epoch 443, Batch 0, Loss: 0.695027232170105\n",
      "Epoch 444, Batch 0, Loss: 0.689979076385498\n",
      "Epoch 445, Batch 0, Loss: 0.7006787061691284\n",
      "Epoch 446, Batch 0, Loss: 0.6939443349838257\n",
      "Epoch 447, Batch 0, Loss: 0.685742974281311\n",
      "Epoch 448, Batch 0, Loss: 0.6857431530952454\n",
      "Epoch 449, Batch 0, Loss: 0.6918851137161255\n",
      "Epoch 450, Batch 0, Loss: 0.6933871507644653\n",
      "Epoch 451, Batch 0, Loss: 0.6939509510993958\n",
      "Epoch 452, Batch 0, Loss: 0.6918001174926758\n",
      "Epoch 453, Batch 0, Loss: 0.6934599876403809\n",
      "Epoch 454, Batch 0, Loss: 0.693924069404602\n",
      "Epoch 455, Batch 0, Loss: 0.6948976516723633\n",
      "Epoch 456, Batch 0, Loss: 0.700648307800293\n",
      "Epoch 457, Batch 0, Loss: 0.6925122141838074\n",
      "Epoch 458, Batch 0, Loss: 0.6959599256515503\n",
      "Epoch 459, Batch 0, Loss: 0.6925269365310669\n",
      "Epoch 460, Batch 0, Loss: 0.6857870817184448\n",
      "Epoch 461, Batch 0, Loss: 0.6857852935791016\n",
      "Epoch 462, Batch 0, Loss: 0.693892240524292\n",
      "Epoch 463, Batch 0, Loss: 0.6857818961143494\n",
      "Epoch 464, Batch 0, Loss: 0.6925351619720459\n",
      "Epoch 465, Batch 0, Loss: 0.7006403207778931\n",
      "Epoch 466, Batch 0, Loss: 0.6905733942985535\n",
      "Epoch 467, Batch 0, Loss: 0.6925284266471863\n",
      "Epoch 468, Batch 0, Loss: 0.692531943321228\n",
      "Epoch 469, Batch 0, Loss: 0.6922168135643005\n",
      "Epoch 470, Batch 0, Loss: 0.6936914920806885\n",
      "Epoch 471, Batch 0, Loss: 0.6938963532447815\n",
      "Epoch 472, Batch 0, Loss: 0.700656533241272\n",
      "Epoch 473, Batch 0, Loss: 0.6938856840133667\n",
      "Epoch 474, Batch 0, Loss: 0.6918474435806274\n",
      "Epoch 475, Batch 0, Loss: 0.6934998035430908\n",
      "Epoch 476, Batch 0, Loss: 0.700623631477356\n",
      "Epoch 477, Batch 0, Loss: 0.6938555240631104\n",
      "Epoch 478, Batch 0, Loss: 0.6925598382949829\n",
      "Epoch 479, Batch 0, Loss: 0.6947638988494873\n",
      "Epoch 480, Batch 0, Loss: 0.6925745010375977\n",
      "Epoch 481, Batch 0, Loss: 0.68580162525177\n",
      "Epoch 482, Batch 0, Loss: 0.6925790309906006\n",
      "Epoch 483, Batch 0, Loss: 0.7006133794784546\n",
      "Epoch 484, Batch 0, Loss: 0.6955658197402954\n",
      "Epoch 485, Batch 0, Loss: 0.7006043791770935\n",
      "Epoch 486, Batch 0, Loss: 0.6938192844390869\n",
      "Epoch 487, Batch 0, Loss: 0.6925990581512451\n",
      "Epoch 488, Batch 0, Loss: 0.7006052136421204\n",
      "Epoch 489, Batch 0, Loss: 0.6960770487785339\n",
      "Epoch 490, Batch 0, Loss: 0.6974133849143982\n",
      "Epoch 491, Batch 0, Loss: 0.6926286816596985\n",
      "Epoch 492, Batch 0, Loss: 0.6982923746109009\n",
      "Epoch 493, Batch 0, Loss: 0.6858524680137634\n",
      "Epoch 494, Batch 0, Loss: 0.6926407814025879\n",
      "Epoch 495, Batch 0, Loss: 0.6986571550369263\n",
      "Epoch 496, Batch 0, Loss: 0.6998003721237183\n",
      "Epoch 497, Batch 0, Loss: 0.6926676034927368\n",
      "Epoch 498, Batch 0, Loss: 0.7005773782730103\n",
      "Epoch 499, Batch 0, Loss: 0.6858640909194946\n",
      "Epoch 500, Batch 0, Loss: 0.7005765438079834\n",
      "Epoch 501, Batch 0, Loss: 0.6858540773391724\n",
      "Epoch 502, Batch 0, Loss: 0.6858500242233276\n",
      "Epoch 503, Batch 0, Loss: 0.6858464479446411\n",
      "Epoch 504, Batch 0, Loss: 0.6885237693786621\n",
      "Epoch 505, Batch 0, Loss: 0.6902662515640259\n",
      "Epoch 506, Batch 0, Loss: 0.6858192682266235\n",
      "Epoch 507, Batch 0, Loss: 0.6926342844963074\n",
      "Epoch 508, Batch 0, Loss: 0.6944341659545898\n",
      "Epoch 509, Batch 0, Loss: 0.6905167698860168\n",
      "Epoch 510, Batch 0, Loss: 0.7005770206451416\n",
      "Epoch 511, Batch 0, Loss: 0.6858240365982056\n",
      "Epoch 512, Batch 0, Loss: 0.7005763053894043\n",
      "Epoch 513, Batch 0, Loss: 0.7005760073661804\n",
      "Epoch 514, Batch 0, Loss: 0.6941377520561218\n",
      "Epoch 515, Batch 0, Loss: 0.7005645632743835\n",
      "Epoch 516, Batch 0, Loss: 0.6937373876571655\n",
      "Epoch 517, Batch 0, Loss: 0.6953315734863281\n",
      "Epoch 518, Batch 0, Loss: 0.6896889805793762\n",
      "Epoch 519, Batch 0, Loss: 0.6950571537017822\n",
      "Epoch 520, Batch 0, Loss: 0.6899418830871582\n",
      "Epoch 521, Batch 0, Loss: 0.6948225498199463\n",
      "Epoch 522, Batch 0, Loss: 0.6962454319000244\n",
      "Epoch 523, Batch 0, Loss: 0.6937221884727478\n",
      "Epoch 524, Batch 0, Loss: 0.6891393661499023\n",
      "Epoch 525, Batch 0, Loss: 0.6926758289337158\n",
      "Epoch 526, Batch 0, Loss: 0.7005409002304077\n",
      "Epoch 527, Batch 0, Loss: 0.6937171220779419\n",
      "Epoch 528, Batch 0, Loss: 0.6937137246131897\n",
      "Epoch 529, Batch 0, Loss: 0.6858550310134888\n",
      "Epoch 530, Batch 0, Loss: 0.6914109587669373\n",
      "Epoch 531, Batch 0, Loss: 0.6926802396774292\n",
      "Epoch 532, Batch 0, Loss: 0.7005470991134644\n",
      "Epoch 533, Batch 0, Loss: 0.6926878094673157\n",
      "Epoch 534, Batch 0, Loss: 0.693535327911377\n",
      "Epoch 535, Batch 0, Loss: 0.693693220615387\n",
      "Epoch 536, Batch 0, Loss: 0.6927048563957214\n",
      "Epoch 537, Batch 0, Loss: 0.6936861276626587\n",
      "Epoch 538, Batch 0, Loss: 0.6916145086288452\n",
      "Epoch 539, Batch 0, Loss: 0.6858513951301575\n",
      "Epoch 540, Batch 0, Loss: 0.7005402445793152\n",
      "Epoch 541, Batch 0, Loss: 0.6927096247673035\n",
      "Epoch 542, Batch 0, Loss: 0.700539231300354\n",
      "Epoch 543, Batch 0, Loss: 0.7005387544631958\n",
      "Epoch 544, Batch 0, Loss: 0.6934244632720947\n",
      "Epoch 545, Batch 0, Loss: 0.691444456577301\n",
      "Epoch 546, Batch 0, Loss: 0.6934256553649902\n",
      "Epoch 547, Batch 0, Loss: 0.693659245967865\n",
      "Epoch 548, Batch 0, Loss: 0.6858707070350647\n",
      "Epoch 549, Batch 0, Loss: 0.7005200982093811\n",
      "Epoch 550, Batch 0, Loss: 0.6916936635971069\n",
      "Epoch 551, Batch 0, Loss: 0.6931943893432617\n",
      "Epoch 552, Batch 0, Loss: 0.6936469078063965\n",
      "Epoch 553, Batch 0, Loss: 0.6917362213134766\n",
      "Epoch 554, Batch 0, Loss: 0.6927367448806763\n",
      "Epoch 555, Batch 0, Loss: 0.6858681440353394\n",
      "Epoch 556, Batch 0, Loss: 0.693124532699585\n",
      "Epoch 557, Batch 0, Loss: 0.7005259990692139\n",
      "Epoch 558, Batch 0, Loss: 0.6936469078063965\n",
      "Epoch 559, Batch 0, Loss: 0.692130446434021\n",
      "Epoch 560, Batch 0, Loss: 0.693745493888855\n",
      "Epoch 561, Batch 0, Loss: 0.69524085521698\n",
      "Epoch 562, Batch 0, Loss: 0.6897683143615723\n",
      "Epoch 563, Batch 0, Loss: 0.6936213970184326\n",
      "Epoch 564, Batch 0, Loss: 0.7004933953285217\n",
      "Epoch 565, Batch 0, Loss: 0.6947891712188721\n",
      "Epoch 566, Batch 0, Loss: 0.6859060525894165\n",
      "Epoch 567, Batch 0, Loss: 0.6859046220779419\n",
      "Epoch 568, Batch 0, Loss: 0.6905150413513184\n",
      "Epoch 569, Batch 0, Loss: 0.6858938932418823\n",
      "Epoch 570, Batch 0, Loss: 0.6936039924621582\n",
      "Epoch 571, Batch 0, Loss: 0.6858930587768555\n",
      "Epoch 572, Batch 0, Loss: 0.7004896402359009\n",
      "Epoch 573, Batch 0, Loss: 0.6927895545959473\n",
      "Epoch 574, Batch 0, Loss: 0.6927930116653442\n",
      "Epoch 575, Batch 0, Loss: 0.6927965879440308\n",
      "Epoch 576, Batch 0, Loss: 0.7004901170730591\n",
      "Epoch 577, Batch 0, Loss: 0.6858909130096436\n",
      "Epoch 578, Batch 0, Loss: 0.6940504908561707\n",
      "Epoch 579, Batch 0, Loss: 0.6928178668022156\n",
      "Epoch 580, Batch 0, Loss: 0.6953986883163452\n",
      "Epoch 581, Batch 0, Loss: 0.6859173774719238\n",
      "Epoch 582, Batch 0, Loss: 0.6965532302856445\n",
      "Epoch 583, Batch 0, Loss: 0.6935491561889648\n",
      "Epoch 584, Batch 0, Loss: 0.6859269142150879\n",
      "Epoch 585, Batch 0, Loss: 0.6972517967224121\n",
      "Epoch 586, Batch 0, Loss: 0.6928613185882568\n",
      "Epoch 587, Batch 0, Loss: 0.693536102771759\n",
      "Epoch 588, Batch 0, Loss: 0.6978031396865845\n",
      "Epoch 589, Batch 0, Loss: 0.685947060585022\n",
      "Epoch 590, Batch 0, Loss: 0.6928740739822388\n",
      "Epoch 591, Batch 0, Loss: 0.6928738951683044\n",
      "Epoch 592, Batch 0, Loss: 0.6979291439056396\n",
      "Epoch 593, Batch 0, Loss: 0.6928874254226685\n",
      "Epoch 594, Batch 0, Loss: 0.6928867101669312\n",
      "Epoch 595, Batch 0, Loss: 0.6859408617019653\n",
      "Epoch 596, Batch 0, Loss: 0.6859371662139893\n",
      "Epoch 597, Batch 0, Loss: 0.6935038566589355\n",
      "Epoch 598, Batch 0, Loss: 0.688981294631958\n",
      "Epoch 599, Batch 0, Loss: 0.6928732395172119\n",
      "Epoch 600, Batch 0, Loss: 0.6859160661697388\n",
      "Epoch 601, Batch 0, Loss: 0.690934419631958\n",
      "Epoch 602, Batch 0, Loss: 0.6938900351524353\n",
      "Epoch 603, Batch 0, Loss: 0.6859195232391357\n",
      "Epoch 604, Batch 0, Loss: 0.695257306098938\n",
      "Epoch 605, Batch 0, Loss: 0.6897473335266113\n",
      "Epoch 606, Batch 0, Loss: 0.694991946220398\n",
      "Epoch 607, Batch 0, Loss: 0.69289231300354\n",
      "Epoch 608, Batch 0, Loss: 0.6901777982711792\n",
      "Epoch 609, Batch 0, Loss: 0.6859234571456909\n",
      "Epoch 610, Batch 0, Loss: 0.6928858757019043\n",
      "Epoch 611, Batch 0, Loss: 0.6859220266342163\n",
      "Epoch 612, Batch 0, Loss: 0.6934787631034851\n",
      "Epoch 613, Batch 0, Loss: 0.692895770072937\n",
      "Epoch 614, Batch 0, Loss: 0.6859198808670044\n",
      "Epoch 615, Batch 0, Loss: 0.6943153142929077\n",
      "Epoch 616, Batch 0, Loss: 0.6859326362609863\n",
      "Epoch 617, Batch 0, Loss: 0.693456768989563\n",
      "Epoch 618, Batch 0, Loss: 0.6954723596572876\n",
      "Epoch 619, Batch 0, Loss: 0.6934477686882019\n",
      "Epoch 620, Batch 0, Loss: 0.6897835731506348\n",
      "Epoch 621, Batch 0, Loss: 0.691413402557373\n",
      "Epoch 622, Batch 0, Loss: 0.6934576630592346\n",
      "Epoch 623, Batch 0, Loss: 0.685922384262085\n",
      "Epoch 624, Batch 0, Loss: 0.6928747892379761\n",
      "Epoch 625, Batch 0, Loss: 0.6942769289016724\n",
      "Epoch 626, Batch 0, Loss: 0.6907995939254761\n",
      "Epoch 627, Batch 0, Loss: 0.6859244704246521\n",
      "Epoch 628, Batch 0, Loss: 0.6929183006286621\n",
      "Epoch 629, Batch 0, Loss: 0.6936676502227783\n",
      "Epoch 630, Batch 0, Loss: 0.685921311378479\n",
      "Epoch 631, Batch 0, Loss: 0.6934483051300049\n",
      "Epoch 632, Batch 0, Loss: 0.6917068958282471\n",
      "Epoch 633, Batch 0, Loss: 0.6933391094207764\n",
      "Epoch 634, Batch 0, Loss: 0.6934213042259216\n",
      "Epoch 635, Batch 0, Loss: 0.6859427690505981\n",
      "Epoch 636, Batch 0, Loss: 0.6946961879730225\n",
      "Epoch 637, Batch 0, Loss: 0.7004138231277466\n",
      "Epoch 638, Batch 0, Loss: 0.7004145979881287\n",
      "Epoch 639, Batch 0, Loss: 0.7004154324531555\n",
      "Epoch 640, Batch 0, Loss: 0.690768837928772\n",
      "Epoch 641, Batch 0, Loss: 0.6929580569267273\n",
      "Epoch 642, Batch 0, Loss: 0.692961573600769\n",
      "Epoch 643, Batch 0, Loss: 0.6933964490890503\n",
      "Epoch 644, Batch 0, Loss: 0.7004225254058838\n",
      "Epoch 645, Batch 0, Loss: 0.6929721236228943\n",
      "Epoch 646, Batch 0, Loss: 0.6939593553543091\n",
      "Epoch 647, Batch 0, Loss: 0.6859501600265503\n",
      "Epoch 648, Batch 0, Loss: 0.693373441696167\n",
      "Epoch 649, Batch 0, Loss: 0.6951850056648254\n",
      "Epoch 650, Batch 0, Loss: 0.6933640241622925\n",
      "Epoch 651, Batch 0, Loss: 0.6933608055114746\n",
      "Epoch 652, Batch 0, Loss: 0.6930072903633118\n",
      "Epoch 653, Batch 0, Loss: 0.693009614944458\n",
      "Epoch 654, Batch 0, Loss: 0.6859517097473145\n",
      "Epoch 655, Batch 0, Loss: 0.6859502792358398\n",
      "Epoch 656, Batch 0, Loss: 0.693344235420227\n",
      "Epoch 657, Batch 0, Loss: 0.6909641027450562\n",
      "Epoch 658, Batch 0, Loss: 0.6930094957351685\n",
      "Epoch 659, Batch 0, Loss: 0.7004191279411316\n",
      "Epoch 660, Batch 0, Loss: 0.6859374046325684\n",
      "Epoch 661, Batch 0, Loss: 0.6930204629898071\n",
      "Epoch 662, Batch 0, Loss: 0.6925092935562134\n",
      "Epoch 663, Batch 0, Loss: 0.692423939704895\n",
      "Epoch 664, Batch 0, Loss: 0.6930261850357056\n",
      "Epoch 665, Batch 0, Loss: 0.6859409809112549\n",
      "Epoch 666, Batch 0, Loss: 0.7004145383834839\n",
      "Epoch 667, Batch 0, Loss: 0.6933180093765259\n",
      "Epoch 668, Batch 0, Loss: 0.6930403709411621\n",
      "Epoch 669, Batch 0, Loss: 0.6859381794929504\n",
      "Epoch 670, Batch 0, Loss: 0.685937762260437\n",
      "Epoch 671, Batch 0, Loss: 0.6930512189865112\n",
      "Epoch 672, Batch 0, Loss: 0.6932985186576843\n",
      "Epoch 673, Batch 0, Loss: 0.6930583715438843\n",
      "Epoch 674, Batch 0, Loss: 0.6859347820281982\n",
      "Epoch 675, Batch 0, Loss: 0.6932868957519531\n",
      "Epoch 676, Batch 0, Loss: 0.6859334111213684\n",
      "Epoch 677, Batch 0, Loss: 0.6859328746795654\n",
      "Epoch 678, Batch 0, Loss: 0.6859323978424072\n",
      "Epoch 679, Batch 0, Loss: 0.6859318017959595\n",
      "Epoch 680, Batch 0, Loss: 0.7004197239875793\n",
      "Epoch 681, Batch 0, Loss: 0.6932635307312012\n",
      "Epoch 682, Batch 0, Loss: 0.7004207372665405\n",
      "Epoch 683, Batch 0, Loss: 0.6925225257873535\n",
      "Epoch 684, Batch 0, Loss: 0.692406415939331\n",
      "Epoch 685, Batch 0, Loss: 0.6859346628189087\n",
      "Epoch 686, Batch 0, Loss: 0.6932494044303894\n",
      "Epoch 687, Batch 0, Loss: 0.69394850730896\n",
      "Epoch 688, Batch 0, Loss: 0.7004061937332153\n",
      "Epoch 689, Batch 0, Loss: 0.6932342052459717\n",
      "Epoch 690, Batch 0, Loss: 0.6931194067001343\n",
      "Epoch 691, Batch 0, Loss: 0.7004091739654541\n",
      "Epoch 692, Batch 0, Loss: 0.6931252479553223\n",
      "Epoch 693, Batch 0, Loss: 0.6932203769683838\n",
      "Epoch 694, Batch 0, Loss: 0.6859363317489624\n",
      "Epoch 695, Batch 0, Loss: 0.6932132244110107\n",
      "Epoch 696, Batch 0, Loss: 0.6932097673416138\n",
      "Epoch 697, Batch 0, Loss: 0.6931407451629639\n",
      "Epoch 698, Batch 0, Loss: 0.6932026147842407\n",
      "Epoch 699, Batch 0, Loss: 0.6944433450698853\n",
      "Epoch 700, Batch 0, Loss: 0.6904831528663635\n",
      "Epoch 701, Batch 0, Loss: 0.6920534372329712\n",
      "Epoch 702, Batch 0, Loss: 0.70041823387146\n",
      "Epoch 703, Batch 0, Loss: 0.7004182934761047\n",
      "Epoch 704, Batch 0, Loss: 0.6859264969825745\n",
      "Epoch 705, Batch 0, Loss: 0.7004181146621704\n",
      "Epoch 706, Batch 0, Loss: 0.6930665969848633\n",
      "Epoch 707, Batch 0, Loss: 0.7004069089889526\n",
      "Epoch 708, Batch 0, Loss: 0.6859360337257385\n",
      "Epoch 709, Batch 0, Loss: 0.7004082202911377\n",
      "Epoch 710, Batch 0, Loss: 0.6944080591201782\n",
      "Epoch 711, Batch 0, Loss: 0.6859465837478638\n",
      "Epoch 712, Batch 0, Loss: 0.6906603574752808\n",
      "Epoch 713, Batch 0, Loss: 0.6931778192520142\n",
      "Epoch 714, Batch 0, Loss: 0.7004066705703735\n",
      "Epoch 715, Batch 0, Loss: 0.6922695636749268\n",
      "Epoch 716, Batch 0, Loss: 0.6937068700790405\n",
      "Epoch 717, Batch 0, Loss: 0.6950385570526123\n",
      "Epoch 718, Batch 0, Loss: 0.6931700706481934\n",
      "Epoch 719, Batch 0, Loss: 0.6959881782531738\n",
      "Epoch 720, Batch 0, Loss: 0.7004367113113403\n",
      "Epoch 721, Batch 0, Loss: 0.7004337310791016\n",
      "Epoch 722, Batch 0, Loss: 0.6931811571121216\n",
      "Epoch 723, Batch 0, Loss: 0.6931854486465454\n",
      "Epoch 724, Batch 0, Loss: 0.6931897401809692\n",
      "Epoch 725, Batch 0, Loss: 0.6931940317153931\n",
      "Epoch 726, Batch 0, Loss: 0.6859214305877686\n",
      "Epoch 727, Batch 0, Loss: 0.6931414604187012\n",
      "Epoch 728, Batch 0, Loss: 0.6950241923332214\n",
      "Epoch 729, Batch 0, Loss: 0.693145751953125\n",
      "Epoch 730, Batch 0, Loss: 0.6859192252159119\n",
      "Epoch 731, Batch 0, Loss: 0.6859192252159119\n",
      "Epoch 732, Batch 0, Loss: 0.6908584237098694\n",
      "Epoch 733, Batch 0, Loss: 0.6931130290031433\n",
      "Epoch 734, Batch 0, Loss: 0.6937154531478882\n",
      "Epoch 735, Batch 0, Loss: 0.6931174993515015\n",
      "Epoch 736, Batch 0, Loss: 0.6932277083396912\n",
      "Epoch 737, Batch 0, Loss: 0.6946898698806763\n",
      "Epoch 738, Batch 0, Loss: 0.6859209537506104\n",
      "Epoch 739, Batch 0, Loss: 0.6906558871269226\n",
      "Epoch 740, Batch 0, Loss: 0.6859289407730103\n",
      "Epoch 741, Batch 0, Loss: 0.6930909752845764\n",
      "Epoch 742, Batch 0, Loss: 0.6859279870986938\n",
      "Epoch 743, Batch 0, Loss: 0.6936863660812378\n",
      "Epoch 744, Batch 0, Loss: 0.7004139423370361\n",
      "Epoch 745, Batch 0, Loss: 0.7004131078720093\n",
      "Epoch 746, Batch 0, Loss: 0.6859238147735596\n",
      "Epoch 747, Batch 0, Loss: 0.6918176412582397\n",
      "Epoch 748, Batch 0, Loss: 0.6929059028625488\n",
      "Epoch 749, Batch 0, Loss: 0.7004052996635437\n",
      "Epoch 750, Batch 0, Loss: 0.6921606063842773\n",
      "Epoch 751, Batch 0, Loss: 0.6932750940322876\n",
      "Epoch 752, Batch 0, Loss: 0.6925787925720215\n",
      "Epoch 753, Batch 0, Loss: 0.6923439502716064\n",
      "Epoch 754, Batch 0, Loss: 0.6932811141014099\n",
      "Epoch 755, Batch 0, Loss: 0.6859405040740967\n",
      "Epoch 756, Batch 0, Loss: 0.7003892660140991\n",
      "Epoch 757, Batch 0, Loss: 0.6930367946624756\n",
      "Epoch 758, Batch 0, Loss: 0.7003909349441528\n",
      "Epoch 759, Batch 0, Loss: 0.6930290460586548\n",
      "Epoch 760, Batch 0, Loss: 0.6930251121520996\n",
      "Epoch 761, Batch 0, Loss: 0.693818986415863\n",
      "Epoch 762, Batch 0, Loss: 0.6952780485153198\n",
      "Epoch 763, Batch 0, Loss: 0.6897057294845581\n",
      "Epoch 764, Batch 0, Loss: 0.6930122375488281\n",
      "Epoch 765, Batch 0, Loss: 0.6949043273925781\n",
      "Epoch 766, Batch 0, Loss: 0.6933302879333496\n",
      "Epoch 767, Batch 0, Loss: 0.6961022615432739\n",
      "Epoch 768, Batch 0, Loss: 0.7003705501556396\n",
      "Epoch 769, Batch 0, Loss: 0.6929910778999329\n",
      "Epoch 770, Batch 0, Loss: 0.6929877400398254\n",
      "Epoch 771, Batch 0, Loss: 0.6933485269546509\n",
      "Epoch 772, Batch 0, Loss: 0.6859549880027771\n",
      "Epoch 773, Batch 0, Loss: 0.7003777027130127\n",
      "Epoch 774, Batch 0, Loss: 0.6903015375137329\n",
      "Epoch 775, Batch 0, Loss: 0.7003836035728455\n",
      "Epoch 776, Batch 0, Loss: 0.6859388947486877\n",
      "Epoch 777, Batch 0, Loss: 0.6919728517532349\n",
      "Epoch 778, Batch 0, Loss: 0.685931384563446\n",
      "Epoch 779, Batch 0, Loss: 0.6933509707450867\n",
      "Epoch 780, Batch 0, Loss: 0.6916268467903137\n",
      "Epoch 781, Batch 0, Loss: 0.6932422518730164\n",
      "Epoch 782, Batch 0, Loss: 0.6947392225265503\n",
      "Epoch 783, Batch 0, Loss: 0.6929560899734497\n",
      "Epoch 784, Batch 0, Loss: 0.6929527521133423\n",
      "Epoch 785, Batch 0, Loss: 0.6859531998634338\n",
      "Epoch 786, Batch 0, Loss: 0.6929459571838379\n",
      "Epoch 787, Batch 0, Loss: 0.6933802962303162\n",
      "Epoch 788, Batch 0, Loss: 0.6859465837478638\n",
      "Epoch 789, Batch 0, Loss: 0.6933859586715698\n",
      "Epoch 790, Batch 0, Loss: 0.6933889389038086\n",
      "Epoch 791, Batch 0, Loss: 0.7003796100616455\n",
      "Epoch 792, Batch 0, Loss: 0.6929249167442322\n",
      "Epoch 793, Batch 0, Loss: 0.6933978796005249\n",
      "Epoch 794, Batch 0, Loss: 0.6934010982513428\n",
      "Epoch 795, Batch 0, Loss: 0.6916136145591736\n",
      "Epoch 796, Batch 0, Loss: 0.6929212212562561\n",
      "Epoch 797, Batch 0, Loss: 0.6929168701171875\n",
      "Epoch 798, Batch 0, Loss: 0.700392484664917\n",
      "Epoch 799, Batch 0, Loss: 0.7003931999206543\n",
      "Epoch 800, Batch 0, Loss: 0.6929784417152405\n",
      "Epoch 801, Batch 0, Loss: 0.69340580701828\n",
      "Epoch 802, Batch 0, Loss: 0.7004009485244751\n",
      "Epoch 803, Batch 0, Loss: 0.6940972208976746\n",
      "Epoch 804, Batch 0, Loss: 0.6953885555267334\n",
      "Epoch 805, Batch 0, Loss: 0.6965849995613098\n",
      "Epoch 806, Batch 0, Loss: 0.6886461973190308\n",
      "Epoch 807, Batch 0, Loss: 0.6904743909835815\n",
      "Epoch 808, Batch 0, Loss: 0.6941466331481934\n",
      "Epoch 809, Batch 0, Loss: 0.7003990411758423\n",
      "Epoch 810, Batch 0, Loss: 0.6934180855751038\n",
      "Epoch 811, Batch 0, Loss: 0.6859205365180969\n",
      "Epoch 812, Batch 0, Loss: 0.6859200596809387\n",
      "Epoch 813, Batch 0, Loss: 0.694654107093811\n",
      "Epoch 814, Batch 0, Loss: 0.6934280395507812\n",
      "Epoch 815, Batch 0, Loss: 0.6859167814254761\n",
      "Epoch 816, Batch 0, Loss: 0.6859163045883179\n",
      "Epoch 817, Batch 0, Loss: 0.6911277770996094\n",
      "Epoch 818, Batch 0, Loss: 0.6928625106811523\n",
      "Epoch 819, Batch 0, Loss: 0.6934734582901001\n",
      "Epoch 820, Batch 0, Loss: 0.6928666830062866\n",
      "Epoch 821, Batch 0, Loss: 0.691665530204773\n",
      "Epoch 822, Batch 0, Loss: 0.6932709217071533\n",
      "Epoch 823, Batch 0, Loss: 0.7003730535507202\n",
      "Epoch 824, Batch 0, Loss: 0.6928361654281616\n",
      "Epoch 825, Batch 0, Loss: 0.7003759145736694\n",
      "Epoch 826, Batch 0, Loss: 0.6945271492004395\n",
      "Epoch 827, Batch 0, Loss: 0.6928218007087708\n",
      "Epoch 828, Batch 0, Loss: 0.6928184628486633\n",
      "Epoch 829, Batch 0, Loss: 0.6928150653839111\n",
      "Epoch 830, Batch 0, Loss: 0.6859349608421326\n",
      "Epoch 831, Batch 0, Loss: 0.6935009956359863\n",
      "Epoch 832, Batch 0, Loss: 0.7003778219223022\n",
      "Epoch 833, Batch 0, Loss: 0.7003792524337769\n",
      "Epoch 834, Batch 0, Loss: 0.6859264969825745\n",
      "Epoch 835, Batch 0, Loss: 0.6949106454849243\n",
      "Epoch 836, Batch 0, Loss: 0.693524181842804\n",
      "Epoch 837, Batch 0, Loss: 0.6927838325500488\n",
      "Epoch 838, Batch 0, Loss: 0.6959149837493896\n",
      "Epoch 839, Batch 0, Loss: 0.7003731727600098\n",
      "Epoch 840, Batch 0, Loss: 0.7003748416900635\n",
      "Epoch 841, Batch 0, Loss: 0.6896147727966309\n",
      "Epoch 842, Batch 0, Loss: 0.6859248280525208\n",
      "Epoch 843, Batch 0, Loss: 0.694987952709198\n",
      "Epoch 844, Batch 0, Loss: 0.7003746032714844\n",
      "Epoch 845, Batch 0, Loss: 0.6935479640960693\n",
      "Epoch 846, Batch 0, Loss: 0.6859288811683655\n",
      "Epoch 847, Batch 0, Loss: 0.6927531957626343\n",
      "Epoch 848, Batch 0, Loss: 0.6906482577323914\n",
      "Epoch 849, Batch 0, Loss: 0.693545401096344\n",
      "Epoch 850, Batch 0, Loss: 0.6859129667282104\n",
      "Epoch 851, Batch 0, Loss: 0.6859116554260254\n",
      "Epoch 852, Batch 0, Loss: 0.7003897428512573\n",
      "Epoch 853, Batch 0, Loss: 0.6922693848609924\n",
      "Epoch 854, Batch 0, Loss: 0.6927472352981567\n",
      "Epoch 855, Batch 0, Loss: 0.6927427053451538\n",
      "Epoch 856, Batch 0, Loss: 0.6859003901481628\n",
      "Epoch 857, Batch 0, Loss: 0.6858994960784912\n",
      "Epoch 858, Batch 0, Loss: 0.7004001140594482\n",
      "Epoch 859, Batch 0, Loss: 0.6927245855331421\n",
      "Epoch 860, Batch 0, Loss: 0.6930166482925415\n",
      "Epoch 861, Batch 0, Loss: 0.6927106380462646\n",
      "Epoch 862, Batch 0, Loss: 0.693590521812439\n",
      "Epoch 863, Batch 0, Loss: 0.6859021186828613\n",
      "Epoch 864, Batch 0, Loss: 0.692699134349823\n",
      "Epoch 865, Batch 0, Loss: 0.6858986020088196\n",
      "Epoch 866, Batch 0, Loss: 0.6920188665390015\n",
      "Epoch 867, Batch 0, Loss: 0.6934528350830078\n",
      "Epoch 868, Batch 0, Loss: 0.6858868598937988\n",
      "Epoch 869, Batch 0, Loss: 0.685886025428772\n",
      "Epoch 870, Batch 0, Loss: 0.6926963329315186\n",
      "Epoch 871, Batch 0, Loss: 0.6919807195663452\n",
      "Epoch 872, Batch 0, Loss: 0.69361412525177\n",
      "Epoch 873, Batch 0, Loss: 0.6927156448364258\n",
      "Epoch 874, Batch 0, Loss: 0.6921977996826172\n",
      "Epoch 875, Batch 0, Loss: 0.6937567591667175\n",
      "Epoch 876, Batch 0, Loss: 0.6926637887954712\n",
      "Epoch 877, Batch 0, Loss: 0.7003911733627319\n",
      "Epoch 878, Batch 0, Loss: 0.7003928422927856\n",
      "Epoch 879, Batch 0, Loss: 0.68589848279953\n",
      "Epoch 880, Batch 0, Loss: 0.694801926612854\n",
      "Epoch 881, Batch 0, Loss: 0.6936544179916382\n",
      "Epoch 882, Batch 0, Loss: 0.6936568021774292\n",
      "Epoch 883, Batch 0, Loss: 0.7003929615020752\n",
      "Epoch 884, Batch 0, Loss: 0.7003946304321289\n",
      "Epoch 885, Batch 0, Loss: 0.6936644911766052\n",
      "Epoch 886, Batch 0, Loss: 0.6936674118041992\n",
      "Epoch 887, Batch 0, Loss: 0.695285439491272\n",
      "Epoch 888, Batch 0, Loss: 0.7003941535949707\n",
      "Epoch 889, Batch 0, Loss: 0.700395941734314\n",
      "Epoch 890, Batch 0, Loss: 0.690102219581604\n",
      "Epoch 891, Batch 0, Loss: 0.6946172714233398\n",
      "Epoch 892, Batch 0, Loss: 0.6936860084533691\n",
      "Epoch 893, Batch 0, Loss: 0.6936885714530945\n",
      "Epoch 894, Batch 0, Loss: 0.6905996799468994\n",
      "Epoch 895, Batch 0, Loss: 0.692131757736206\n",
      "Epoch 896, Batch 0, Loss: 0.6858764886856079\n",
      "Epoch 897, Batch 0, Loss: 0.6926077604293823\n",
      "Epoch 898, Batch 0, Loss: 0.6928764581680298\n",
      "Epoch 899, Batch 0, Loss: 0.6919034719467163\n",
      "Epoch 900, Batch 0, Loss: 0.7004084587097168\n",
      "Epoch 901, Batch 0, Loss: 0.6932883262634277\n",
      "Epoch 902, Batch 0, Loss: 0.6926038265228271\n",
      "Epoch 903, Batch 0, Loss: 0.6918097138404846\n",
      "Epoch 904, Batch 0, Loss: 0.685877799987793\n",
      "Epoch 905, Batch 0, Loss: 0.6934245824813843\n",
      "Epoch 906, Batch 0, Loss: 0.7003977298736572\n",
      "Epoch 907, Batch 0, Loss: 0.7003993988037109\n",
      "Epoch 908, Batch 0, Loss: 0.6947101354598999\n",
      "Epoch 909, Batch 0, Loss: 0.696080207824707\n",
      "Epoch 910, Batch 0, Loss: 0.7003898620605469\n",
      "Epoch 911, Batch 0, Loss: 0.6970754265785217\n",
      "Epoch 912, Batch 0, Loss: 0.7003883123397827\n",
      "Epoch 913, Batch 0, Loss: 0.688368558883667\n",
      "Epoch 914, Batch 0, Loss: 0.6925463080406189\n",
      "Epoch 915, Batch 0, Loss: 0.6960374712944031\n",
      "Epoch 916, Batch 0, Loss: 0.6925380229949951\n",
      "Epoch 917, Batch 0, Loss: 0.6892524361610413\n",
      "Epoch 918, Batch 0, Loss: 0.6937428712844849\n",
      "Epoch 919, Batch 0, Loss: 0.6909899711608887\n",
      "Epoch 920, Batch 0, Loss: 0.7004022598266602\n",
      "Epoch 921, Batch 0, Loss: 0.6924983859062195\n",
      "Epoch 922, Batch 0, Loss: 0.6858683228492737\n",
      "Epoch 923, Batch 0, Loss: 0.7004110217094421\n",
      "Epoch 924, Batch 0, Loss: 0.6936943531036377\n",
      "Epoch 925, Batch 0, Loss: 0.7004194855690002\n",
      "Epoch 926, Batch 0, Loss: 0.6948206424713135\n",
      "Epoch 927, Batch 0, Loss: 0.6902481317520142\n",
      "Epoch 928, Batch 0, Loss: 0.7004131078720093\n",
      "Epoch 929, Batch 0, Loss: 0.6937515139579773\n",
      "Epoch 930, Batch 0, Loss: 0.6858620643615723\n",
      "Epoch 931, Batch 0, Loss: 0.6937602758407593\n",
      "Epoch 932, Batch 0, Loss: 0.6924285292625427\n",
      "Epoch 933, Batch 0, Loss: 0.69395911693573\n",
      "Epoch 934, Batch 0, Loss: 0.6858775615692139\n",
      "Epoch 935, Batch 0, Loss: 0.695264995098114\n",
      "Epoch 936, Batch 0, Loss: 0.6937987804412842\n",
      "Epoch 937, Batch 0, Loss: 0.6924780607223511\n",
      "Epoch 938, Batch 0, Loss: 0.6961836814880371\n",
      "Epoch 939, Batch 0, Loss: 0.6924698352813721\n",
      "Epoch 940, Batch 0, Loss: 0.6891232132911682\n",
      "Epoch 941, Batch 0, Loss: 0.6924699544906616\n",
      "Epoch 942, Batch 0, Loss: 0.693806529045105\n",
      "Epoch 943, Batch 0, Loss: 0.6924625635147095\n",
      "Epoch 944, Batch 0, Loss: 0.6951497793197632\n",
      "Epoch 945, Batch 0, Loss: 0.7004008293151855\n",
      "Epoch 946, Batch 0, Loss: 0.6962713003158569\n",
      "Epoch 947, Batch 0, Loss: 0.6975202560424805\n",
      "Epoch 948, Batch 0, Loss: 0.6938512325286865\n",
      "Epoch 949, Batch 0, Loss: 0.6879668235778809\n",
      "Epoch 950, Batch 0, Loss: 0.6924395561218262\n",
      "Epoch 951, Batch 0, Loss: 0.6858717799186707\n",
      "Epoch 952, Batch 0, Loss: 0.6924322843551636\n",
      "Epoch 953, Batch 0, Loss: 0.6902662515640259\n",
      "Epoch 954, Batch 0, Loss: 0.7004109025001526\n",
      "Epoch 955, Batch 0, Loss: 0.7004126310348511\n",
      "Epoch 956, Batch 0, Loss: 0.6924259662628174\n",
      "Epoch 957, Batch 0, Loss: 0.6924219727516174\n",
      "Epoch 958, Batch 0, Loss: 0.6924180388450623\n",
      "Epoch 959, Batch 0, Loss: 0.6858439445495605\n",
      "Epoch 960, Batch 0, Loss: 0.6858419179916382\n",
      "Epoch 961, Batch 0, Loss: 0.6938579082489014\n",
      "Epoch 962, Batch 0, Loss: 0.6858376860618591\n",
      "Epoch 963, Batch 0, Loss: 0.7004272937774658\n",
      "Epoch 964, Batch 0, Loss: 0.692194402217865\n",
      "Epoch 965, Batch 0, Loss: 0.6926660537719727\n",
      "Epoch 966, Batch 0, Loss: 0.6858360171318054\n",
      "Epoch 967, Batch 0, Loss: 0.693875789642334\n",
      "Epoch 968, Batch 0, Loss: 0.6923816204071045\n",
      "Epoch 969, Batch 0, Loss: 0.7004315853118896\n",
      "Epoch 970, Batch 0, Loss: 0.6922032833099365\n",
      "Epoch 971, Batch 0, Loss: 0.693880558013916\n",
      "Epoch 972, Batch 0, Loss: 0.6927441358566284\n",
      "Epoch 973, Batch 0, Loss: 0.6920142769813538\n",
      "Epoch 974, Batch 0, Loss: 0.69283127784729\n",
      "Epoch 975, Batch 0, Loss: 0.69432532787323\n",
      "Epoch 976, Batch 0, Loss: 0.6905511617660522\n",
      "Epoch 977, Batch 0, Loss: 0.6941885948181152\n",
      "Epoch 978, Batch 0, Loss: 0.7004203796386719\n",
      "Epoch 979, Batch 0, Loss: 0.695440411567688\n",
      "Epoch 980, Batch 0, Loss: 0.6939234733581543\n",
      "Epoch 981, Batch 0, Loss: 0.6897328495979309\n",
      "Epoch 982, Batch 0, Loss: 0.6923427581787109\n",
      "Epoch 983, Batch 0, Loss: 0.7004262208938599\n",
      "Epoch 984, Batch 0, Loss: 0.6914927959442139\n",
      "Epoch 985, Batch 0, Loss: 0.7004340887069702\n",
      "Epoch 986, Batch 0, Loss: 0.6923368573188782\n",
      "Epoch 987, Batch 0, Loss: 0.6858164072036743\n",
      "Epoch 988, Batch 0, Loss: 0.6923278570175171\n",
      "Epoch 989, Batch 0, Loss: 0.7004408240318298\n",
      "Epoch 990, Batch 0, Loss: 0.6923190355300903\n",
      "Epoch 991, Batch 0, Loss: 0.6939379572868347\n",
      "Epoch 992, Batch 0, Loss: 0.692310094833374\n",
      "Epoch 993, Batch 0, Loss: 0.693946123123169\n",
      "Epoch 994, Batch 0, Loss: 0.6939501762390137\n",
      "Epoch 995, Batch 0, Loss: 0.6927143931388855\n",
      "Epoch 996, Batch 0, Loss: 0.6940737366676331\n",
      "Epoch 997, Batch 0, Loss: 0.6857913732528687\n",
      "Epoch 998, Batch 0, Loss: 0.6923062801361084\n",
      "Epoch 999, Batch 0, Loss: 0.6949349641799927\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 定义数据集类\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "# 定义神经网络模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_x, n_h)\n",
    "        self.layer2 = nn.Linear(n_h, n_y)\n",
    "\n",
    "    def forward(self, X):\n",
    "        A1 = torch.sigmoid(self.layer1(X))\n",
    "        A2 = torch.sigmoid(self.layer2(A1))\n",
    "        return A2\n",
    "\n",
    "# 初始化模型\n",
    "def initialize_model(n_x, n_h, n_y):\n",
    "    model = SimpleNN(n_x, n_h, n_y)\n",
    "    return model\n",
    "\n",
    "# 计算损失\n",
    "def compute_loss(model, X, Y):\n",
    "        output = model(X)\n",
    "        loss = F.binary_cross_entropy(output, Y)\n",
    "        return loss\n",
    "\n",
    "# 训练模型\n",
    "def train(model, dataloader, learning_rate, epochs):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (X_batch, Y_batch) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = compute_loss(model, X_batch, Y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "\n",
    "# 示例数据\n",
    "X = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float32)\n",
    "Y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# 创建数据集\n",
    "dataset = SimpleDataset(X, Y)\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 2  # 可以调整批量大小\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 初始化模型\n",
    "n_x = X.shape[1]\n",
    "n_h = 2\n",
    "n_y = 1\n",
    "model = initialize_model(n_x, n_h, n_y)\n",
    "\n",
    "# 训练模型\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "train(model, dataloader, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using nn.parameter\n",
    "\n",
    "在这个重构的代码中，SimpleNN类中的_initialize_weights方法使用nn.init模块来初始化权重和偏置。nn.init.xavier_uniform_是一个常用的权重初始化方法，它适用于激活函数是Sigmoid或Tanh的情况。nn.init.zeros_用于将偏置初始化为零。\n",
    "\n",
    "请注意，我们没有显式地将self.linear1.weight和self.linear1.bias注册为nn.Parameter，因为在PyTorch中，nn.Linear的权重和偏置默认就是nn.Parameter。这意味着它们会自动被优化器跟踪和更新。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 定义数据集类\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "# 定义神经网络模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.n_h = n_h\n",
    "        self.linear1 = nn.Linear(n_x, n_h)\n",
    "        self.linear2 = nn.Linear(n_h, n_y)\n",
    "        # 初始化权重和偏置\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, X):\n",
    "        A1 = torch.sigmoid(self.linear1(X))\n",
    "        A2 = torch.sigmoid(self.linear2(A1))\n",
    "        return A2\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # 使用nn.init来初始化权重和偏置\n",
    "        nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        nn.init.zeros_(self.linear1.bias)\n",
    "        nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        nn.init.zeros_(self.linear2.bias)\n",
    "\n",
    "# 初始化模型\n",
    "def initialize_model(n_x, n_h, n_y):\n",
    "    model = SimpleNN(n_x, n_h, n_y)\n",
    "    return model\n",
    "\n",
    "# 计算损失\n",
    "def compute_loss(output, Y):\n",
    "    loss = F.binary_cross_entropy(output, Y)\n",
    "    return loss\n",
    "\n",
    "# 训练模型\n",
    "def train(model, dataloader, learning_rate, epochs):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        for X_batch, Y_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = compute_loss(output, Y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# 示例数据\n",
    "X = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float32)\n",
    "Y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# 创建数据集\n",
    "dataset = SimpleDataset(X, Y)\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 2  # 可以调整批量大小\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 初始化模型\n",
    "n_x = X.shape[1]\n",
    "n_h = 2\n",
    "n_y = 1\n",
    "model = initialize_model(n_x, n_h, n_y)\n",
    "\n",
    "# 训练模型\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "train(model, dataloader, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add validation \n",
    "\n",
    "在这个示例中，我们首先使用random_split函数将原始数据集随机分割为训练集和验证集。然后，我们创建了两个DataLoader：一个用于训练，另一个用于验证。\n",
    "\n",
    "在训练循环中，我们首先将模型设置为训练模式，然后执行前向传播、损失计算、反向传播和参数更新。在每个epoch结束时，我们将模型设置为评估模式，并计算验证集上的损失和准确率。注意，在评估模式下，我们使用torch.no_grad()上下文管理器来禁用梯度计算，这样可以节省内存并防止影响模型评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss 0.6307094097137451, Validation Loss 0.6307094097137451, Validation Accuracy 0.0000\n",
      "Epoch 1: Training Loss 0.6739773750305176, Validation Loss 0.6739773750305176, Validation Accuracy 0.0000\n",
      "Epoch 2: Training Loss 0.7157135009765625, Validation Loss 0.7157135009765625, Validation Accuracy 0.0000\n",
      "Epoch 3: Training Loss 0.7572778463363647, Validation Loss 0.7572778463363647, Validation Accuracy 0.0000\n",
      "Epoch 4: Training Loss 0.750220000743866, Validation Loss 0.750220000743866, Validation Accuracy 0.0000\n",
      "Epoch 5: Training Loss 0.7902514934539795, Validation Loss 0.7902514934539795, Validation Accuracy 0.0000\n",
      "Epoch 6: Training Loss 0.8283535242080688, Validation Loss 0.8283535242080688, Validation Accuracy 0.0000\n",
      "Epoch 7: Training Loss 0.8154942393302917, Validation Loss 0.8154942393302917, Validation Accuracy 0.0000\n",
      "Epoch 8: Training Loss 0.8037827014923096, Validation Loss 0.8037827014923096, Validation Accuracy 0.0000\n",
      "Epoch 9: Training Loss 0.7931150794029236, Validation Loss 0.7931150794029236, Validation Accuracy 0.0000\n",
      "Epoch 10: Training Loss 0.7833968997001648, Validation Loss 0.7833968997001648, Validation Accuracy 0.0000\n",
      "Epoch 11: Training Loss 0.8206692337989807, Validation Loss 0.8206692337989807, Validation Accuracy 0.0000\n",
      "Epoch 12: Training Loss 0.8561579585075378, Validation Loss 0.8561579585075378, Validation Accuracy 0.0000\n",
      "Epoch 13: Training Loss 0.8412774205207825, Validation Loss 0.8412774205207825, Validation Accuracy 0.0000\n",
      "Epoch 14: Training Loss 0.827721893787384, Validation Loss 0.827721893787384, Validation Accuracy 0.0000\n",
      "Epoch 15: Training Loss 0.8153712749481201, Validation Loss 0.8153712749481201, Validation Accuracy 0.0000\n",
      "Epoch 16: Training Loss 0.8525564074516296, Validation Loss 0.8525564074516296, Validation Accuracy 0.0000\n",
      "Epoch 17: Training Loss 0.8382071256637573, Validation Loss 0.8382071256637573, Validation Accuracy 0.0000\n",
      "Epoch 18: Training Loss 0.8730321526527405, Validation Loss 0.8730321526527405, Validation Accuracy 0.0000\n",
      "Epoch 19: Training Loss 0.9060955047607422, Validation Loss 0.9060955047607422, Validation Accuracy 0.0000\n",
      "Epoch 20: Training Loss 0.9374347925186157, Validation Loss 0.9374347925186157, Validation Accuracy 0.0000\n",
      "Epoch 21: Training Loss 0.9671008586883545, Validation Loss 0.9671008586883545, Validation Accuracy 0.0000\n",
      "Epoch 22: Training Loss 0.9961652159690857, Validation Loss 0.9961652159690857, Validation Accuracy 0.0000\n",
      "Epoch 23: Training Loss 1.022618293762207, Validation Loss 1.022618293762207, Validation Accuracy 0.0000\n",
      "Epoch 24: Training Loss 1.0485069751739502, Validation Loss 1.0485069751739502, Validation Accuracy 0.0000\n",
      "Epoch 25: Training Loss 1.017716884613037, Validation Loss 1.017716884613037, Validation Accuracy 0.0000\n",
      "Epoch 26: Training Loss 1.0430772304534912, Validation Loss 1.0430772304534912, Validation Accuracy 0.0000\n",
      "Epoch 27: Training Loss 1.067016839981079, Validation Loss 1.067016839981079, Validation Accuracy 0.0000\n",
      "Epoch 28: Training Loss 1.034829020500183, Validation Loss 1.034829020500183, Validation Accuracy 0.0000\n",
      "Epoch 29: Training Loss 1.0593440532684326, Validation Loss 1.0593440532684326, Validation Accuracy 0.0000\n",
      "Epoch 30: Training Loss 1.082481861114502, Validation Loss 1.082481861114502, Validation Accuracy 0.0000\n",
      "Epoch 31: Training Loss 1.04916512966156, Validation Loss 1.04916512966156, Validation Accuracy 0.0000\n",
      "Epoch 32: Training Loss 1.0738794803619385, Validation Loss 1.0738794803619385, Validation Accuracy 0.0000\n",
      "Epoch 33: Training Loss 1.0414369106292725, Validation Loss 1.0414369106292725, Validation Accuracy 0.0000\n",
      "Epoch 34: Training Loss 1.0658177137374878, Validation Loss 1.0658177137374878, Validation Accuracy 0.0000\n",
      "Epoch 35: Training Loss 1.0896937847137451, Validation Loss 1.0896937847137451, Validation Accuracy 0.0000\n",
      "Epoch 36: Training Loss 1.111364483833313, Validation Loss 1.111364483833313, Validation Accuracy 0.0000\n",
      "Epoch 37: Training Loss 1.0759601593017578, Validation Loss 1.0759601593017578, Validation Accuracy 0.0000\n",
      "Epoch 38: Training Loss 1.0985385179519653, Validation Loss 1.0985385179519653, Validation Accuracy 0.0000\n",
      "Epoch 39: Training Loss 1.0643832683563232, Validation Loss 1.0643832683563232, Validation Accuracy 0.0000\n",
      "Epoch 40: Training Loss 1.0877392292022705, Validation Loss 1.0877392292022705, Validation Accuracy 0.0000\n",
      "Epoch 41: Training Loss 1.054657220840454, Validation Loss 1.054657220840454, Validation Accuracy 0.0000\n",
      "Epoch 42: Training Loss 1.0244957208633423, Validation Loss 1.0244957208633423, Validation Accuracy 0.0000\n",
      "Epoch 43: Training Loss 1.0512622594833374, Validation Loss 1.0512622594833374, Validation Accuracy 0.0000\n",
      "Epoch 44: Training Loss 1.0755895376205444, Validation Loss 1.0755895376205444, Validation Accuracy 0.0000\n",
      "Epoch 45: Training Loss 1.0994312763214111, Validation Loss 1.0994312763214111, Validation Accuracy 0.0000\n",
      "Epoch 46: Training Loss 1.0656802654266357, Validation Loss 1.0656802654266357, Validation Accuracy 0.0000\n",
      "Epoch 47: Training Loss 1.0902315378189087, Validation Loss 1.0902315378189087, Validation Accuracy 0.0000\n",
      "Epoch 48: Training Loss 1.1125175952911377, Validation Loss 1.1125175952911377, Validation Accuracy 0.0000\n",
      "Epoch 49: Training Loss 1.1335529088974, Validation Loss 1.1335529088974, Validation Accuracy 0.0000\n",
      "Epoch 50: Training Loss 1.097159504890442, Validation Loss 1.097159504890442, Validation Accuracy 0.0000\n",
      "Epoch 51: Training Loss 1.1200588941574097, Validation Loss 1.1200588941574097, Validation Accuracy 0.0000\n",
      "Epoch 52: Training Loss 1.084961175918579, Validation Loss 1.084961175918579, Validation Accuracy 0.0000\n",
      "Epoch 53: Training Loss 1.1087149381637573, Validation Loss 1.1087149381637573, Validation Accuracy 0.0000\n",
      "Epoch 54: Training Loss 1.1311073303222656, Validation Loss 1.1311073303222656, Validation Accuracy 0.0000\n",
      "Epoch 55: Training Loss 1.1522141695022583, Validation Loss 1.1522141695022583, Validation Accuracy 0.0000\n",
      "Epoch 56: Training Loss 1.114637851715088, Validation Loss 1.114637851715088, Validation Accuracy 0.0000\n",
      "Epoch 57: Training Loss 1.1360281705856323, Validation Loss 1.1360281705856323, Validation Accuracy 0.0000\n",
      "Epoch 58: Training Loss 1.1570289134979248, Validation Loss 1.1570289134979248, Validation Accuracy 0.0000\n",
      "Epoch 59: Training Loss 1.1768255233764648, Validation Loss 1.1768255233764648, Validation Accuracy 0.0000\n",
      "Epoch 60: Training Loss 1.194743037223816, Validation Loss 1.194743037223816, Validation Accuracy 0.0000\n",
      "Epoch 61: Training Loss 1.2116597890853882, Validation Loss 1.2116597890853882, Validation Accuracy 0.0000\n",
      "Epoch 62: Training Loss 1.2276345491409302, Validation Loss 1.2276345491409302, Validation Accuracy 0.0000\n",
      "Epoch 63: Training Loss 1.1841952800750732, Validation Loss 1.1841952800750732, Validation Accuracy 0.0000\n",
      "Epoch 64: Training Loss 1.202673316001892, Validation Loss 1.202673316001892, Validation Accuracy 0.0000\n",
      "Epoch 65: Training Loss 1.2193794250488281, Validation Loss 1.2193794250488281, Validation Accuracy 0.0000\n",
      "Epoch 66: Training Loss 1.176878809928894, Validation Loss 1.176878809928894, Validation Accuracy 0.0000\n",
      "Epoch 67: Training Loss 1.1952416896820068, Validation Loss 1.1952416896820068, Validation Accuracy 0.0000\n",
      "Epoch 68: Training Loss 1.1549546718597412, Validation Loss 1.1549546718597412, Validation Accuracy 0.0000\n",
      "Epoch 69: Training Loss 1.1181721687316895, Validation Loss 1.1181721687316895, Validation Accuracy 0.0000\n",
      "Epoch 70: Training Loss 1.1401759386062622, Validation Loss 1.1401759386062622, Validation Accuracy 0.0000\n",
      "Epoch 71: Training Loss 1.1048178672790527, Validation Loss 1.1048178672790527, Validation Accuracy 0.0000\n",
      "Epoch 72: Training Loss 1.1277272701263428, Validation Loss 1.1277272701263428, Validation Accuracy 0.0000\n",
      "Epoch 73: Training Loss 1.1493699550628662, Validation Loss 1.1493699550628662, Validation Accuracy 0.0000\n",
      "Epoch 74: Training Loss 1.1706486940383911, Validation Loss 1.1706486940383911, Validation Accuracy 0.0000\n",
      "Epoch 75: Training Loss 1.1899179220199585, Validation Loss 1.1899179220199585, Validation Accuracy 0.0000\n",
      "Epoch 76: Training Loss 1.150757908821106, Validation Loss 1.150757908821106, Validation Accuracy 0.0000\n",
      "Epoch 77: Training Loss 1.1721751689910889, Validation Loss 1.1721751689910889, Validation Accuracy 0.0000\n",
      "Epoch 78: Training Loss 1.19157075881958, Validation Loss 1.19157075881958, Validation Accuracy 0.0000\n",
      "Epoch 79: Training Loss 1.1525262594223022, Validation Loss 1.1525262594223022, Validation Accuracy 0.0000\n",
      "Epoch 80: Training Loss 1.1732159852981567, Validation Loss 1.1732159852981567, Validation Accuracy 0.0000\n",
      "Epoch 81: Training Loss 1.19358491897583, Validation Loss 1.19358491897583, Validation Accuracy 0.0000\n",
      "Epoch 82: Training Loss 1.2128074169158936, Validation Loss 1.2128074169158936, Validation Accuracy 0.0000\n",
      "Epoch 83: Training Loss 1.2301841974258423, Validation Loss 1.2301841974258423, Validation Accuracy 0.0000\n",
      "Epoch 84: Training Loss 1.1883208751678467, Validation Loss 1.1883208751678467, Validation Accuracy 0.0000\n",
      "Epoch 85: Training Loss 1.2081021070480347, Validation Loss 1.2081021070480347, Validation Accuracy 0.0000\n",
      "Epoch 86: Training Loss 1.2259904146194458, Validation Loss 1.2259904146194458, Validation Accuracy 0.0000\n",
      "Epoch 87: Training Loss 1.2436590194702148, Validation Loss 1.2436590194702148, Validation Accuracy 0.0000\n",
      "Epoch 88: Training Loss 1.2010223865509033, Validation Loss 1.2010223865509033, Validation Accuracy 0.0000\n",
      "Epoch 89: Training Loss 1.2195676565170288, Validation Loss 1.2195676565170288, Validation Accuracy 0.0000\n",
      "Epoch 90: Training Loss 1.2370983362197876, Validation Loss 1.2370983362197876, Validation Accuracy 0.0000\n",
      "Epoch 91: Training Loss 1.254430890083313, Validation Loss 1.254430890083313, Validation Accuracy 0.0000\n",
      "Epoch 92: Training Loss 1.2708028554916382, Validation Loss 1.2708028554916382, Validation Accuracy 0.0000\n",
      "Epoch 93: Training Loss 1.2862720489501953, Validation Loss 1.2862720489501953, Validation Accuracy 0.0000\n",
      "Epoch 94: Training Loss 1.2406351566314697, Validation Loss 1.2406351566314697, Validation Accuracy 0.0000\n",
      "Epoch 95: Training Loss 1.1989185810089111, Validation Loss 1.1989185810089111, Validation Accuracy 0.0000\n",
      "Epoch 96: Training Loss 1.1608108282089233, Validation Loss 1.1608108282089233, Validation Accuracy 0.0000\n",
      "Epoch 97: Training Loss 1.1823835372924805, Validation Loss 1.1823835372924805, Validation Accuracy 0.0000\n",
      "Epoch 98: Training Loss 1.1458748579025269, Validation Loss 1.1458748579025269, Validation Accuracy 0.0000\n",
      "Epoch 99: Training Loss 1.1684844493865967, Validation Loss 1.1684844493865967, Validation Accuracy 0.0000\n",
      "Epoch 100: Training Loss 1.189864993095398, Validation Loss 1.189864993095398, Validation Accuracy 0.0000\n",
      "Epoch 101: Training Loss 1.210082769393921, Validation Loss 1.210082769393921, Validation Accuracy 0.0000\n",
      "Epoch 102: Training Loss 1.1716687679290771, Validation Loss 1.1716687679290771, Validation Accuracy 0.0000\n",
      "Epoch 103: Training Loss 1.1940302848815918, Validation Loss 1.1940302848815918, Validation Accuracy 0.0000\n",
      "Epoch 104: Training Loss 1.1571677923202515, Validation Loss 1.1571677923202515, Validation Accuracy 0.0000\n",
      "Epoch 105: Training Loss 1.1796579360961914, Validation Loss 1.1796579360961914, Validation Accuracy 0.0000\n",
      "Epoch 106: Training Loss 1.2009307146072388, Validation Loss 1.2009307146072388, Validation Accuracy 0.0000\n",
      "Epoch 107: Training Loss 1.2210524082183838, Validation Loss 1.2210524082183838, Validation Accuracy 0.0000\n",
      "Epoch 108: Training Loss 1.1823700666427612, Validation Loss 1.1823700666427612, Validation Accuracy 0.0000\n",
      "Epoch 109: Training Loss 1.1470448970794678, Validation Loss 1.1470448970794678, Validation Accuracy 0.0000\n",
      "Epoch 110: Training Loss 1.171555995941162, Validation Loss 1.171555995941162, Validation Accuracy 0.0000\n",
      "Epoch 111: Training Loss 1.1373533010482788, Validation Loss 1.1373533010482788, Validation Accuracy 0.0000\n",
      "Epoch 112: Training Loss 1.1626492738723755, Validation Loss 1.1626492738723755, Validation Accuracy 0.0000\n",
      "Epoch 113: Training Loss 1.1855989694595337, Validation Loss 1.1855989694595337, Validation Accuracy 0.0000\n",
      "Epoch 114: Training Loss 1.207313895225525, Validation Loss 1.207313895225525, Validation Accuracy 0.0000\n",
      "Epoch 115: Training Loss 1.2287648916244507, Validation Loss 1.2287648916244507, Validation Accuracy 0.0000\n",
      "Epoch 116: Training Loss 1.2481658458709717, Validation Loss 1.2481658458709717, Validation Accuracy 0.0000\n",
      "Epoch 117: Training Loss 1.267379641532898, Validation Loss 1.267379641532898, Validation Accuracy 0.0000\n",
      "Epoch 118: Training Loss 1.2259596586227417, Validation Loss 1.2259596586227417, Validation Accuracy 0.0000\n",
      "Epoch 119: Training Loss 1.188123345375061, Validation Loss 1.188123345375061, Validation Accuracy 0.0000\n",
      "Epoch 120: Training Loss 1.210329532623291, Validation Loss 1.210329532623291, Validation Accuracy 0.0000\n",
      "Epoch 121: Training Loss 1.231346845626831, Validation Loss 1.231346845626831, Validation Accuracy 0.0000\n",
      "Epoch 122: Training Loss 1.1934447288513184, Validation Loss 1.1934447288513184, Validation Accuracy 0.0000\n",
      "Epoch 123: Training Loss 1.158839464187622, Validation Loss 1.158839464187622, Validation Accuracy 0.0000\n",
      "Epoch 124: Training Loss 1.1272577047348022, Validation Loss 1.1272577047348022, Validation Accuracy 0.0000\n",
      "Epoch 125: Training Loss 1.0984445810317993, Validation Loss 1.0984445810317993, Validation Accuracy 0.0000\n",
      "Epoch 126: Training Loss 1.1275697946548462, Validation Loss 1.1275697946548462, Validation Accuracy 0.0000\n",
      "Epoch 127: Training Loss 1.155133843421936, Validation Loss 1.155133843421936, Validation Accuracy 0.0000\n",
      "Epoch 128: Training Loss 1.180165410041809, Validation Loss 1.180165410041809, Validation Accuracy 0.0000\n",
      "Epoch 129: Training Loss 1.1474343538284302, Validation Loss 1.1474343538284302, Validation Accuracy 0.0000\n",
      "Epoch 130: Training Loss 1.1175744533538818, Validation Loss 1.1175744533538818, Validation Accuracy 0.0000\n",
      "Epoch 131: Training Loss 1.1451033353805542, Validation Loss 1.1451033353805542, Validation Accuracy 0.0000\n",
      "Epoch 132: Training Loss 1.1157127618789673, Validation Loss 1.1157127618789673, Validation Accuracy 0.0000\n",
      "Epoch 133: Training Loss 1.1435881853103638, Validation Loss 1.1435881853103638, Validation Accuracy 0.0000\n",
      "Epoch 134: Training Loss 1.1711032390594482, Validation Loss 1.1711032390594482, Validation Accuracy 0.0000\n",
      "Epoch 135: Training Loss 1.197139024734497, Validation Loss 1.197139024734497, Validation Accuracy 0.0000\n",
      "Epoch 136: Training Loss 1.2217704057693481, Validation Loss 1.2217704057693481, Validation Accuracy 0.0000\n",
      "Epoch 137: Training Loss 1.2450711727142334, Validation Loss 1.2450711727142334, Validation Accuracy 0.0000\n",
      "Epoch 138: Training Loss 1.2671138048171997, Validation Loss 1.2671138048171997, Validation Accuracy 0.0000\n",
      "Epoch 139: Training Loss 1.2284924983978271, Validation Loss 1.2284924983978271, Validation Accuracy 0.0000\n",
      "Epoch 140: Training Loss 1.250823974609375, Validation Loss 1.250823974609375, Validation Accuracy 0.0000\n",
      "Epoch 141: Training Loss 1.2138723134994507, Validation Loss 1.2138723134994507, Validation Accuracy 0.0000\n",
      "Epoch 142: Training Loss 1.1801584959030151, Validation Loss 1.1801584959030151, Validation Accuracy 0.0000\n",
      "Epoch 143: Training Loss 1.2057074308395386, Validation Loss 1.2057074308395386, Validation Accuracy 0.0000\n",
      "Epoch 144: Training Loss 1.2299131155014038, Validation Loss 1.2299131155014038, Validation Accuracy 0.0000\n",
      "Epoch 145: Training Loss 1.2538750171661377, Validation Loss 1.2538750171661377, Validation Accuracy 0.0000\n",
      "Epoch 146: Training Loss 1.27555513381958, Validation Loss 1.27555513381958, Validation Accuracy 0.0000\n",
      "Epoch 147: Training Loss 1.2970677614212036, Validation Loss 1.2970677614212036, Validation Accuracy 0.0000\n",
      "Epoch 148: Training Loss 1.3164942264556885, Validation Loss 1.3164942264556885, Validation Accuracy 0.0000\n",
      "Epoch 149: Training Loss 1.3358267545700073, Validation Loss 1.3358267545700073, Validation Accuracy 0.0000\n",
      "Epoch 150: Training Loss 1.3532512187957764, Validation Loss 1.3532512187957764, Validation Accuracy 0.0000\n",
      "Epoch 151: Training Loss 1.3697842359542847, Validation Loss 1.3697842359542847, Validation Accuracy 0.0000\n",
      "Epoch 152: Training Loss 1.3854783773422241, Validation Loss 1.3854783773422241, Validation Accuracy 0.0000\n",
      "Epoch 153: Training Loss 1.4012179374694824, Validation Loss 1.4012179374694824, Validation Accuracy 0.0000\n",
      "Epoch 154: Training Loss 1.3539376258850098, Validation Loss 1.3539376258850098, Validation Accuracy 0.0000\n",
      "Epoch 155: Training Loss 1.3719316720962524, Validation Loss 1.3719316720962524, Validation Accuracy 0.0000\n",
      "Epoch 156: Training Loss 1.3889882564544678, Validation Loss 1.3889882564544678, Validation Accuracy 0.0000\n",
      "Epoch 157: Training Loss 1.4043247699737549, Validation Loss 1.4043247699737549, Validation Accuracy 0.0000\n",
      "Epoch 158: Training Loss 1.419723391532898, Validation Loss 1.419723391532898, Validation Accuracy 0.0000\n",
      "Epoch 159: Training Loss 1.433542251586914, Validation Loss 1.433542251586914, Validation Accuracy 0.0000\n",
      "Epoch 160: Training Loss 1.384665608406067, Validation Loss 1.384665608406067, Validation Accuracy 0.0000\n",
      "Epoch 161: Training Loss 1.3399946689605713, Validation Loss 1.3399946689605713, Validation Accuracy 0.0000\n",
      "Epoch 162: Training Loss 1.3590658903121948, Validation Loss 1.3590658903121948, Validation Accuracy 0.0000\n",
      "Epoch 163: Training Loss 1.3168972730636597, Validation Loss 1.3168972730636597, Validation Accuracy 0.0000\n",
      "Epoch 164: Training Loss 1.33869206905365, Validation Loss 1.33869206905365, Validation Accuracy 0.0000\n",
      "Epoch 165: Training Loss 1.298581600189209, Validation Loss 1.298581600189209, Validation Accuracy 0.0000\n",
      "Epoch 166: Training Loss 1.3208065032958984, Validation Loss 1.3208065032958984, Validation Accuracy 0.0000\n",
      "Epoch 167: Training Loss 1.2825812101364136, Validation Loss 1.2825812101364136, Validation Accuracy 0.0000\n",
      "Epoch 168: Training Loss 1.3061047792434692, Validation Loss 1.3061047792434692, Validation Accuracy 0.0000\n",
      "Epoch 169: Training Loss 1.3284142017364502, Validation Loss 1.3284142017364502, Validation Accuracy 0.0000\n",
      "Epoch 170: Training Loss 1.3506243228912354, Validation Loss 1.3506243228912354, Validation Accuracy 0.0000\n",
      "Epoch 171: Training Loss 1.3706556558609009, Validation Loss 1.3706556558609009, Validation Accuracy 0.0000\n",
      "Epoch 172: Training Loss 1.3896703720092773, Validation Loss 1.3896703720092773, Validation Accuracy 0.0000\n",
      "Epoch 173: Training Loss 1.4077280759811401, Validation Loss 1.4077280759811401, Validation Accuracy 0.0000\n",
      "Epoch 174: Training Loss 1.4248852729797363, Validation Loss 1.4248852729797363, Validation Accuracy 0.0000\n",
      "Epoch 175: Training Loss 1.441196084022522, Validation Loss 1.441196084022522, Validation Accuracy 0.0000\n",
      "Epoch 176: Training Loss 1.3949708938598633, Validation Loss 1.3949708938598633, Validation Accuracy 0.0000\n",
      "Epoch 177: Training Loss 1.3527610301971436, Validation Loss 1.3527610301971436, Validation Accuracy 0.0000\n",
      "Epoch 178: Training Loss 1.3750585317611694, Validation Loss 1.3750585317611694, Validation Accuracy 0.0000\n",
      "Epoch 179: Training Loss 1.3961894512176514, Validation Loss 1.3961894512176514, Validation Accuracy 0.0000\n",
      "Epoch 180: Training Loss 1.3545130491256714, Validation Loss 1.3545130491256714, Validation Accuracy 0.0000\n",
      "Epoch 181: Training Loss 1.3762353658676147, Validation Loss 1.3762353658676147, Validation Accuracy 0.0000\n",
      "Epoch 182: Training Loss 1.3968565464019775, Validation Loss 1.3968565464019775, Validation Accuracy 0.0000\n",
      "Epoch 183: Training Loss 1.3558344841003418, Validation Loss 1.3558344841003418, Validation Accuracy 0.0000\n",
      "Epoch 184: Training Loss 1.3792197704315186, Validation Loss 1.3792197704315186, Validation Accuracy 0.0000\n",
      "Epoch 185: Training Loss 1.4002907276153564, Validation Loss 1.4002907276153564, Validation Accuracy 0.0000\n",
      "Epoch 186: Training Loss 1.4213664531707764, Validation Loss 1.4213664531707764, Validation Accuracy 0.0000\n",
      "Epoch 187: Training Loss 1.4413505792617798, Validation Loss 1.4413505792617798, Validation Accuracy 0.0000\n",
      "Epoch 188: Training Loss 1.4603078365325928, Validation Loss 1.4603078365325928, Validation Accuracy 0.0000\n",
      "Epoch 189: Training Loss 1.4154424667358398, Validation Loss 1.4154424667358398, Validation Accuracy 0.0000\n",
      "Epoch 190: Training Loss 1.435420274734497, Validation Loss 1.435420274734497, Validation Accuracy 0.0000\n",
      "Epoch 191: Training Loss 1.3931103944778442, Validation Loss 1.3931103944778442, Validation Accuracy 0.0000\n",
      "Epoch 192: Training Loss 1.4159895181655884, Validation Loss 1.4159895181655884, Validation Accuracy 0.0000\n",
      "Epoch 193: Training Loss 1.4376733303070068, Validation Loss 1.4376733303070068, Validation Accuracy 0.0000\n",
      "Epoch 194: Training Loss 1.4572114944458008, Validation Loss 1.4572114944458008, Validation Accuracy 0.0000\n",
      "Epoch 195: Training Loss 1.4767855405807495, Validation Loss 1.4767855405807495, Validation Accuracy 0.0000\n",
      "Epoch 196: Training Loss 1.432316780090332, Validation Loss 1.432316780090332, Validation Accuracy 0.0000\n",
      "Epoch 197: Training Loss 1.4528744220733643, Validation Loss 1.4528744220733643, Validation Accuracy 0.0000\n",
      "Epoch 198: Training Loss 1.4734431505203247, Validation Loss 1.4734431505203247, Validation Accuracy 0.0000\n",
      "Epoch 199: Training Loss 1.4919687509536743, Validation Loss 1.4919687509536743, Validation Accuracy 0.0000\n",
      "Epoch 200: Training Loss 1.5105628967285156, Validation Loss 1.5105628967285156, Validation Accuracy 0.0000\n",
      "Epoch 201: Training Loss 1.5282208919525146, Validation Loss 1.5282208919525146, Validation Accuracy 0.0000\n",
      "Epoch 202: Training Loss 1.544114351272583, Validation Loss 1.544114351272583, Validation Accuracy 0.0000\n",
      "Epoch 203: Training Loss 1.559269905090332, Validation Loss 1.559269905090332, Validation Accuracy 0.0000\n",
      "Epoch 204: Training Loss 1.5737334489822388, Validation Loss 1.5737334489822388, Validation Accuracy 0.0000\n",
      "Epoch 205: Training Loss 1.5238006114959717, Validation Loss 1.5238006114959717, Validation Accuracy 0.0000\n",
      "Epoch 206: Training Loss 1.5409250259399414, Validation Loss 1.5409250259399414, Validation Accuracy 0.0000\n",
      "Epoch 207: Training Loss 1.5581837892532349, Validation Loss 1.5581837892532349, Validation Accuracy 0.0000\n",
      "Epoch 208: Training Loss 1.5103726387023926, Validation Loss 1.5103726387023926, Validation Accuracy 0.0000\n",
      "Epoch 209: Training Loss 1.466769814491272, Validation Loss 1.466769814491272, Validation Accuracy 0.0000\n",
      "Epoch 210: Training Loss 1.4895223379135132, Validation Loss 1.4895223379135132, Validation Accuracy 0.0000\n",
      "Epoch 211: Training Loss 1.448207974433899, Validation Loss 1.448207974433899, Validation Accuracy 0.0000\n",
      "Epoch 212: Training Loss 1.4715250730514526, Validation Loss 1.4715250730514526, Validation Accuracy 0.0000\n",
      "Epoch 213: Training Loss 1.493676781654358, Validation Loss 1.493676781654358, Validation Accuracy 0.0000\n",
      "Epoch 214: Training Loss 1.5158278942108154, Validation Loss 1.5158278942108154, Validation Accuracy 0.0000\n",
      "Epoch 215: Training Loss 1.5368314981460571, Validation Loss 1.5368314981460571, Validation Accuracy 0.0000\n",
      "Epoch 216: Training Loss 1.4931553602218628, Validation Loss 1.4931553602218628, Validation Accuracy 0.0000\n",
      "Epoch 217: Training Loss 1.5150704383850098, Validation Loss 1.5150704383850098, Validation Accuracy 0.0000\n",
      "Epoch 218: Training Loss 1.5369714498519897, Validation Loss 1.5369714498519897, Validation Accuracy 0.0000\n",
      "Epoch 219: Training Loss 1.4942331314086914, Validation Loss 1.4942331314086914, Validation Accuracy 0.0000\n",
      "Epoch 220: Training Loss 1.5169248580932617, Validation Loss 1.5169248580932617, Validation Accuracy 0.0000\n",
      "Epoch 221: Training Loss 1.476504921913147, Validation Loss 1.476504921913147, Validation Accuracy 0.0000\n",
      "Epoch 222: Training Loss 1.5008679628372192, Validation Loss 1.5008679628372192, Validation Accuracy 0.0000\n",
      "Epoch 223: Training Loss 1.4624512195587158, Validation Loss 1.4624512195587158, Validation Accuracy 0.0000\n",
      "Epoch 224: Training Loss 1.489524006843567, Validation Loss 1.489524006843567, Validation Accuracy 0.0000\n",
      "Epoch 225: Training Loss 1.5151487588882446, Validation Loss 1.5151487588882446, Validation Accuracy 0.0000\n",
      "Epoch 226: Training Loss 1.5383018255233765, Validation Loss 1.5383018255233765, Validation Accuracy 0.0000\n",
      "Epoch 227: Training Loss 1.4981344938278198, Validation Loss 1.4981344938278198, Validation Accuracy 0.0000\n",
      "Epoch 228: Training Loss 1.5229498147964478, Validation Loss 1.5229498147964478, Validation Accuracy 0.0000\n",
      "Epoch 229: Training Loss 1.5476661920547485, Validation Loss 1.5476661920547485, Validation Accuracy 0.0000\n",
      "Epoch 230: Training Loss 1.5077736377716064, Validation Loss 1.5077736377716064, Validation Accuracy 0.0000\n",
      "Epoch 231: Training Loss 1.5340956449508667, Validation Loss 1.5340956449508667, Validation Accuracy 0.0000\n",
      "Epoch 232: Training Loss 1.5579020977020264, Validation Loss 1.5579020977020264, Validation Accuracy 0.0000\n",
      "Epoch 233: Training Loss 1.5805234909057617, Validation Loss 1.5805234909057617, Validation Accuracy 0.0000\n",
      "Epoch 234: Training Loss 1.539381504058838, Validation Loss 1.539381504058838, Validation Accuracy 0.0000\n",
      "Epoch 235: Training Loss 1.501999020576477, Validation Loss 1.501999020576477, Validation Accuracy 0.0000\n",
      "Epoch 236: Training Loss 1.5290902853012085, Validation Loss 1.5290902853012085, Validation Accuracy 0.0000\n",
      "Epoch 237: Training Loss 1.4933242797851562, Validation Loss 1.4933242797851562, Validation Accuracy 0.0000\n",
      "Epoch 238: Training Loss 1.5216163396835327, Validation Loss 1.5216163396835327, Validation Accuracy 0.0000\n",
      "Epoch 239: Training Loss 1.5484613180160522, Validation Loss 1.5484613180160522, Validation Accuracy 0.0000\n",
      "Epoch 240: Training Loss 1.5751655101776123, Validation Loss 1.5751655101776123, Validation Accuracy 0.0000\n",
      "Epoch 241: Training Loss 1.599287509918213, Validation Loss 1.599287509918213, Validation Accuracy 0.0000\n",
      "Epoch 242: Training Loss 1.6233081817626953, Validation Loss 1.6233081817626953, Validation Accuracy 0.0000\n",
      "Epoch 243: Training Loss 1.6460490226745605, Validation Loss 1.6460490226745605, Validation Accuracy 0.0000\n",
      "Epoch 244: Training Loss 1.6033134460449219, Validation Loss 1.6033134460449219, Validation Accuracy 0.0000\n",
      "Epoch 245: Training Loss 1.6280450820922852, Validation Loss 1.6280450820922852, Validation Accuracy 0.0000\n",
      "Epoch 246: Training Loss 1.6514472961425781, Validation Loss 1.6514472961425781, Validation Accuracy 0.0000\n",
      "Epoch 247: Training Loss 1.6736040115356445, Validation Loss 1.6736040115356445, Validation Accuracy 0.0000\n",
      "Epoch 248: Training Loss 1.6945949792861938, Validation Loss 1.6945949792861938, Validation Accuracy 0.0000\n",
      "Epoch 249: Training Loss 1.7137508392333984, Validation Loss 1.7137508392333984, Validation Accuracy 0.0000\n",
      "Epoch 250: Training Loss 1.7327349185943604, Validation Loss 1.7327349185943604, Validation Accuracy 0.0000\n",
      "Epoch 251: Training Loss 1.6855595111846924, Validation Loss 1.6855595111846924, Validation Accuracy 0.0000\n",
      "Epoch 252: Training Loss 1.7071101665496826, Validation Loss 1.7071101665496826, Validation Accuracy 0.0000\n",
      "Epoch 253: Training Loss 1.7275307178497314, Validation Loss 1.7275307178497314, Validation Accuracy 0.0000\n",
      "Epoch 254: Training Loss 1.746264934539795, Validation Loss 1.746264934539795, Validation Accuracy 0.0000\n",
      "Epoch 255: Training Loss 1.7647430896759033, Validation Loss 1.7647430896759033, Validation Accuracy 0.0000\n",
      "Epoch 256: Training Loss 1.7169677019119263, Validation Loss 1.7169677019119263, Validation Accuracy 0.0000\n",
      "Epoch 257: Training Loss 1.7381141185760498, Validation Loss 1.7381141185760498, Validation Accuracy 0.0000\n",
      "Epoch 258: Training Loss 1.758152723312378, Validation Loss 1.758152723312378, Validation Accuracy 0.0000\n",
      "Epoch 259: Training Loss 1.7771573066711426, Validation Loss 1.7771573066711426, Validation Accuracy 0.0000\n",
      "Epoch 260: Training Loss 1.7951970100402832, Validation Loss 1.7951970100402832, Validation Accuracy 0.0000\n",
      "Epoch 261: Training Loss 1.7469544410705566, Validation Loss 1.7469544410705566, Validation Accuracy 0.0000\n",
      "Epoch 262: Training Loss 1.7672415971755981, Validation Loss 1.7672415971755981, Validation Accuracy 0.0000\n",
      "Epoch 263: Training Loss 1.7865705490112305, Validation Loss 1.7865705490112305, Validation Accuracy 0.0000\n",
      "Epoch 264: Training Loss 1.8050038814544678, Validation Loss 1.8050038814544678, Validation Accuracy 0.0000\n",
      "Epoch 265: Training Loss 1.8225997686386108, Validation Loss 1.8225997686386108, Validation Accuracy 0.0000\n",
      "Epoch 266: Training Loss 1.774475336074829, Validation Loss 1.774475336074829, Validation Accuracy 0.0000\n",
      "Epoch 267: Training Loss 1.7307765483856201, Validation Loss 1.7307765483856201, Validation Accuracy 0.0000\n",
      "Epoch 268: Training Loss 1.7550681829452515, Validation Loss 1.7550681829452515, Validation Accuracy 0.0000\n",
      "Epoch 269: Training Loss 1.7780263423919678, Validation Loss 1.7780263423919678, Validation Accuracy 0.0000\n",
      "Epoch 270: Training Loss 1.7992440462112427, Validation Loss 1.7992440462112427, Validation Accuracy 0.0000\n",
      "Epoch 271: Training Loss 1.819909691810608, Validation Loss 1.819909691810608, Validation Accuracy 0.0000\n",
      "Epoch 272: Training Loss 1.839113473892212, Validation Loss 1.839113473892212, Validation Accuracy 0.0000\n",
      "Epoch 273: Training Loss 1.7928317785263062, Validation Loss 1.7928317785263062, Validation Accuracy 0.0000\n",
      "Epoch 274: Training Loss 1.814599871635437, Validation Loss 1.814599871635437, Validation Accuracy 0.0000\n",
      "Epoch 275: Training Loss 1.7713347673416138, Validation Loss 1.7713347673416138, Validation Accuracy 0.0000\n",
      "Epoch 276: Training Loss 1.7321357727050781, Validation Loss 1.7321357727050781, Validation Accuracy 0.0000\n",
      "Epoch 277: Training Loss 1.7591274976730347, Validation Loss 1.7591274976730347, Validation Accuracy 0.0000\n",
      "Epoch 278: Training Loss 1.7219516038894653, Validation Loss 1.7219516038894653, Validation Accuracy 0.0000\n",
      "Epoch 279: Training Loss 1.688353419303894, Validation Loss 1.688353419303894, Validation Accuracy 0.0000\n",
      "Epoch 280: Training Loss 1.7204134464263916, Validation Loss 1.7204134464263916, Validation Accuracy 0.0000\n",
      "Epoch 281: Training Loss 1.7505767345428467, Validation Loss 1.7505767345428467, Validation Accuracy 0.0000\n",
      "Epoch 282: Training Loss 1.7783393859863281, Validation Loss 1.7783393859863281, Validation Accuracy 0.0000\n",
      "Epoch 283: Training Loss 1.8052256107330322, Validation Loss 1.8052256107330322, Validation Accuracy 0.0000\n",
      "Epoch 284: Training Loss 1.8301218748092651, Validation Loss 1.8301218748092651, Validation Accuracy 0.0000\n",
      "Epoch 285: Training Loss 1.8541535139083862, Validation Loss 1.8541535139083862, Validation Accuracy 0.0000\n",
      "Epoch 286: Training Loss 1.8768386840820312, Validation Loss 1.8768386840820312, Validation Accuracy 0.0000\n",
      "Epoch 287: Training Loss 1.8981001377105713, Validation Loss 1.8981001377105713, Validation Accuracy 0.0000\n",
      "Epoch 288: Training Loss 1.9183416366577148, Validation Loss 1.9183416366577148, Validation Accuracy 0.0000\n",
      "Epoch 289: Training Loss 1.9377429485321045, Validation Loss 1.9377429485321045, Validation Accuracy 0.0000\n",
      "Epoch 290: Training Loss 1.8916871547698975, Validation Loss 1.8916871547698975, Validation Accuracy 0.0000\n",
      "Epoch 291: Training Loss 1.9135751724243164, Validation Loss 1.9135751724243164, Validation Accuracy 0.0000\n",
      "Epoch 292: Training Loss 1.8706084489822388, Validation Loss 1.8706084489822388, Validation Accuracy 0.0000\n",
      "Epoch 293: Training Loss 1.8947654962539673, Validation Loss 1.8947654962539673, Validation Accuracy 0.0000\n",
      "Epoch 294: Training Loss 1.9177050590515137, Validation Loss 1.9177050590515137, Validation Accuracy 0.0000\n",
      "Epoch 295: Training Loss 1.9395087957382202, Validation Loss 1.9395087957382202, Validation Accuracy 0.0000\n",
      "Epoch 296: Training Loss 1.9602535963058472, Validation Loss 1.9602535963058472, Validation Accuracy 0.0000\n",
      "Epoch 297: Training Loss 1.9162300825119019, Validation Loss 1.9162300825119019, Validation Accuracy 0.0000\n",
      "Epoch 298: Training Loss 1.8763898611068726, Validation Loss 1.8763898611068726, Validation Accuracy 0.0000\n",
      "Epoch 299: Training Loss 1.903135895729065, Validation Loss 1.903135895729065, Validation Accuracy 0.0000\n",
      "Epoch 300: Training Loss 1.8654013872146606, Validation Loss 1.8654013872146606, Validation Accuracy 0.0000\n",
      "Epoch 301: Training Loss 1.893306016921997, Validation Loss 1.893306016921997, Validation Accuracy 0.0000\n",
      "Epoch 302: Training Loss 1.9197301864624023, Validation Loss 1.9197301864624023, Validation Accuracy 0.0000\n",
      "Epoch 303: Training Loss 1.9447729587554932, Validation Loss 1.9447729587554932, Validation Accuracy 0.0000\n",
      "Epoch 304: Training Loss 1.9687834978103638, Validation Loss 1.9687834978103638, Validation Accuracy 0.0000\n",
      "Epoch 305: Training Loss 1.9912824630737305, Validation Loss 1.9912824630737305, Validation Accuracy 0.0000\n",
      "Epoch 306: Training Loss 2.012755870819092, Validation Loss 2.012755870819092, Validation Accuracy 0.0000\n",
      "Epoch 307: Training Loss 1.9698017835617065, Validation Loss 1.9698017835617065, Validation Accuracy 0.0000\n",
      "Epoch 308: Training Loss 1.9309682846069336, Validation Loss 1.9309682846069336, Validation Accuracy 0.0000\n",
      "Epoch 309: Training Loss 1.9577778577804565, Validation Loss 1.9577778577804565, Validation Accuracy 0.0000\n",
      "Epoch 310: Training Loss 1.9831711053848267, Validation Loss 1.9831711053848267, Validation Accuracy 0.0000\n",
      "Epoch 311: Training Loss 2.0074000358581543, Validation Loss 2.0074000358581543, Validation Accuracy 0.0000\n",
      "Epoch 312: Training Loss 1.9677019119262695, Validation Loss 1.9677019119262695, Validation Accuracy 0.0000\n",
      "Epoch 313: Training Loss 1.9318674802780151, Validation Loss 1.9318674802780151, Validation Accuracy 0.0000\n",
      "Epoch 314: Training Loss 1.8995791673660278, Validation Loss 1.8995791673660278, Validation Accuracy 0.0000\n",
      "Epoch 315: Training Loss 1.9316083192825317, Validation Loss 1.9316083192825317, Validation Accuracy 0.0000\n",
      "Epoch 316: Training Loss 1.9613778591156006, Validation Loss 1.9613778591156006, Validation Accuracy 0.0000\n",
      "Epoch 317: Training Loss 1.9282995462417603, Validation Loss 1.9282995462417603, Validation Accuracy 0.0000\n",
      "Epoch 318: Training Loss 1.9594770669937134, Validation Loss 1.9594770669937134, Validation Accuracy 0.0000\n",
      "Epoch 319: Training Loss 1.9885793924331665, Validation Loss 1.9885793924331665, Validation Accuracy 0.0000\n",
      "Epoch 320: Training Loss 2.0161190032958984, Validation Loss 2.0161190032958984, Validation Accuracy 0.0000\n",
      "Epoch 321: Training Loss 1.9806712865829468, Validation Loss 1.9806712865829468, Validation Accuracy 0.0000\n",
      "Epoch 322: Training Loss 2.009580612182617, Validation Loss 2.009580612182617, Validation Accuracy 0.0000\n",
      "Epoch 323: Training Loss 2.0368409156799316, Validation Loss 2.0368409156799316, Validation Accuracy 0.0000\n",
      "Epoch 324: Training Loss 2.0013833045959473, Validation Loss 2.0013833045959473, Validation Accuracy 0.0000\n",
      "Epoch 325: Training Loss 2.03013014793396, Validation Loss 2.03013014793396, Validation Accuracy 0.0000\n",
      "Epoch 326: Training Loss 2.057142496109009, Validation Loss 2.057142496109009, Validation Accuracy 0.0000\n",
      "Epoch 327: Training Loss 2.08248233795166, Validation Loss 2.08248233795166, Validation Accuracy 0.0000\n",
      "Epoch 328: Training Loss 2.1062819957733154, Validation Loss 2.1062819957733154, Validation Accuracy 0.0000\n",
      "Epoch 329: Training Loss 2.129265308380127, Validation Loss 2.129265308380127, Validation Accuracy 0.0000\n",
      "Epoch 330: Training Loss 2.0895278453826904, Validation Loss 2.0895278453826904, Validation Accuracy 0.0000\n",
      "Epoch 331: Training Loss 2.0536978244781494, Validation Loss 2.0536978244781494, Validation Accuracy 0.0000\n",
      "Epoch 332: Training Loss 2.081392288208008, Validation Loss 2.081392288208008, Validation Accuracy 0.0000\n",
      "Epoch 333: Training Loss 2.0473530292510986, Validation Loss 2.0473530292510986, Validation Accuracy 0.0000\n",
      "Epoch 334: Training Loss 2.076709032058716, Validation Loss 2.076709032058716, Validation Accuracy 0.0000\n",
      "Epoch 335: Training Loss 2.1044273376464844, Validation Loss 2.1044273376464844, Validation Accuracy 0.0000\n",
      "Epoch 336: Training Loss 2.130103826522827, Validation Loss 2.130103826522827, Validation Accuracy 0.0000\n",
      "Epoch 337: Training Loss 2.1548690795898438, Validation Loss 2.1548690795898438, Validation Accuracy 0.0000\n",
      "Epoch 338: Training Loss 2.178331136703491, Validation Loss 2.178331136703491, Validation Accuracy 0.0000\n",
      "Epoch 339: Training Loss 2.200584650039673, Validation Loss 2.200584650039673, Validation Accuracy 0.0000\n",
      "Epoch 340: Training Loss 2.220944404602051, Validation Loss 2.220944404602051, Validation Accuracy 0.0000\n",
      "Epoch 341: Training Loss 2.1802189350128174, Validation Loss 2.1802189350128174, Validation Accuracy 0.0000\n",
      "Epoch 342: Training Loss 2.202878713607788, Validation Loss 2.202878713607788, Validation Accuracy 0.0000\n",
      "Epoch 343: Training Loss 2.2250733375549316, Validation Loss 2.2250733375549316, Validation Accuracy 0.0000\n",
      "Epoch 344: Training Loss 2.2452356815338135, Validation Loss 2.2452356815338135, Validation Accuracy 0.0000\n",
      "Epoch 345: Training Loss 2.264244794845581, Validation Loss 2.264244794845581, Validation Accuracy 0.0000\n",
      "Epoch 346: Training Loss 2.222808837890625, Validation Loss 2.222808837890625, Validation Accuracy 0.0000\n",
      "Epoch 347: Training Loss 2.2452774047851562, Validation Loss 2.2452774047851562, Validation Accuracy 0.0000\n",
      "Epoch 348: Training Loss 2.2066965103149414, Validation Loss 2.2066965103149414, Validation Accuracy 0.0000\n",
      "Epoch 349: Training Loss 2.1719255447387695, Validation Loss 2.1719255447387695, Validation Accuracy 0.0000\n",
      "Epoch 350: Training Loss 2.1406452655792236, Validation Loss 2.1406452655792236, Validation Accuracy 0.0000\n",
      "Epoch 351: Training Loss 2.1125595569610596, Validation Loss 2.1125595569610596, Validation Accuracy 0.0000\n",
      "Epoch 352: Training Loss 2.1446030139923096, Validation Loss 2.1446030139923096, Validation Accuracy 0.0000\n",
      "Epoch 353: Training Loss 2.17525577545166, Validation Loss 2.17525577545166, Validation Accuracy 0.0000\n",
      "Epoch 354: Training Loss 2.203303813934326, Validation Loss 2.203303813934326, Validation Accuracy 0.0000\n",
      "Epoch 355: Training Loss 2.2305362224578857, Validation Loss 2.2305362224578857, Validation Accuracy 0.0000\n",
      "Epoch 356: Training Loss 2.2562577724456787, Validation Loss 2.2562577724456787, Validation Accuracy 0.0000\n",
      "Epoch 357: Training Loss 2.2221765518188477, Validation Loss 2.2221765518188477, Validation Accuracy 0.0000\n",
      "Epoch 358: Training Loss 2.1915252208709717, Validation Loss 2.1915252208709717, Validation Accuracy 0.0000\n",
      "Epoch 359: Training Loss 2.164012908935547, Validation Loss 2.164012908935547, Validation Accuracy 0.0000\n",
      "Epoch 360: Training Loss 2.1955387592315674, Validation Loss 2.1955387592315674, Validation Accuracy 0.0000\n",
      "Epoch 361: Training Loss 2.2249255180358887, Validation Loss 2.2249255180358887, Validation Accuracy 0.0000\n",
      "Epoch 362: Training Loss 2.2535107135772705, Validation Loss 2.2535107135772705, Validation Accuracy 0.0000\n",
      "Epoch 363: Training Loss 2.2231292724609375, Validation Loss 2.2231292724609375, Validation Accuracy 0.0000\n",
      "Epoch 364: Training Loss 2.252725124359131, Validation Loss 2.252725124359131, Validation Accuracy 0.0000\n",
      "Epoch 365: Training Loss 2.2794692516326904, Validation Loss 2.2794692516326904, Validation Accuracy 0.0000\n",
      "Epoch 366: Training Loss 2.248725414276123, Validation Loss 2.248725414276123, Validation Accuracy 0.0000\n",
      "Epoch 367: Training Loss 2.276498794555664, Validation Loss 2.276498794555664, Validation Accuracy 0.0000\n",
      "Epoch 368: Training Loss 2.3038227558135986, Validation Loss 2.3038227558135986, Validation Accuracy 0.0000\n",
      "Epoch 369: Training Loss 2.3296141624450684, Validation Loss 2.3296141624450684, Validation Accuracy 0.0000\n",
      "Epoch 370: Training Loss 2.3525521755218506, Validation Loss 2.3525521755218506, Validation Accuracy 0.0000\n",
      "Epoch 371: Training Loss 2.3756484985351562, Validation Loss 2.3756484985351562, Validation Accuracy 0.0000\n",
      "Epoch 372: Training Loss 2.397535800933838, Validation Loss 2.397535800933838, Validation Accuracy 0.0000\n",
      "Epoch 373: Training Loss 2.418304920196533, Validation Loss 2.418304920196533, Validation Accuracy 0.0000\n",
      "Epoch 374: Training Loss 2.436439037322998, Validation Loss 2.436439037322998, Validation Accuracy 0.0000\n",
      "Epoch 375: Training Loss 2.4552595615386963, Validation Loss 2.4552595615386963, Validation Accuracy 0.0000\n",
      "Epoch 376: Training Loss 2.4731922149658203, Validation Loss 2.4731922149658203, Validation Accuracy 0.0000\n",
      "Epoch 377: Training Loss 2.488607406616211, Validation Loss 2.488607406616211, Validation Accuracy 0.0000\n",
      "Epoch 378: Training Loss 2.448169708251953, Validation Loss 2.448169708251953, Validation Accuracy 0.0000\n",
      "Epoch 379: Training Loss 2.466181993484497, Validation Loss 2.466181993484497, Validation Accuracy 0.0000\n",
      "Epoch 380: Training Loss 2.428734540939331, Validation Loss 2.428734540939331, Validation Accuracy 0.0000\n",
      "Epoch 381: Training Loss 2.450587511062622, Validation Loss 2.450587511062622, Validation Accuracy 0.0000\n",
      "Epoch 382: Training Loss 2.47131609916687, Validation Loss 2.47131609916687, Validation Accuracy 0.0000\n",
      "Epoch 383: Training Loss 2.435408592224121, Validation Loss 2.435408592224121, Validation Accuracy 0.0000\n",
      "Epoch 384: Training Loss 2.4580678939819336, Validation Loss 2.4580678939819336, Validation Accuracy 0.0000\n",
      "Epoch 385: Training Loss 2.424506902694702, Validation Loss 2.424506902694702, Validation Accuracy 0.0000\n",
      "Epoch 386: Training Loss 2.39426851272583, Validation Loss 2.39426851272583, Validation Accuracy 0.0000\n",
      "Epoch 387: Training Loss 2.4211206436157227, Validation Loss 2.4211206436157227, Validation Accuracy 0.0000\n",
      "Epoch 388: Training Loss 2.4450268745422363, Validation Loss 2.4450268745422363, Validation Accuracy 0.0000\n",
      "Epoch 389: Training Loss 2.4674038887023926, Validation Loss 2.4674038887023926, Validation Accuracy 0.0000\n",
      "Epoch 390: Training Loss 2.4883875846862793, Validation Loss 2.4883875846862793, Validation Accuracy 0.0000\n",
      "Epoch 391: Training Loss 2.5081005096435547, Validation Loss 2.5081005096435547, Validation Accuracy 0.0000\n",
      "Epoch 392: Training Loss 2.474337339401245, Validation Loss 2.474337339401245, Validation Accuracy 0.0000\n",
      "Epoch 393: Training Loss 2.4957900047302246, Validation Loss 2.4957900047302246, Validation Accuracy 0.0000\n",
      "Epoch 394: Training Loss 2.464128255844116, Validation Loss 2.464128255844116, Validation Accuracy 0.0000\n",
      "Epoch 395: Training Loss 2.4356143474578857, Validation Loss 2.4356143474578857, Validation Accuracy 0.0000\n",
      "Epoch 396: Training Loss 2.409977436065674, Validation Loss 2.409977436065674, Validation Accuracy 0.0000\n",
      "Epoch 397: Training Loss 2.386972427368164, Validation Loss 2.386972427368164, Validation Accuracy 0.0000\n",
      "Epoch 398: Training Loss 2.418449640274048, Validation Loss 2.418449640274048, Validation Accuracy 0.0000\n",
      "Epoch 399: Training Loss 2.3959367275238037, Validation Loss 2.3959367275238037, Validation Accuracy 0.0000\n",
      "Epoch 400: Training Loss 2.3757803440093994, Validation Loss 2.3757803440093994, Validation Accuracy 0.0000\n",
      "Epoch 401: Training Loss 2.3577780723571777, Validation Loss 2.3577780723571777, Validation Accuracy 0.0000\n",
      "Epoch 402: Training Loss 2.3928728103637695, Validation Loss 2.3928728103637695, Validation Accuracy 0.0000\n",
      "Epoch 403: Training Loss 2.3746256828308105, Validation Loss 2.3746256828308105, Validation Accuracy 0.0000\n",
      "Epoch 404: Training Loss 2.409207344055176, Validation Loss 2.409207344055176, Validation Accuracy 0.0000\n",
      "Epoch 405: Training Loss 2.441626787185669, Validation Loss 2.441626787185669, Validation Accuracy 0.0000\n",
      "Epoch 406: Training Loss 2.421211004257202, Validation Loss 2.421211004257202, Validation Accuracy 0.0000\n",
      "Epoch 407: Training Loss 2.453486442565918, Validation Loss 2.453486442565918, Validation Accuracy 0.0000\n",
      "Epoch 408: Training Loss 2.433260679244995, Validation Loss 2.433260679244995, Validation Accuracy 0.0000\n",
      "Epoch 409: Training Loss 2.4653823375701904, Validation Loss 2.4653823375701904, Validation Accuracy 0.0000\n",
      "Epoch 410: Training Loss 2.4453392028808594, Validation Loss 2.4453392028808594, Validation Accuracy 0.0000\n",
      "Epoch 411: Training Loss 2.4274134635925293, Validation Loss 2.4274134635925293, Validation Accuracy 0.0000\n",
      "Epoch 412: Training Loss 2.461019515991211, Validation Loss 2.461019515991211, Validation Accuracy 0.0000\n",
      "Epoch 413: Training Loss 2.492536783218384, Validation Loss 2.492536783218384, Validation Accuracy 0.0000\n",
      "Epoch 414: Training Loss 2.5205318927764893, Validation Loss 2.5205318927764893, Validation Accuracy 0.0000\n",
      "Epoch 415: Training Loss 2.4987823963165283, Validation Loss 2.4987823963165283, Validation Accuracy 0.0000\n",
      "Epoch 416: Training Loss 2.5285565853118896, Validation Loss 2.5285565853118896, Validation Accuracy 0.0000\n",
      "Epoch 417: Training Loss 2.5073065757751465, Validation Loss 2.5073065757751465, Validation Accuracy 0.0000\n",
      "Epoch 418: Training Loss 2.4882562160491943, Validation Loss 2.4882562160491943, Validation Accuracy 0.0000\n",
      "Epoch 419: Training Loss 2.5198397636413574, Validation Loss 2.5198397636413574, Validation Accuracy 0.0000\n",
      "Epoch 420: Training Loss 2.5009026527404785, Validation Loss 2.5009026527404785, Validation Accuracy 0.0000\n",
      "Epoch 421: Training Loss 2.5306427478790283, Validation Loss 2.5306427478790283, Validation Accuracy 0.0000\n",
      "Epoch 422: Training Loss 2.5583178997039795, Validation Loss 2.5583178997039795, Validation Accuracy 0.0000\n",
      "Epoch 423: Training Loss 2.586071014404297, Validation Loss 2.586071014404297, Validation Accuracy 0.0000\n",
      "Epoch 424: Training Loss 2.6102559566497803, Validation Loss 2.6102559566497803, Validation Accuracy 0.0000\n",
      "Epoch 425: Training Loss 2.6349775791168213, Validation Loss 2.6349775791168213, Validation Accuracy 0.0000\n",
      "Epoch 426: Training Loss 2.6562507152557373, Validation Loss 2.6562507152557373, Validation Accuracy 0.0000\n",
      "Epoch 427: Training Loss 2.6301302909851074, Validation Loss 2.6301302909851074, Validation Accuracy 0.0000\n",
      "Epoch 428: Training Loss 2.6544830799102783, Validation Loss 2.6544830799102783, Validation Accuracy 0.0000\n",
      "Epoch 429: Training Loss 2.6753785610198975, Validation Loss 2.6753785610198975, Validation Accuracy 0.0000\n",
      "Epoch 430: Training Loss 2.6972365379333496, Validation Loss 2.6972365379333496, Validation Accuracy 0.0000\n",
      "Epoch 431: Training Loss 2.6701788902282715, Validation Loss 2.6701788902282715, Validation Accuracy 0.0000\n",
      "Epoch 432: Training Loss 2.693128824234009, Validation Loss 2.693128824234009, Validation Accuracy 0.0000\n",
      "Epoch 433: Training Loss 2.7148542404174805, Validation Loss 2.7148542404174805, Validation Accuracy 0.0000\n",
      "Epoch 434: Training Loss 2.7333178520202637, Validation Loss 2.7333178520202637, Validation Accuracy 0.0000\n",
      "Epoch 435: Training Loss 2.7529408931732178, Validation Loss 2.7529408931732178, Validation Accuracy 0.0000\n",
      "Epoch 436: Training Loss 2.7715976238250732, Validation Loss 2.7715976238250732, Validation Accuracy 0.0000\n",
      "Epoch 437: Training Loss 2.742271661758423, Validation Loss 2.742271661758423, Validation Accuracy 0.0000\n",
      "Epoch 438: Training Loss 2.7623775005340576, Validation Loss 2.7623775005340576, Validation Accuracy 0.0000\n",
      "Epoch 439: Training Loss 2.734903335571289, Validation Loss 2.734903335571289, Validation Accuracy 0.0000\n",
      "Epoch 440: Training Loss 2.756213426589966, Validation Loss 2.756213426589966, Validation Accuracy 0.0000\n",
      "Epoch 441: Training Loss 2.7764225006103516, Validation Loss 2.7764225006103516, Validation Accuracy 0.0000\n",
      "Epoch 442: Training Loss 2.7935919761657715, Validation Loss 2.7935919761657715, Validation Accuracy 0.0000\n",
      "Epoch 443: Training Loss 2.811903476715088, Validation Loss 2.811903476715088, Validation Accuracy 0.0000\n",
      "Epoch 444: Training Loss 2.783418893814087, Validation Loss 2.783418893814087, Validation Accuracy 0.0000\n",
      "Epoch 445: Training Loss 2.803107738494873, Validation Loss 2.803107738494873, Validation Accuracy 0.0000\n",
      "Epoch 446: Training Loss 2.776393413543701, Validation Loss 2.776393413543701, Validation Accuracy 0.0000\n",
      "Epoch 447: Training Loss 2.752199649810791, Validation Loss 2.752199649810791, Validation Accuracy 0.0000\n",
      "Epoch 448: Training Loss 2.7749860286712646, Validation Loss 2.7749860286712646, Validation Accuracy 0.0000\n",
      "Epoch 449: Training Loss 2.794660806655884, Validation Loss 2.794660806655884, Validation Accuracy 0.0000\n",
      "Epoch 450: Training Loss 2.8151490688323975, Validation Loss 2.8151490688323975, Validation Accuracy 0.0000\n",
      "Epoch 451: Training Loss 2.7900784015655518, Validation Loss 2.7900784015655518, Validation Accuracy 0.0000\n",
      "Epoch 452: Training Loss 2.8115196228027344, Validation Loss 2.8115196228027344, Validation Accuracy 0.0000\n",
      "Epoch 453: Training Loss 2.8318469524383545, Validation Loss 2.8318469524383545, Validation Accuracy 0.0000\n",
      "Epoch 454: Training Loss 2.807133197784424, Validation Loss 2.807133197784424, Validation Accuracy 0.0000\n",
      "Epoch 455: Training Loss 2.826533794403076, Validation Loss 2.826533794403076, Validation Accuracy 0.0000\n",
      "Epoch 456: Training Loss 2.8030965328216553, Validation Loss 2.8030965328216553, Validation Accuracy 0.0000\n",
      "Epoch 457: Training Loss 2.8232617378234863, Validation Loss 2.8232617378234863, Validation Accuracy 0.0000\n",
      "Epoch 458: Training Loss 2.800931692123413, Validation Loss 2.800931692123413, Validation Accuracy 0.0000\n",
      "Epoch 459: Training Loss 2.821719169616699, Validation Loss 2.821719169616699, Validation Accuracy 0.0000\n",
      "Epoch 460: Training Loss 2.843278408050537, Validation Loss 2.843278408050537, Validation Accuracy 0.0000\n",
      "Epoch 461: Training Loss 2.8637187480926514, Validation Loss 2.8637187480926514, Validation Accuracy 0.0000\n",
      "Epoch 462: Training Loss 2.840346336364746, Validation Loss 2.840346336364746, Validation Accuracy 0.0000\n",
      "Epoch 463: Training Loss 2.8596251010894775, Validation Loss 2.8596251010894775, Validation Accuracy 0.0000\n",
      "Epoch 464: Training Loss 2.877774953842163, Validation Loss 2.877774953842163, Validation Accuracy 0.0000\n",
      "Epoch 465: Training Loss 2.8546297550201416, Validation Loss 2.8546297550201416, Validation Accuracy 0.0000\n",
      "Epoch 466: Training Loss 2.833629846572876, Validation Loss 2.833629846572876, Validation Accuracy 0.0000\n",
      "Epoch 467: Training Loss 2.854311943054199, Validation Loss 2.854311943054199, Validation Accuracy 0.0000\n",
      "Epoch 468: Training Loss 2.8341562747955322, Validation Loss 2.8341562747955322, Validation Accuracy 0.0000\n",
      "Epoch 469: Training Loss 2.8552443981170654, Validation Loss 2.8552443981170654, Validation Accuracy 0.0000\n",
      "Epoch 470: Training Loss 2.87506365776062, Validation Loss 2.87506365776062, Validation Accuracy 0.0000\n",
      "Epoch 471: Training Loss 2.8937273025512695, Validation Loss 2.8937273025512695, Validation Accuracy 0.0000\n",
      "Epoch 472: Training Loss 2.91133451461792, Validation Loss 2.91133451461792, Validation Accuracy 0.0000\n",
      "Epoch 473: Training Loss 2.8890490531921387, Validation Loss 2.8890490531921387, Validation Accuracy 0.0000\n",
      "Epoch 474: Training Loss 2.9098033905029297, Validation Loss 2.9098033905029297, Validation Accuracy 0.0000\n",
      "Epoch 475: Training Loss 2.8885998725891113, Validation Loss 2.8885998725891113, Validation Accuracy 0.0000\n",
      "Epoch 476: Training Loss 2.9099557399749756, Validation Loss 2.9099557399749756, Validation Accuracy 0.0000\n",
      "Epoch 477: Training Loss 2.9302196502685547, Validation Loss 2.9302196502685547, Validation Accuracy 0.0000\n",
      "Epoch 478: Training Loss 2.9494733810424805, Validation Loss 2.9494733810424805, Validation Accuracy 0.0000\n",
      "Epoch 479: Training Loss 2.9274203777313232, Validation Loss 2.9274203777313232, Validation Accuracy 0.0000\n",
      "Epoch 480: Training Loss 2.9073634147644043, Validation Loss 2.9073634147644043, Validation Accuracy 0.0000\n",
      "Epoch 481: Training Loss 2.928913116455078, Validation Loss 2.928913116455078, Validation Accuracy 0.0000\n",
      "Epoch 482: Training Loss 2.909663200378418, Validation Loss 2.909663200378418, Validation Accuracy 0.0000\n",
      "Epoch 483: Training Loss 2.9294822216033936, Validation Loss 2.9294822216033936, Validation Accuracy 0.0000\n",
      "Epoch 484: Training Loss 2.9109559059143066, Validation Loss 2.9109559059143066, Validation Accuracy 0.0000\n",
      "Epoch 485: Training Loss 2.933253526687622, Validation Loss 2.933253526687622, Validation Accuracy 0.0000\n",
      "Epoch 486: Training Loss 2.9522454738616943, Validation Loss 2.9522454738616943, Validation Accuracy 0.0000\n",
      "Epoch 487: Training Loss 2.9701602458953857, Validation Loss 2.9701602458953857, Validation Accuracy 0.0000\n",
      "Epoch 488: Training Loss 2.9893672466278076, Validation Loss 2.9893672466278076, Validation Accuracy 0.0000\n",
      "Epoch 489: Training Loss 2.9687557220458984, Validation Loss 2.9687557220458984, Validation Accuracy 0.0000\n",
      "Epoch 490: Training Loss 2.949984550476074, Validation Loss 2.949984550476074, Validation Accuracy 0.0000\n",
      "Epoch 491: Training Loss 2.9712448120117188, Validation Loss 2.9712448120117188, Validation Accuracy 0.0000\n",
      "Epoch 492: Training Loss 2.991427421569824, Validation Loss 2.991427421569824, Validation Accuracy 0.0000\n",
      "Epoch 493: Training Loss 3.008455753326416, Validation Loss 3.008455753326416, Validation Accuracy 0.0000\n",
      "Epoch 494: Training Loss 2.988645076751709, Validation Loss 2.988645076751709, Validation Accuracy 0.0000\n",
      "Epoch 495: Training Loss 2.9705913066864014, Validation Loss 2.9705913066864014, Validation Accuracy 0.0000\n",
      "Epoch 496: Training Loss 2.954155921936035, Validation Loss 2.954155921936035, Validation Accuracy 0.0000\n",
      "Epoch 497: Training Loss 2.974458694458008, Validation Loss 2.974458694458008, Validation Accuracy 0.0000\n",
      "Epoch 498: Training Loss 2.993589162826538, Validation Loss 2.993589162826538, Validation Accuracy 0.0000\n",
      "Epoch 499: Training Loss 3.0116472244262695, Validation Loss 3.0116472244262695, Validation Accuracy 0.0000\n",
      "Epoch 500: Training Loss 2.9937899112701416, Validation Loss 2.9937899112701416, Validation Accuracy 0.0000\n",
      "Epoch 501: Training Loss 3.014530897140503, Validation Loss 3.014530897140503, Validation Accuracy 0.0000\n",
      "Epoch 502: Training Loss 2.9973130226135254, Validation Loss 2.9973130226135254, Validation Accuracy 0.0000\n",
      "Epoch 503: Training Loss 3.016062021255493, Validation Loss 3.016062021255493, Validation Accuracy 0.0000\n",
      "Epoch 504: Training Loss 3.0360913276672363, Validation Loss 3.0360913276672363, Validation Accuracy 0.0000\n",
      "Epoch 505: Training Loss 3.018578290939331, Validation Loss 3.018578290939331, Validation Accuracy 0.0000\n",
      "Epoch 506: Training Loss 3.0026063919067383, Validation Loss 3.0026063919067383, Validation Accuracy 0.0000\n",
      "Epoch 507: Training Loss 3.0241591930389404, Validation Loss 3.0241591930389404, Validation Accuracy 0.0000\n",
      "Epoch 508: Training Loss 3.0446274280548096, Validation Loss 3.0446274280548096, Validation Accuracy 0.0000\n",
      "Epoch 509: Training Loss 3.028153657913208, Validation Loss 3.028153657913208, Validation Accuracy 0.0000\n",
      "Epoch 510: Training Loss 3.0131256580352783, Validation Loss 3.0131256580352783, Validation Accuracy 0.0000\n",
      "Epoch 511: Training Loss 2.999434471130371, Validation Loss 2.999434471130371, Validation Accuracy 0.0000\n",
      "Epoch 512: Training Loss 3.0201449394226074, Validation Loss 3.0201449394226074, Validation Accuracy 0.0000\n",
      "Epoch 513: Training Loss 3.0396714210510254, Validation Loss 3.0396714210510254, Validation Accuracy 0.0000\n",
      "Epoch 514: Training Loss 3.0581142902374268, Validation Loss 3.0581142902374268, Validation Accuracy 0.0000\n",
      "Epoch 515: Training Loss 3.0755629539489746, Validation Loss 3.0755629539489746, Validation Accuracy 0.0000\n",
      "Epoch 516: Training Loss 3.0593411922454834, Validation Loss 3.0593411922454834, Validation Accuracy 0.0000\n",
      "Epoch 517: Training Loss 3.044527053833008, Validation Loss 3.044527053833008, Validation Accuracy 0.0000\n",
      "Epoch 518: Training Loss 3.0657145977020264, Validation Loss 3.0657145977020264, Validation Accuracy 0.0000\n",
      "Epoch 519: Training Loss 3.0858569145202637, Validation Loss 3.0858569145202637, Validation Accuracy 0.0000\n",
      "Epoch 520: Training Loss 3.10274600982666, Validation Loss 3.10274600982666, Validation Accuracy 0.0000\n",
      "Epoch 521: Training Loss 3.1187684535980225, Validation Loss 3.1187684535980225, Validation Accuracy 0.0000\n",
      "Epoch 522: Training Loss 3.133992910385132, Validation Loss 3.133992910385132, Validation Accuracy 0.0000\n",
      "Epoch 523: Training Loss 3.1164844036102295, Validation Loss 3.1164844036102295, Validation Accuracy 0.0000\n",
      "Epoch 524: Training Loss 3.100450038909912, Validation Loss 3.100450038909912, Validation Accuracy 0.0000\n",
      "Epoch 525: Training Loss 3.119722843170166, Validation Loss 3.119722843170166, Validation Accuracy 0.0000\n",
      "Epoch 526: Training Loss 3.138094425201416, Validation Loss 3.138094425201416, Validation Accuracy 0.0000\n",
      "Epoch 527: Training Loss 3.1556248664855957, Validation Loss 3.1556248664855957, Validation Accuracy 0.0000\n",
      "Epoch 528: Training Loss 3.170090675354004, Validation Loss 3.170090675354004, Validation Accuracy 0.0000\n",
      "Epoch 529: Training Loss 3.183875799179077, Validation Loss 3.183875799179077, Validation Accuracy 0.0000\n",
      "Epoch 530: Training Loss 3.165766716003418, Validation Loss 3.165766716003418, Validation Accuracy 0.0000\n",
      "Epoch 531: Training Loss 3.180180072784424, Validation Loss 3.180180072784424, Validation Accuracy 0.0000\n",
      "Epoch 532: Training Loss 3.162961483001709, Validation Loss 3.162961483001709, Validation Accuracy 0.0000\n",
      "Epoch 533: Training Loss 3.1802480220794678, Validation Loss 3.1802480220794678, Validation Accuracy 0.0000\n",
      "Epoch 534: Training Loss 3.194457530975342, Validation Loss 3.194457530975342, Validation Accuracy 0.0000\n",
      "Epoch 535: Training Loss 3.2103559970855713, Validation Loss 3.2103559970855713, Validation Accuracy 0.0000\n",
      "Epoch 536: Training Loss 3.225585699081421, Validation Loss 3.225585699081421, Validation Accuracy 0.0000\n",
      "Epoch 537: Training Loss 3.2073817253112793, Validation Loss 3.2073817253112793, Validation Accuracy 0.0000\n",
      "Epoch 538: Training Loss 3.220940589904785, Validation Loss 3.220940589904785, Validation Accuracy 0.0000\n",
      "Epoch 539: Training Loss 3.203662395477295, Validation Loss 3.203662395477295, Validation Accuracy 0.0000\n",
      "Epoch 540: Training Loss 3.217787265777588, Validation Loss 3.217787265777588, Validation Accuracy 0.0000\n",
      "Epoch 541: Training Loss 3.2335715293884277, Validation Loss 3.2335715293884277, Validation Accuracy 0.0000\n",
      "Epoch 542: Training Loss 3.2165277004241943, Validation Loss 3.2165277004241943, Validation Accuracy 0.0000\n",
      "Epoch 543: Training Loss 3.200845241546631, Validation Loss 3.200845241546631, Validation Accuracy 0.0000\n",
      "Epoch 544: Training Loss 3.186427116394043, Validation Loss 3.186427116394043, Validation Accuracy 0.0000\n",
      "Epoch 545: Training Loss 3.1731820106506348, Validation Loss 3.1731820106506348, Validation Accuracy 0.0000\n",
      "Epoch 546: Training Loss 3.1901824474334717, Validation Loss 3.1901824474334717, Validation Accuracy 0.0000\n",
      "Epoch 547: Training Loss 3.177262544631958, Validation Loss 3.177262544631958, Validation Accuracy 0.0000\n",
      "Epoch 548: Training Loss 3.1654062271118164, Validation Loss 3.1654062271118164, Validation Accuracy 0.0000\n",
      "Epoch 549: Training Loss 3.1855430603027344, Validation Loss 3.1855430603027344, Validation Accuracy 0.0000\n",
      "Epoch 550: Training Loss 3.173815965652466, Validation Loss 3.173815965652466, Validation Accuracy 0.0000\n",
      "Epoch 551: Training Loss 3.163062334060669, Validation Loss 3.163062334060669, Validation Accuracy 0.0000\n",
      "Epoch 552: Training Loss 3.1532154083251953, Validation Loss 3.1532154083251953, Validation Accuracy 0.0000\n",
      "Epoch 553: Training Loss 3.1746838092803955, Validation Loss 3.1746838092803955, Validation Accuracy 0.0000\n",
      "Epoch 554: Training Loss 3.195120096206665, Validation Loss 3.195120096206665, Validation Accuracy 0.0000\n",
      "Epoch 555: Training Loss 3.2124569416046143, Validation Loss 3.2124569416046143, Validation Accuracy 0.0000\n",
      "Epoch 556: Training Loss 3.231105327606201, Validation Loss 3.231105327606201, Validation Accuracy 0.0000\n",
      "Epoch 557: Training Loss 3.248918056488037, Validation Loss 3.248918056488037, Validation Accuracy 0.0000\n",
      "Epoch 558: Training Loss 3.235863447189331, Validation Loss 3.235863447189331, Validation Accuracy 0.0000\n",
      "Epoch 559: Training Loss 3.251648426055908, Validation Loss 3.251648426055908, Validation Accuracy 0.0000\n",
      "Epoch 560: Training Loss 3.266681671142578, Validation Loss 3.266681671142578, Validation Accuracy 0.0000\n",
      "Epoch 561: Training Loss 3.2831971645355225, Validation Loss 3.2831971645355225, Validation Accuracy 0.0000\n",
      "Epoch 562: Training Loss 3.2692673206329346, Validation Loss 3.2692673206329346, Validation Accuracy 0.0000\n",
      "Epoch 563: Training Loss 3.256420612335205, Validation Loss 3.256420612335205, Validation Accuracy 0.0000\n",
      "Epoch 564: Training Loss 3.273995876312256, Validation Loss 3.273995876312256, Validation Accuracy 0.0000\n",
      "Epoch 565: Training Loss 3.2887091636657715, Validation Loss 3.2887091636657715, Validation Accuracy 0.0000\n",
      "Epoch 566: Training Loss 3.2755892276763916, Validation Loss 3.2755892276763916, Validation Accuracy 0.0000\n",
      "Epoch 567: Training Loss 3.2634873390197754, Validation Loss 3.2634873390197754, Validation Accuracy 0.0000\n",
      "Epoch 568: Training Loss 3.2791976928710938, Validation Loss 3.2791976928710938, Validation Accuracy 0.0000\n",
      "Epoch 569: Training Loss 3.294171094894409, Validation Loss 3.294171094894409, Validation Accuracy 0.0000\n",
      "Epoch 570: Training Loss 3.281749725341797, Validation Loss 3.281749725341797, Validation Accuracy 0.0000\n",
      "Epoch 571: Training Loss 3.296855926513672, Validation Loss 3.296855926513672, Validation Accuracy 0.0000\n",
      "Epoch 572: Training Loss 3.3134589195251465, Validation Loss 3.3134589195251465, Validation Accuracy 0.0000\n",
      "Epoch 573: Training Loss 3.300767421722412, Validation Loss 3.300767421722412, Validation Accuracy 0.0000\n",
      "Epoch 574: Training Loss 3.289048194885254, Validation Loss 3.289048194885254, Validation Accuracy 0.0000\n",
      "Epoch 575: Training Loss 3.2782351970672607, Validation Loss 3.2782351970672607, Validation Accuracy 0.0000\n",
      "Epoch 576: Training Loss 3.294299602508545, Validation Loss 3.294299602508545, Validation Accuracy 0.0000\n",
      "Epoch 577: Training Loss 3.309612274169922, Validation Loss 3.309612274169922, Validation Accuracy 0.0000\n",
      "Epoch 578: Training Loss 3.2983505725860596, Validation Loss 3.2983505725860596, Validation Accuracy 0.0000\n",
      "Epoch 579: Training Loss 3.3137028217315674, Validation Loss 3.3137028217315674, Validation Accuracy 0.0000\n",
      "Epoch 580: Training Loss 3.302687406539917, Validation Loss 3.302687406539917, Validation Accuracy 0.0000\n",
      "Epoch 581: Training Loss 3.3180618286132812, Validation Loss 3.3180618286132812, Validation Accuracy 0.0000\n",
      "Epoch 582: Training Loss 3.3072726726531982, Validation Loss 3.3072726726531982, Validation Accuracy 0.0000\n",
      "Epoch 583: Training Loss 3.322654962539673, Validation Loss 3.322654962539673, Validation Accuracy 0.0000\n",
      "Epoch 584: Training Loss 3.312077283859253, Validation Loss 3.312077283859253, Validation Accuracy 0.0000\n",
      "Epoch 585: Training Loss 3.3274548053741455, Validation Loss 3.3274548053741455, Validation Accuracy 0.0000\n",
      "Epoch 586: Training Loss 3.3170723915100098, Validation Loss 3.3170723915100098, Validation Accuracy 0.0000\n",
      "Epoch 587: Training Loss 3.3324358463287354, Validation Loss 3.3324358463287354, Validation Accuracy 0.0000\n",
      "Epoch 588: Training Loss 3.3493807315826416, Validation Loss 3.3493807315826416, Validation Accuracy 0.0000\n",
      "Epoch 589: Training Loss 3.3385605812072754, Validation Loss 3.3385605812072754, Validation Accuracy 0.0000\n",
      "Epoch 590: Training Loss 3.328557014465332, Validation Loss 3.328557014465332, Validation Accuracy 0.0000\n",
      "Epoch 591: Training Loss 3.3193178176879883, Validation Loss 3.3193178176879883, Validation Accuracy 0.0000\n",
      "Epoch 592: Training Loss 3.310793876647949, Validation Loss 3.310793876647949, Validation Accuracy 0.0000\n",
      "Epoch 593: Training Loss 3.329519033432007, Validation Loss 3.329519033432007, Validation Accuracy 0.0000\n",
      "Epoch 594: Training Loss 3.3452556133270264, Validation Loss 3.3452556133270264, Validation Accuracy 0.0000\n",
      "Epoch 595: Training Loss 3.3624939918518066, Validation Loss 3.3624939918518066, Validation Accuracy 0.0000\n",
      "Epoch 596: Training Loss 3.3768467903137207, Validation Loss 3.3768467903137207, Validation Accuracy 0.0000\n",
      "Epoch 597: Training Loss 3.366461992263794, Validation Loss 3.366461992263794, Validation Accuracy 0.0000\n",
      "Epoch 598: Training Loss 3.3830418586730957, Validation Loss 3.3830418586730957, Validation Accuracy 0.0000\n",
      "Epoch 599: Training Loss 3.398956298828125, Validation Loss 3.398956298828125, Validation Accuracy 0.0000\n",
      "Epoch 600: Training Loss 3.41424298286438, Validation Loss 3.41424298286438, Validation Accuracy 0.0000\n",
      "Epoch 601: Training Loss 3.402960777282715, Validation Loss 3.402960777282715, Validation Accuracy 0.0000\n",
      "Epoch 602: Training Loss 3.4162425994873047, Validation Loss 3.4162425994873047, Validation Accuracy 0.0000\n",
      "Epoch 603: Training Loss 3.405291795730591, Validation Loss 3.405291795730591, Validation Accuracy 0.0000\n",
      "Epoch 604: Training Loss 3.3951292037963867, Validation Loss 3.3951292037963867, Validation Accuracy 0.0000\n",
      "Epoch 605: Training Loss 3.4091575145721436, Validation Loss 3.4091575145721436, Validation Accuracy 0.0000\n",
      "Epoch 606: Training Loss 3.422600507736206, Validation Loss 3.422600507736206, Validation Accuracy 0.0000\n",
      "Epoch 607: Training Loss 3.435497999191284, Validation Loss 3.435497999191284, Validation Accuracy 0.0000\n",
      "Epoch 608: Training Loss 3.4500296115875244, Validation Loss 3.4500296115875244, Validation Accuracy 0.0000\n",
      "Epoch 609: Training Loss 3.464022159576416, Validation Loss 3.464022159576416, Validation Accuracy 0.0000\n",
      "Epoch 610: Training Loss 3.452216386795044, Validation Loss 3.452216386795044, Validation Accuracy 0.0000\n",
      "Epoch 611: Training Loss 3.466386556625366, Validation Loss 3.466386556625366, Validation Accuracy 0.0000\n",
      "Epoch 612: Training Loss 3.4549505710601807, Validation Loss 3.4549505710601807, Validation Accuracy 0.0000\n",
      "Epoch 613: Training Loss 3.4692656993865967, Validation Loss 3.4692656993865967, Validation Accuracy 0.0000\n",
      "Epoch 614: Training Loss 3.4830565452575684, Validation Loss 3.4830565452575684, Validation Accuracy 0.0000\n",
      "Epoch 615: Training Loss 3.496351957321167, Validation Loss 3.496351957321167, Validation Accuracy 0.0000\n",
      "Epoch 616: Training Loss 3.4844110012054443, Validation Loss 3.4844110012054443, Validation Accuracy 0.0000\n",
      "Epoch 617: Training Loss 3.4979145526885986, Validation Loss 3.4979145526885986, Validation Accuracy 0.0000\n",
      "Epoch 618: Training Loss 3.5109410285949707, Validation Loss 3.5109410285949707, Validation Accuracy 0.0000\n",
      "Epoch 619: Training Loss 3.5215988159179688, Validation Loss 3.5215988159179688, Validation Accuracy 0.0000\n",
      "Epoch 620: Training Loss 3.5338034629821777, Validation Loss 3.5338034629821777, Validation Accuracy 0.0000\n",
      "Epoch 621: Training Loss 3.5211868286132812, Validation Loss 3.5211868286132812, Validation Accuracy 0.0000\n",
      "Epoch 622: Training Loss 3.5336811542510986, Validation Loss 3.5336811542510986, Validation Accuracy 0.0000\n",
      "Epoch 623: Training Loss 3.545752763748169, Validation Loss 3.545752763748169, Validation Accuracy 0.0000\n",
      "Epoch 624: Training Loss 3.5332701206207275, Validation Loss 3.5332701206207275, Validation Accuracy 0.0000\n",
      "Epoch 625: Training Loss 3.545624256134033, Validation Loss 3.545624256134033, Validation Accuracy 0.0000\n",
      "Epoch 626: Training Loss 3.557565927505493, Validation Loss 3.557565927505493, Validation Accuracy 0.0000\n",
      "Epoch 627: Training Loss 3.545212507247925, Validation Loss 3.545212507247925, Validation Accuracy 0.0000\n",
      "Epoch 628: Training Loss 3.5336666107177734, Validation Loss 3.5336666107177734, Validation Accuracy 0.0000\n",
      "Epoch 629: Training Loss 3.544724464416504, Validation Loss 3.544724464416504, Validation Accuracy 0.0000\n",
      "Epoch 630: Training Loss 3.5335841178894043, Validation Loss 3.5335841178894043, Validation Accuracy 0.0000\n",
      "Epoch 631: Training Loss 3.5231757164001465, Validation Loss 3.5231757164001465, Validation Accuracy 0.0000\n",
      "Epoch 632: Training Loss 3.535022497177124, Validation Loss 3.535022497177124, Validation Accuracy 0.0000\n",
      "Epoch 633: Training Loss 3.5464229583740234, Validation Loss 3.5464229583740234, Validation Accuracy 0.0000\n",
      "Epoch 634: Training Loss 3.5574042797088623, Validation Loss 3.5574042797088623, Validation Accuracy 0.0000\n",
      "Epoch 635: Training Loss 3.5679922103881836, Validation Loss 3.5679922103881836, Validation Accuracy 0.0000\n",
      "Epoch 636: Training Loss 3.55684232711792, Validation Loss 3.55684232711792, Validation Accuracy 0.0000\n",
      "Epoch 637: Training Loss 3.5464119911193848, Validation Loss 3.5464119911193848, Validation Accuracy 0.0000\n",
      "Epoch 638: Training Loss 3.5578019618988037, Validation Loss 3.5578019618988037, Validation Accuracy 0.0000\n",
      "Epoch 639: Training Loss 3.568777561187744, Validation Loss 3.568777561187744, Validation Accuracy 0.0000\n",
      "Epoch 640: Training Loss 3.5793635845184326, Validation Loss 3.5793635845184326, Validation Accuracy 0.0000\n",
      "Epoch 641: Training Loss 3.5914466381073, Validation Loss 3.5914466381073, Validation Accuracy 0.0000\n",
      "Epoch 642: Training Loss 3.603135585784912, Validation Loss 3.603135585784912, Validation Accuracy 0.0000\n",
      "Epoch 643: Training Loss 3.612635374069214, Validation Loss 3.612635374069214, Validation Accuracy 0.0000\n",
      "Epoch 644: Training Loss 3.6009340286254883, Validation Loss 3.6009340286254883, Validation Accuracy 0.0000\n",
      "Epoch 645: Training Loss 3.5899674892425537, Validation Loss 3.5899674892425537, Validation Accuracy 0.0000\n",
      "Epoch 646: Training Loss 3.6021690368652344, Validation Loss 3.6021690368652344, Validation Accuracy 0.0000\n",
      "Epoch 647: Training Loss 3.612172842025757, Validation Loss 3.612172842025757, Validation Accuracy 0.0000\n",
      "Epoch 648: Training Loss 3.621842861175537, Validation Loss 3.621842861175537, Validation Accuracy 0.0000\n",
      "Epoch 649: Training Loss 3.6106629371643066, Validation Loss 3.6106629371643066, Validation Accuracy 0.0000\n",
      "Epoch 650: Training Loss 3.622406482696533, Validation Loss 3.622406482696533, Validation Accuracy 0.0000\n",
      "Epoch 651: Training Loss 3.633777618408203, Validation Loss 3.633777618408203, Validation Accuracy 0.0000\n",
      "Epoch 652: Training Loss 3.6447949409484863, Validation Loss 3.6447949409484863, Validation Accuracy 0.0000\n",
      "Epoch 653: Training Loss 3.653731107711792, Validation Loss 3.653731107711792, Validation Accuracy 0.0000\n",
      "Epoch 654: Training Loss 3.6421027183532715, Validation Loss 3.6421027183532715, Validation Accuracy 0.0000\n",
      "Epoch 655: Training Loss 3.631183624267578, Validation Loss 3.631183624267578, Validation Accuracy 0.0000\n",
      "Epoch 656: Training Loss 3.6209347248077393, Validation Loss 3.6209347248077393, Validation Accuracy 0.0000\n",
      "Epoch 657: Training Loss 3.611318588256836, Validation Loss 3.611318588256836, Validation Accuracy 0.0000\n",
      "Epoch 658: Training Loss 3.602299451828003, Validation Loss 3.602299451828003, Validation Accuracy 0.0000\n",
      "Epoch 659: Training Loss 3.6136045455932617, Validation Loss 3.6136045455932617, Validation Accuracy 0.0000\n",
      "Epoch 660: Training Loss 3.6262569427490234, Validation Loss 3.6262569427490234, Validation Accuracy 0.0000\n",
      "Epoch 661: Training Loss 3.636765956878662, Validation Loss 3.636765956878662, Validation Accuracy 0.0000\n",
      "Epoch 662: Training Loss 3.62728214263916, Validation Loss 3.62728214263916, Validation Accuracy 0.0000\n",
      "Epoch 663: Training Loss 3.6183812618255615, Validation Loss 3.6183812618255615, Validation Accuracy 0.0000\n",
      "Epoch 664: Training Loss 3.6312108039855957, Validation Loss 3.6312108039855957, Validation Accuracy 0.0000\n",
      "Epoch 665: Training Loss 3.6224911212921143, Validation Loss 3.6224911212921143, Validation Accuracy 0.0000\n",
      "Epoch 666: Training Loss 3.633639335632324, Validation Loss 3.633639335632324, Validation Accuracy 0.0000\n",
      "Epoch 667: Training Loss 3.6443986892700195, Validation Loss 3.6443986892700195, Validation Accuracy 0.0000\n",
      "Epoch 668: Training Loss 3.654790163040161, Validation Loss 3.654790163040161, Validation Accuracy 0.0000\n",
      "Epoch 669: Training Loss 3.664835214614868, Validation Loss 3.664835214614868, Validation Accuracy 0.0000\n",
      "Epoch 670: Training Loss 3.6553947925567627, Validation Loss 3.6553947925567627, Validation Accuracy 0.0000\n",
      "Epoch 671: Training Loss 3.6465225219726562, Validation Loss 3.6465225219726562, Validation Accuracy 0.0000\n",
      "Epoch 672: Training Loss 3.65889835357666, Validation Loss 3.65889835357666, Validation Accuracy 0.0000\n",
      "Epoch 673: Training Loss 3.670877695083618, Validation Loss 3.670877695083618, Validation Accuracy 0.0000\n",
      "Epoch 674: Training Loss 3.6824800968170166, Validation Loss 3.6824800968170166, Validation Accuracy 0.0000\n",
      "Epoch 675: Training Loss 3.673165798187256, Validation Loss 3.673165798187256, Validation Accuracy 0.0000\n",
      "Epoch 676: Training Loss 3.6848692893981934, Validation Loss 3.6848692893981934, Validation Accuracy 0.0000\n",
      "Epoch 677: Training Loss 3.6757941246032715, Validation Loss 3.6757941246032715, Validation Accuracy 0.0000\n",
      "Epoch 678: Training Loss 3.6875803470611572, Validation Loss 3.6875803470611572, Validation Accuracy 0.0000\n",
      "Epoch 679: Training Loss 3.6990013122558594, Validation Loss 3.6990013122558594, Validation Accuracy 0.0000\n",
      "Epoch 680: Training Loss 3.7084639072418213, Validation Loss 3.7084639072418213, Validation Accuracy 0.0000\n",
      "Epoch 681: Training Loss 3.7176284790039062, Validation Loss 3.7176284790039062, Validation Accuracy 0.0000\n",
      "Epoch 682: Training Loss 3.726511001586914, Validation Loss 3.726511001586914, Validation Accuracy 0.0000\n",
      "Epoch 683: Training Loss 3.736755847930908, Validation Loss 3.736755847930908, Validation Accuracy 0.0000\n",
      "Epoch 684: Training Loss 3.7467100620269775, Validation Loss 3.7467100620269775, Validation Accuracy 0.0000\n",
      "Epoch 685: Training Loss 3.7563865184783936, Validation Loss 3.7563865184783936, Validation Accuracy 0.0000\n",
      "Epoch 686: Training Loss 3.765796661376953, Validation Loss 3.765796661376953, Validation Accuracy 0.0000\n",
      "Epoch 687: Training Loss 3.7749552726745605, Validation Loss 3.7749552726745605, Validation Accuracy 0.0000\n",
      "Epoch 688: Training Loss 3.7639598846435547, Validation Loss 3.7639598846435547, Validation Accuracy 0.0000\n",
      "Epoch 689: Training Loss 3.773380756378174, Validation Loss 3.773380756378174, Validation Accuracy 0.0000\n",
      "Epoch 690: Training Loss 3.7627878189086914, Validation Loss 3.7627878189086914, Validation Accuracy 0.0000\n",
      "Epoch 691: Training Loss 3.770923137664795, Validation Loss 3.770923137664795, Validation Accuracy 0.0000\n",
      "Epoch 692: Training Loss 3.7803494930267334, Validation Loss 3.7803494930267334, Validation Accuracy 0.0000\n",
      "Epoch 693: Training Loss 3.76993989944458, Validation Loss 3.76993989944458, Validation Accuracy 0.0000\n",
      "Epoch 694: Training Loss 3.760106325149536, Validation Loss 3.760106325149536, Validation Accuracy 0.0000\n",
      "Epoch 695: Training Loss 3.768702507019043, Validation Loss 3.768702507019043, Validation Accuracy 0.0000\n",
      "Epoch 696: Training Loss 3.7591967582702637, Validation Loss 3.7591967582702637, Validation Accuracy 0.0000\n",
      "Epoch 697: Training Loss 3.7694849967956543, Validation Loss 3.7694849967956543, Validation Accuracy 0.0000\n",
      "Epoch 698: Training Loss 3.7779934406280518, Validation Loss 3.7779934406280518, Validation Accuracy 0.0000\n",
      "Epoch 699: Training Loss 3.7862517833709717, Validation Loss 3.7862517833709717, Validation Accuracy 0.0000\n",
      "Epoch 700: Training Loss 3.7957770824432373, Validation Loss 3.7957770824432373, Validation Accuracy 0.0000\n",
      "Epoch 701: Training Loss 3.8050448894500732, Validation Loss 3.8050448894500732, Validation Accuracy 0.0000\n",
      "Epoch 702: Training Loss 3.7949860095977783, Validation Loss 3.7949860095977783, Validation Accuracy 0.0000\n",
      "Epoch 703: Training Loss 3.7854721546173096, Validation Loss 3.7854721546173096, Validation Accuracy 0.0000\n",
      "Epoch 704: Training Loss 3.7764761447906494, Validation Loss 3.7764761447906494, Validation Accuracy 0.0000\n",
      "Epoch 705: Training Loss 3.7867839336395264, Validation Loss 3.7867839336395264, Validation Accuracy 0.0000\n",
      "Epoch 706: Training Loss 3.778031349182129, Validation Loss 3.778031349182129, Validation Accuracy 0.0000\n",
      "Epoch 707: Training Loss 3.7884442806243896, Validation Loss 3.7884442806243896, Validation Accuracy 0.0000\n",
      "Epoch 708: Training Loss 3.79856276512146, Validation Loss 3.79856276512146, Validation Accuracy 0.0000\n",
      "Epoch 709: Training Loss 3.8069868087768555, Validation Loss 3.8069868087768555, Validation Accuracy 0.0000\n",
      "Epoch 710: Training Loss 3.81658673286438, Validation Loss 3.81658673286438, Validation Accuracy 0.0000\n",
      "Epoch 711: Training Loss 3.8073933124542236, Validation Loss 3.8073933124542236, Validation Accuracy 0.0000\n",
      "Epoch 712: Training Loss 3.817141056060791, Validation Loss 3.817141056060791, Validation Accuracy 0.0000\n",
      "Epoch 713: Training Loss 3.825237989425659, Validation Loss 3.825237989425659, Validation Accuracy 0.0000\n",
      "Epoch 714: Training Loss 3.8331077098846436, Validation Loss 3.8331077098846436, Validation Accuracy 0.0000\n",
      "Epoch 715: Training Loss 3.8238017559051514, Validation Loss 3.8238017559051514, Validation Accuracy 0.0000\n",
      "Epoch 716: Training Loss 3.8318607807159424, Validation Loss 3.8318607807159424, Validation Accuracy 0.0000\n",
      "Epoch 717: Training Loss 3.841097831726074, Validation Loss 3.841097831726074, Validation Accuracy 0.0000\n",
      "Epoch 718: Training Loss 3.8500940799713135, Validation Loss 3.8500940799713135, Validation Accuracy 0.0000\n",
      "Epoch 719: Training Loss 3.8588597774505615, Validation Loss 3.8588597774505615, Validation Accuracy 0.0000\n",
      "Epoch 720: Training Loss 3.867405891418457, Validation Loss 3.867405891418457, Validation Accuracy 0.0000\n",
      "Epoch 721: Training Loss 3.8743956089019775, Validation Loss 3.8743956089019775, Validation Accuracy 0.0000\n",
      "Epoch 722: Training Loss 3.8812074661254883, Validation Loss 3.8812074661254883, Validation Accuracy 0.0000\n",
      "Epoch 723: Training Loss 3.8892033100128174, Validation Loss 3.8892033100128174, Validation Accuracy 0.0000\n",
      "Epoch 724: Training Loss 3.897010087966919, Validation Loss 3.897010087966919, Validation Accuracy 0.0000\n",
      "Epoch 725: Training Loss 3.886672019958496, Validation Loss 3.886672019958496, Validation Accuracy 0.0000\n",
      "Epoch 726: Training Loss 3.876858711242676, Validation Loss 3.876858711242676, Validation Accuracy 0.0000\n",
      "Epoch 727: Training Loss 3.8853230476379395, Validation Loss 3.8853230476379395, Validation Accuracy 0.0000\n",
      "Epoch 728: Training Loss 3.8922722339630127, Validation Loss 3.8922722339630127, Validation Accuracy 0.0000\n",
      "Epoch 729: Training Loss 3.899045705795288, Validation Loss 3.899045705795288, Validation Accuracy 0.0000\n",
      "Epoch 730: Training Loss 3.906967878341675, Validation Loss 3.906967878341675, Validation Accuracy 0.0000\n",
      "Epoch 731: Training Loss 3.9134018421173096, Validation Loss 3.9134018421173096, Validation Accuracy 0.0000\n",
      "Epoch 732: Training Loss 3.9033617973327637, Validation Loss 3.9033617973327637, Validation Accuracy 0.0000\n",
      "Epoch 733: Training Loss 3.9100542068481445, Validation Loss 3.9100542068481445, Validation Accuracy 0.0000\n",
      "Epoch 734: Training Loss 3.9003803730010986, Validation Loss 3.9003803730010986, Validation Accuracy 0.0000\n",
      "Epoch 735: Training Loss 3.9073047637939453, Validation Loss 3.9073047637939453, Validation Accuracy 0.0000\n",
      "Epoch 736: Training Loss 3.915369749069214, Validation Loss 3.915369749069214, Validation Accuracy 0.0000\n",
      "Epoch 737: Training Loss 3.9058802127838135, Validation Loss 3.9058802127838135, Validation Accuracy 0.0000\n",
      "Epoch 738: Training Loss 3.914142608642578, Validation Loss 3.914142608642578, Validation Accuracy 0.0000\n",
      "Epoch 739: Training Loss 3.9222073554992676, Validation Loss 3.9222073554992676, Validation Accuracy 0.0000\n",
      "Epoch 740: Training Loss 3.9300811290740967, Validation Loss 3.9300811290740967, Validation Accuracy 0.0000\n",
      "Epoch 741: Training Loss 3.936514139175415, Validation Loss 3.936514139175415, Validation Accuracy 0.0000\n",
      "Epoch 742: Training Loss 3.9427921772003174, Validation Loss 3.9427921772003174, Validation Accuracy 0.0000\n",
      "Epoch 743: Training Loss 3.9489243030548096, Validation Loss 3.9489243030548096, Validation Accuracy 0.0000\n",
      "Epoch 744: Training Loss 3.9549155235290527, Validation Loss 3.9549155235290527, Validation Accuracy 0.0000\n",
      "Epoch 745: Training Loss 3.9620473384857178, Validation Loss 3.9620473384857178, Validation Accuracy 0.0000\n",
      "Epoch 746: Training Loss 3.967764377593994, Validation Loss 3.967764377593994, Validation Accuracy 0.0000\n",
      "Epoch 747: Training Loss 3.95760440826416, Validation Loss 3.95760440826416, Validation Accuracy 0.0000\n",
      "Epoch 748: Training Loss 3.964860439300537, Validation Loss 3.964860439300537, Validation Accuracy 0.0000\n",
      "Epoch 749: Training Loss 3.9719595909118652, Validation Loss 3.9719595909118652, Validation Accuracy 0.0000\n",
      "Epoch 750: Training Loss 3.9620463848114014, Validation Loss 3.9620463848114014, Validation Accuracy 0.0000\n",
      "Epoch 751: Training Loss 3.9681413173675537, Validation Loss 3.9681413173675537, Validation Accuracy 0.0000\n",
      "Epoch 752: Training Loss 3.9740967750549316, Validation Loss 3.9740967750549316, Validation Accuracy 0.0000\n",
      "Epoch 753: Training Loss 3.9799203872680664, Validation Loss 3.9799203872680664, Validation Accuracy 0.0000\n",
      "Epoch 754: Training Loss 3.9856152534484863, Validation Loss 3.9856152534484863, Validation Accuracy 0.0000\n",
      "Epoch 755: Training Loss 3.9911880493164062, Validation Loss 3.9911880493164062, Validation Accuracy 0.0000\n",
      "Epoch 756: Training Loss 3.9978981018066406, Validation Loss 3.9978981018066406, Validation Accuracy 0.0000\n",
      "Epoch 757: Training Loss 4.003229141235352, Validation Loss 4.003229141235352, Validation Accuracy 0.0000\n",
      "Epoch 758: Training Loss 4.009696006774902, Validation Loss 4.009696006774902, Validation Accuracy 0.0000\n",
      "Epoch 759: Training Loss 3.9994752407073975, Validation Loss 3.9994752407073975, Validation Accuracy 0.0000\n",
      "Epoch 760: Training Loss 3.989731788635254, Validation Loss 3.989731788635254, Validation Accuracy 0.0000\n",
      "Epoch 761: Training Loss 3.980445146560669, Validation Loss 3.980445146560669, Validation Accuracy 0.0000\n",
      "Epoch 762: Training Loss 3.9715919494628906, Validation Loss 3.9715919494628906, Validation Accuracy 0.0000\n",
      "Epoch 763: Training Loss 3.963156223297119, Validation Loss 3.963156223297119, Validation Accuracy 0.0000\n",
      "Epoch 764: Training Loss 3.9712777137756348, Validation Loss 3.9712777137756348, Validation Accuracy 0.0000\n",
      "Epoch 765: Training Loss 3.9779913425445557, Validation Loss 3.9779913425445557, Validation Accuracy 0.0000\n",
      "Epoch 766: Training Loss 3.9857659339904785, Validation Loss 3.9857659339904785, Validation Accuracy 0.0000\n",
      "Epoch 767: Training Loss 3.9772896766662598, Validation Loss 3.9772896766662598, Validation Accuracy 0.0000\n",
      "Epoch 768: Training Loss 3.9852113723754883, Validation Loss 3.9852113723754883, Validation Accuracy 0.0000\n",
      "Epoch 769: Training Loss 3.9769794940948486, Validation Loss 3.9769794940948486, Validation Accuracy 0.0000\n",
      "Epoch 770: Training Loss 3.9850351810455322, Validation Loss 3.9850351810455322, Validation Accuracy 0.0000\n",
      "Epoch 771: Training Loss 3.9917192459106445, Validation Loss 3.9917192459106445, Validation Accuracy 0.0000\n",
      "Epoch 772: Training Loss 3.983581066131592, Validation Loss 3.983581066131592, Validation Accuracy 0.0000\n",
      "Epoch 773: Training Loss 3.975818395614624, Validation Loss 3.975818395614624, Validation Accuracy 0.0000\n",
      "Epoch 774: Training Loss 3.9829516410827637, Validation Loss 3.9829516410827637, Validation Accuracy 0.0000\n",
      "Epoch 775: Training Loss 3.9910972118377686, Validation Loss 3.9910972118377686, Validation Accuracy 0.0000\n",
      "Epoch 776: Training Loss 3.983386278152466, Validation Loss 3.983386278152466, Validation Accuracy 0.0000\n",
      "Epoch 777: Training Loss 3.9904580116271973, Validation Loss 3.9904580116271973, Validation Accuracy 0.0000\n",
      "Epoch 778: Training Loss 3.9973552227020264, Validation Loss 3.9973552227020264, Validation Accuracy 0.0000\n",
      "Epoch 779: Training Loss 4.004084587097168, Validation Loss 4.004084587097168, Validation Accuracy 0.0000\n",
      "Epoch 780: Training Loss 3.996302604675293, Validation Loss 3.996302604675293, Validation Accuracy 0.0000\n",
      "Epoch 781: Training Loss 4.0031657218933105, Validation Loss 4.0031657218933105, Validation Accuracy 0.0000\n",
      "Epoch 782: Training Loss 3.9955992698669434, Validation Loss 3.9955992698669434, Validation Accuracy 0.0000\n",
      "Epoch 783: Training Loss 3.9883792400360107, Validation Loss 3.9883792400360107, Validation Accuracy 0.0000\n",
      "Epoch 784: Training Loss 3.995635747909546, Validation Loss 3.995635747909546, Validation Accuracy 0.0000\n",
      "Epoch 785: Training Loss 4.002712249755859, Validation Loss 4.002712249755859, Validation Accuracy 0.0000\n",
      "Epoch 786: Training Loss 3.995521068572998, Validation Loss 3.995521068572998, Validation Accuracy 0.0000\n",
      "Epoch 787: Training Loss 4.003894329071045, Validation Loss 4.003894329071045, Validation Accuracy 0.0000\n",
      "Epoch 788: Training Loss 4.012073040008545, Validation Loss 4.012073040008545, Validation Accuracy 0.0000\n",
      "Epoch 789: Training Loss 4.00488805770874, Validation Loss 4.00488805770874, Validation Accuracy 0.0000\n",
      "Epoch 790: Training Loss 3.9980270862579346, Validation Loss 3.9980270862579346, Validation Accuracy 0.0000\n",
      "Epoch 791: Training Loss 3.991478204727173, Validation Loss 3.991478204727173, Validation Accuracy 0.0000\n",
      "Epoch 792: Training Loss 3.999068021774292, Validation Loss 3.999068021774292, Validation Accuracy 0.0000\n",
      "Epoch 793: Training Loss 4.00764274597168, Validation Loss 4.00764274597168, Validation Accuracy 0.0000\n",
      "Epoch 794: Training Loss 4.016016006469727, Validation Loss 4.016016006469727, Validation Accuracy 0.0000\n",
      "Epoch 795: Training Loss 4.023040771484375, Validation Loss 4.023040771484375, Validation Accuracy 0.0000\n",
      "Epoch 796: Training Loss 4.031055450439453, Validation Loss 4.031055450439453, Validation Accuracy 0.0000\n",
      "Epoch 797: Training Loss 4.038890838623047, Validation Loss 4.038890838623047, Validation Accuracy 0.0000\n",
      "Epoch 798: Training Loss 4.031700134277344, Validation Loss 4.031700134277344, Validation Accuracy 0.0000\n",
      "Epoch 799: Training Loss 4.02482795715332, Validation Loss 4.02482795715332, Validation Accuracy 0.0000\n",
      "Epoch 800: Training Loss 4.018259525299072, Validation Loss 4.018259525299072, Validation Accuracy 0.0000\n",
      "Epoch 801: Training Loss 4.011984825134277, Validation Loss 4.011984825134277, Validation Accuracy 0.0000\n",
      "Epoch 802: Training Loss 4.020631790161133, Validation Loss 4.020631790161133, Validation Accuracy 0.0000\n",
      "Epoch 803: Training Loss 4.027953624725342, Validation Loss 4.027953624725342, Validation Accuracy 0.0000\n",
      "Epoch 804: Training Loss 4.021633148193359, Validation Loss 4.021633148193359, Validation Accuracy 0.0000\n",
      "Epoch 805: Training Loss 4.0301337242126465, Validation Loss 4.0301337242126465, Validation Accuracy 0.0000\n",
      "Epoch 806: Training Loss 4.03843879699707, Validation Loss 4.03843879699707, Validation Accuracy 0.0000\n",
      "Epoch 807: Training Loss 4.045447826385498, Validation Loss 4.045447826385498, Validation Accuracy 0.0000\n",
      "Epoch 808: Training Loss 4.052290439605713, Validation Loss 4.052290439605713, Validation Accuracy 0.0000\n",
      "Epoch 809: Training Loss 4.0456438064575195, Validation Loss 4.0456438064575195, Validation Accuracy 0.0000\n",
      "Epoch 810: Training Loss 4.0392866134643555, Validation Loss 4.0392866134643555, Validation Accuracy 0.0000\n",
      "Epoch 811: Training Loss 4.033206462860107, Validation Loss 4.033206462860107, Validation Accuracy 0.0000\n",
      "Epoch 812: Training Loss 4.027396202087402, Validation Loss 4.027396202087402, Validation Accuracy 0.0000\n",
      "Epoch 813: Training Loss 4.0360918045043945, Validation Loss 4.0360918045043945, Validation Accuracy 0.0000\n",
      "Epoch 814: Training Loss 4.043487548828125, Validation Loss 4.043487548828125, Validation Accuracy 0.0000\n",
      "Epoch 815: Training Loss 4.050703525543213, Validation Loss 4.050703525543213, Validation Accuracy 0.0000\n",
      "Epoch 816: Training Loss 4.044672966003418, Validation Loss 4.044672966003418, Validation Accuracy 0.0000\n",
      "Epoch 817: Training Loss 4.051933765411377, Validation Loss 4.051933765411377, Validation Accuracy 0.0000\n",
      "Epoch 818: Training Loss 4.046015739440918, Validation Loss 4.046015739440918, Validation Accuracy 0.0000\n",
      "Epoch 819: Training Loss 4.053316116333008, Validation Loss 4.053316116333008, Validation Accuracy 0.0000\n",
      "Epoch 820: Training Loss 4.0604424476623535, Validation Loss 4.0604424476623535, Validation Accuracy 0.0000\n",
      "Epoch 821: Training Loss 4.054487228393555, Validation Loss 4.054487228393555, Validation Accuracy 0.0000\n",
      "Epoch 822: Training Loss 4.061657428741455, Validation Loss 4.061657428741455, Validation Accuracy 0.0000\n",
      "Epoch 823: Training Loss 4.069780349731445, Validation Loss 4.069780349731445, Validation Accuracy 0.0000\n",
      "Epoch 824: Training Loss 4.063784599304199, Validation Loss 4.063784599304199, Validation Accuracy 0.0000\n",
      "Epoch 825: Training Loss 4.070831298828125, Validation Loss 4.070831298828125, Validation Accuracy 0.0000\n",
      "Epoch 826: Training Loss 4.077713489532471, Validation Loss 4.077713489532471, Validation Accuracy 0.0000\n",
      "Epoch 827: Training Loss 4.084437847137451, Validation Loss 4.084437847137451, Validation Accuracy 0.0000\n",
      "Epoch 828: Training Loss 4.091012954711914, Validation Loss 4.091012954711914, Validation Accuracy 0.0000\n",
      "Epoch 829: Training Loss 4.084743499755859, Validation Loss 4.084743499755859, Validation Accuracy 0.0000\n",
      "Epoch 830: Training Loss 4.091388702392578, Validation Loss 4.091388702392578, Validation Accuracy 0.0000\n",
      "Epoch 831: Training Loss 4.0978875160217285, Validation Loss 4.0978875160217285, Validation Accuracy 0.0000\n",
      "Epoch 832: Training Loss 4.104244232177734, Validation Loss 4.104244232177734, Validation Accuracy 0.0000\n",
      "Epoch 833: Training Loss 4.110463619232178, Validation Loss 4.110463619232178, Validation Accuracy 0.0000\n",
      "Epoch 834: Training Loss 4.116550922393799, Validation Loss 4.116550922393799, Validation Accuracy 0.0000\n",
      "Epoch 835: Training Loss 4.123641490936279, Validation Loss 4.123641490936279, Validation Accuracy 0.0000\n",
      "Epoch 836: Training Loss 4.130588531494141, Validation Loss 4.130588531494141, Validation Accuracy 0.0000\n",
      "Epoch 837: Training Loss 4.136290073394775, Validation Loss 4.136290073394775, Validation Accuracy 0.0000\n",
      "Epoch 838: Training Loss 4.141878128051758, Validation Loss 4.141878128051758, Validation Accuracy 0.0000\n",
      "Epoch 839: Training Loss 4.147355556488037, Validation Loss 4.147355556488037, Validation Accuracy 0.0000\n",
      "Epoch 840: Training Loss 4.152727127075195, Validation Loss 4.152727127075195, Validation Accuracy 0.0000\n",
      "Epoch 841: Training Loss 4.157996654510498, Validation Loss 4.157996654510498, Validation Accuracy 0.0000\n",
      "Epoch 842: Training Loss 4.164282321929932, Validation Loss 4.164282321929932, Validation Accuracy 0.0000\n",
      "Epoch 843: Training Loss 4.17045259475708, Validation Loss 4.17045259475708, Validation Accuracy 0.0000\n",
      "Epoch 844: Training Loss 4.175415515899658, Validation Loss 4.175415515899658, Validation Accuracy 0.0000\n",
      "Epoch 845: Training Loss 4.167912006378174, Validation Loss 4.167912006378174, Validation Accuracy 0.0000\n",
      "Epoch 846: Training Loss 4.174126148223877, Validation Loss 4.174126148223877, Validation Accuracy 0.0000\n",
      "Epoch 847: Training Loss 4.17913818359375, Validation Loss 4.17913818359375, Validation Accuracy 0.0000\n",
      "Epoch 848: Training Loss 4.185151100158691, Validation Loss 4.185151100158691, Validation Accuracy 0.0000\n",
      "Epoch 849: Training Loss 4.189975261688232, Validation Loss 4.189975261688232, Validation Accuracy 0.0000\n",
      "Epoch 850: Training Loss 4.195796966552734, Validation Loss 4.195796966552734, Validation Accuracy 0.0000\n",
      "Epoch 851: Training Loss 4.200446605682373, Validation Loss 4.200446605682373, Validation Accuracy 0.0000\n",
      "Epoch 852: Training Loss 4.206088066101074, Validation Loss 4.206088066101074, Validation Accuracy 0.0000\n",
      "Epoch 853: Training Loss 4.210573196411133, Validation Loss 4.210573196411133, Validation Accuracy 0.0000\n",
      "Epoch 854: Training Loss 4.214983940124512, Validation Loss 4.214983940124512, Validation Accuracy 0.0000\n",
      "Epoch 855: Training Loss 4.220386505126953, Validation Loss 4.220386505126953, Validation Accuracy 0.0000\n",
      "Epoch 856: Training Loss 4.224647045135498, Validation Loss 4.224647045135498, Validation Accuracy 0.0000\n",
      "Epoch 857: Training Loss 4.2298970222473145, Validation Loss 4.2298970222473145, Validation Accuracy 0.0000\n",
      "Epoch 858: Training Loss 4.234014987945557, Validation Loss 4.234014987945557, Validation Accuracy 0.0000\n",
      "Epoch 859: Training Loss 4.238071441650391, Validation Loss 4.238071441650391, Validation Accuracy 0.0000\n",
      "Epoch 860: Training Loss 4.242066860198975, Validation Loss 4.242066860198975, Validation Accuracy 0.0000\n",
      "Epoch 861: Training Loss 4.246004581451416, Validation Loss 4.246004581451416, Validation Accuracy 0.0000\n",
      "Epoch 862: Training Loss 4.237830638885498, Validation Loss 4.237830638885498, Validation Accuracy 0.0000\n",
      "Epoch 863: Training Loss 4.243011474609375, Validation Loss 4.243011474609375, Validation Accuracy 0.0000\n",
      "Epoch 864: Training Loss 4.235090255737305, Validation Loss 4.235090255737305, Validation Accuracy 0.0000\n",
      "Epoch 865: Training Loss 4.239388942718506, Validation Loss 4.239388942718506, Validation Accuracy 0.0000\n",
      "Epoch 866: Training Loss 4.231718063354492, Validation Loss 4.231718063354492, Validation Accuracy 0.0000\n",
      "Epoch 867: Training Loss 4.237229347229004, Validation Loss 4.237229347229004, Validation Accuracy 0.0000\n",
      "Epoch 868: Training Loss 4.242649078369141, Validation Loss 4.242649078369141, Validation Accuracy 0.0000\n",
      "Epoch 869: Training Loss 4.235133647918701, Validation Loss 4.235133647918701, Validation Accuracy 0.0000\n",
      "Epoch 870: Training Loss 4.23966646194458, Validation Loss 4.23966646194458, Validation Accuracy 0.0000\n",
      "Epoch 871: Training Loss 4.232375621795654, Validation Loss 4.232375621795654, Validation Accuracy 0.0000\n",
      "Epoch 872: Training Loss 4.23705530166626, Validation Loss 4.23705530166626, Validation Accuracy 0.0000\n",
      "Epoch 873: Training Loss 4.229976177215576, Validation Loss 4.229976177215576, Validation Accuracy 0.0000\n",
      "Epoch 874: Training Loss 4.234792232513428, Validation Loss 4.234792232513428, Validation Accuracy 0.0000\n",
      "Epoch 875: Training Loss 4.240558624267578, Validation Loss 4.240558624267578, Validation Accuracy 0.0000\n",
      "Epoch 876: Training Loss 4.2462263107299805, Validation Loss 4.2462263107299805, Validation Accuracy 0.0000\n",
      "Epoch 877: Training Loss 4.2517991065979, Validation Loss 4.2517991065979, Validation Accuracy 0.0000\n",
      "Epoch 878: Training Loss 4.257278919219971, Validation Loss 4.257278919219971, Validation Accuracy 0.0000\n",
      "Epoch 879: Training Loss 4.2616682052612305, Validation Loss 4.2616682052612305, Validation Accuracy 0.0000\n",
      "Epoch 880: Training Loss 4.265985488891602, Validation Loss 4.265985488891602, Validation Accuracy 0.0000\n",
      "Epoch 881: Training Loss 4.258708477020264, Validation Loss 4.258708477020264, Validation Accuracy 0.0000\n",
      "Epoch 882: Training Loss 4.263175010681152, Validation Loss 4.263175010681152, Validation Accuracy 0.0000\n",
      "Epoch 883: Training Loss 4.256109714508057, Validation Loss 4.256109714508057, Validation Accuracy 0.0000\n",
      "Epoch 884: Training Loss 4.249302387237549, Validation Loss 4.249302387237549, Validation Accuracy 0.0000\n",
      "Epoch 885: Training Loss 4.2427473068237305, Validation Loss 4.2427473068237305, Validation Accuracy 0.0000\n",
      "Epoch 886: Training Loss 4.248758316040039, Validation Loss 4.248758316040039, Validation Accuracy 0.0000\n",
      "Epoch 887: Training Loss 4.242360591888428, Validation Loss 4.242360591888428, Validation Accuracy 0.0000\n",
      "Epoch 888: Training Loss 4.247462749481201, Validation Loss 4.247462749481201, Validation Accuracy 0.0000\n",
      "Epoch 889: Training Loss 4.2534708976745605, Validation Loss 4.2534708976745605, Validation Accuracy 0.0000\n",
      "Epoch 890: Training Loss 4.258382797241211, Validation Loss 4.258382797241211, Validation Accuracy 0.0000\n",
      "Epoch 891: Training Loss 4.251989841461182, Validation Loss 4.251989841461182, Validation Accuracy 0.0000\n",
      "Epoch 892: Training Loss 4.2570061683654785, Validation Loss 4.2570061683654785, Validation Accuracy 0.0000\n",
      "Epoch 893: Training Loss 4.262928009033203, Validation Loss 4.262928009033203, Validation Accuracy 0.0000\n",
      "Epoch 894: Training Loss 4.267759799957275, Validation Loss 4.267759799957275, Validation Accuracy 0.0000\n",
      "Epoch 895: Training Loss 4.261378288269043, Validation Loss 4.261378288269043, Validation Accuracy 0.0000\n",
      "Epoch 896: Training Loss 4.2663164138793945, Validation Loss 4.2663164138793945, Validation Accuracy 0.0000\n",
      "Epoch 897: Training Loss 4.272156238555908, Validation Loss 4.272156238555908, Validation Accuracy 0.0000\n",
      "Epoch 898: Training Loss 4.27691650390625, Validation Loss 4.27691650390625, Validation Accuracy 0.0000\n",
      "Epoch 899: Training Loss 4.270549774169922, Validation Loss 4.270549774169922, Validation Accuracy 0.0000\n",
      "Epoch 900: Training Loss 4.264413356781006, Validation Loss 4.264413356781006, Validation Accuracy 0.0000\n",
      "Epoch 901: Training Loss 4.269459247589111, Validation Loss 4.269459247589111, Validation Accuracy 0.0000\n",
      "Epoch 902: Training Loss 4.275399208068848, Validation Loss 4.275399208068848, Validation Accuracy 0.0000\n",
      "Epoch 903: Training Loss 4.269326686859131, Validation Loss 4.269326686859131, Validation Accuracy 0.0000\n",
      "Epoch 904: Training Loss 4.274366855621338, Validation Loss 4.274366855621338, Validation Accuracy 0.0000\n",
      "Epoch 905: Training Loss 4.279318809509277, Validation Loss 4.279318809509277, Validation Accuracy 0.0000\n",
      "Epoch 906: Training Loss 4.273320198059082, Validation Loss 4.273320198059082, Validation Accuracy 0.0000\n",
      "Epoch 907: Training Loss 4.278359889984131, Validation Loss 4.278359889984131, Validation Accuracy 0.0000\n",
      "Epoch 908: Training Loss 4.28331184387207, Validation Loss 4.28331184387207, Validation Accuracy 0.0000\n",
      "Epoch 909: Training Loss 4.277381896972656, Validation Loss 4.277381896972656, Validation Accuracy 0.0000\n",
      "Epoch 910: Training Loss 4.2824201583862305, Validation Loss 4.2824201583862305, Validation Accuracy 0.0000\n",
      "Epoch 911: Training Loss 4.2883501052856445, Validation Loss 4.2883501052856445, Validation Accuracy 0.0000\n",
      "Epoch 912: Training Loss 4.293206691741943, Validation Loss 4.293206691741943, Validation Accuracy 0.0000\n",
      "Epoch 913: Training Loss 4.29797887802124, Validation Loss 4.29797887802124, Validation Accuracy 0.0000\n",
      "Epoch 914: Training Loss 4.291971206665039, Validation Loss 4.291971206665039, Validation Accuracy 0.0000\n",
      "Epoch 915: Training Loss 4.2978105545043945, Validation Loss 4.2978105545043945, Validation Accuracy 0.0000\n",
      "Epoch 916: Training Loss 4.30355167388916, Validation Loss 4.30355167388916, Validation Accuracy 0.0000\n",
      "Epoch 917: Training Loss 4.309197425842285, Validation Loss 4.309197425842285, Validation Accuracy 0.0000\n",
      "Epoch 918: Training Loss 4.313797473907471, Validation Loss 4.313797473907471, Validation Accuracy 0.0000\n",
      "Epoch 919: Training Loss 4.318323135375977, Validation Loss 4.318323135375977, Validation Accuracy 0.0000\n",
      "Epoch 920: Training Loss 4.31217622756958, Validation Loss 4.31217622756958, Validation Accuracy 0.0000\n",
      "Epoch 921: Training Loss 4.306243419647217, Validation Loss 4.306243419647217, Validation Accuracy 0.0000\n",
      "Epoch 922: Training Loss 4.311037063598633, Validation Loss 4.311037063598633, Validation Accuracy 0.0000\n",
      "Epoch 923: Training Loss 4.305243015289307, Validation Loss 4.305243015289307, Validation Accuracy 0.0000\n",
      "Epoch 924: Training Loss 4.311074733734131, Validation Loss 4.311074733734131, Validation Accuracy 0.0000\n",
      "Epoch 925: Training Loss 4.305404186248779, Validation Loss 4.305404186248779, Validation Accuracy 0.0000\n",
      "Epoch 926: Training Loss 4.299929618835449, Validation Loss 4.299929618835449, Validation Accuracy 0.0000\n",
      "Epoch 927: Training Loss 4.305981636047363, Validation Loss 4.305981636047363, Validation Accuracy 0.0000\n",
      "Epoch 928: Training Loss 4.310987949371338, Validation Loss 4.310987949371338, Validation Accuracy 0.0000\n",
      "Epoch 929: Training Loss 4.315907955169678, Validation Loss 4.315907955169678, Validation Accuracy 0.0000\n",
      "Epoch 930: Training Loss 4.321686267852783, Validation Loss 4.321686267852783, Validation Accuracy 0.0000\n",
      "Epoch 931: Training Loss 4.316097259521484, Validation Loss 4.316097259521484, Validation Accuracy 0.0000\n",
      "Epoch 932: Training Loss 4.321937561035156, Validation Loss 4.321937561035156, Validation Accuracy 0.0000\n",
      "Epoch 933: Training Loss 4.316460609436035, Validation Loss 4.316460609436035, Validation Accuracy 0.0000\n",
      "Epoch 934: Training Loss 4.3111724853515625, Validation Loss 4.3111724853515625, Validation Accuracy 0.0000\n",
      "Epoch 935: Training Loss 4.306065559387207, Validation Loss 4.306065559387207, Validation Accuracy 0.0000\n",
      "Epoch 936: Training Loss 4.311325550079346, Validation Loss 4.311325550079346, Validation Accuracy 0.0000\n",
      "Epoch 937: Training Loss 4.317424297332764, Validation Loss 4.317424297332764, Validation Accuracy 0.0000\n",
      "Epoch 938: Training Loss 4.322494983673096, Validation Loss 4.322494983673096, Validation Accuracy 0.0000\n",
      "Epoch 939: Training Loss 4.328400135040283, Validation Loss 4.328400135040283, Validation Accuracy 0.0000\n",
      "Epoch 940: Training Loss 4.323149681091309, Validation Loss 4.323149681091309, Validation Accuracy 0.0000\n",
      "Epoch 941: Training Loss 4.318078517913818, Validation Loss 4.318078517913818, Validation Accuracy 0.0000\n",
      "Epoch 942: Training Loss 4.323256969451904, Validation Loss 4.323256969451904, Validation Accuracy 0.0000\n",
      "Epoch 943: Training Loss 4.318282127380371, Validation Loss 4.318282127380371, Validation Accuracy 0.0000\n",
      "Epoch 944: Training Loss 4.324426174163818, Validation Loss 4.324426174163818, Validation Accuracy 0.0000\n",
      "Epoch 945: Training Loss 4.329555511474609, Validation Loss 4.329555511474609, Validation Accuracy 0.0000\n",
      "Epoch 946: Training Loss 4.324588775634766, Validation Loss 4.324588775634766, Validation Accuracy 0.0000\n",
      "Epoch 947: Training Loss 4.329766273498535, Validation Loss 4.329766273498535, Validation Accuracy 0.0000\n",
      "Epoch 948: Training Loss 4.334853172302246, Validation Loss 4.334853172302246, Validation Accuracy 0.0000\n",
      "Epoch 949: Training Loss 4.329905033111572, Validation Loss 4.329905033111572, Validation Accuracy 0.0000\n",
      "Epoch 950: Training Loss 4.3251214027404785, Validation Loss 4.3251214027404785, Validation Accuracy 0.0000\n",
      "Epoch 951: Training Loss 4.320501804351807, Validation Loss 4.320501804351807, Validation Accuracy 0.0000\n",
      "Epoch 952: Training Loss 4.326815128326416, Validation Loss 4.326815128326416, Validation Accuracy 0.0000\n",
      "Epoch 953: Training Loss 4.332112789154053, Validation Loss 4.332112789154053, Validation Accuracy 0.0000\n",
      "Epoch 954: Training Loss 4.338223934173584, Validation Loss 4.338223934173584, Validation Accuracy 0.0000\n",
      "Epoch 955: Training Loss 4.343331813812256, Validation Loss 4.343331813812256, Validation Accuracy 0.0000\n",
      "Epoch 956: Training Loss 4.34835147857666, Validation Loss 4.34835147857666, Validation Accuracy 0.0000\n",
      "Epoch 957: Training Loss 4.353285789489746, Validation Loss 4.353285789489746, Validation Accuracy 0.0000\n",
      "Epoch 958: Training Loss 4.359041690826416, Validation Loss 4.359041690826416, Validation Accuracy 0.0000\n",
      "Epoch 959: Training Loss 4.363807678222656, Validation Loss 4.363807678222656, Validation Accuracy 0.0000\n",
      "Epoch 960: Training Loss 4.358731269836426, Validation Loss 4.358731269836426, Validation Accuracy 0.0000\n",
      "Epoch 961: Training Loss 4.36445426940918, Validation Loss 4.36445426940918, Validation Accuracy 0.0000\n",
      "Epoch 962: Training Loss 4.359469890594482, Validation Loss 4.359469890594482, Validation Accuracy 0.0000\n",
      "Epoch 963: Training Loss 4.364343166351318, Validation Loss 4.364343166351318, Validation Accuracy 0.0000\n",
      "Epoch 964: Training Loss 4.369135856628418, Validation Loss 4.369135856628418, Validation Accuracy 0.0000\n",
      "Epoch 965: Training Loss 4.364182472229004, Validation Loss 4.364182472229004, Validation Accuracy 0.0000\n",
      "Epoch 966: Training Loss 4.369029521942139, Validation Loss 4.369029521942139, Validation Accuracy 0.0000\n",
      "Epoch 967: Training Loss 4.37468957901001, Validation Loss 4.37468957901001, Validation Accuracy 0.0000\n",
      "Epoch 968: Training Loss 4.379373073577881, Validation Loss 4.379373073577881, Validation Accuracy 0.0000\n",
      "Epoch 969: Training Loss 4.374378681182861, Validation Loss 4.374378681182861, Validation Accuracy 0.0000\n",
      "Epoch 970: Training Loss 4.369546413421631, Validation Loss 4.369546413421631, Validation Accuracy 0.0000\n",
      "Epoch 971: Training Loss 4.364871501922607, Validation Loss 4.364871501922607, Validation Accuracy 0.0000\n",
      "Epoch 972: Training Loss 4.370755672454834, Validation Loss 4.370755672454834, Validation Accuracy 0.0000\n",
      "Epoch 973: Training Loss 4.375660419464111, Validation Loss 4.375660419464111, Validation Accuracy 0.0000\n",
      "Epoch 974: Training Loss 4.380485534667969, Validation Loss 4.380485534667969, Validation Accuracy 0.0000\n",
      "Epoch 975: Training Loss 4.3757500648498535, Validation Loss 4.3757500648498535, Validation Accuracy 0.0000\n",
      "Epoch 976: Training Loss 4.381503105163574, Validation Loss 4.381503105163574, Validation Accuracy 0.0000\n",
      "Epoch 977: Training Loss 4.386290073394775, Validation Loss 4.386290073394775, Validation Accuracy 0.0000\n",
      "Epoch 978: Training Loss 4.391875743865967, Validation Loss 4.391875743865967, Validation Accuracy 0.0000\n",
      "Epoch 979: Training Loss 4.387081623077393, Validation Loss 4.387081623077393, Validation Accuracy 0.0000\n",
      "Epoch 980: Training Loss 4.391833782196045, Validation Loss 4.391833782196045, Validation Accuracy 0.0000\n",
      "Epoch 981: Training Loss 4.396510124206543, Validation Loss 4.396510124206543, Validation Accuracy 0.0000\n",
      "Epoch 982: Training Loss 4.401113510131836, Validation Loss 4.401113510131836, Validation Accuracy 0.0000\n",
      "Epoch 983: Training Loss 4.405643939971924, Validation Loss 4.405643939971924, Validation Accuracy 0.0000\n",
      "Epoch 984: Training Loss 4.410105228424072, Validation Loss 4.410105228424072, Validation Accuracy 0.0000\n",
      "Epoch 985: Training Loss 4.405163288116455, Validation Loss 4.405163288116455, Validation Accuracy 0.0000\n",
      "Epoch 986: Training Loss 4.400376796722412, Validation Loss 4.400376796722412, Validation Accuracy 0.0000\n",
      "Epoch 987: Training Loss 4.3957414627075195, Validation Loss 4.3957414627075195, Validation Accuracy 0.0000\n",
      "Epoch 988: Training Loss 4.391254425048828, Validation Loss 4.391254425048828, Validation Accuracy 0.0000\n",
      "Epoch 989: Training Loss 4.397009372711182, Validation Loss 4.397009372711182, Validation Accuracy 0.0000\n",
      "Epoch 990: Training Loss 4.4018049240112305, Validation Loss 4.4018049240112305, Validation Accuracy 0.0000\n",
      "Epoch 991: Training Loss 4.40652322769165, Validation Loss 4.40652322769165, Validation Accuracy 0.0000\n",
      "Epoch 992: Training Loss 4.401977062225342, Validation Loss 4.401977062225342, Validation Accuracy 0.0000\n",
      "Epoch 993: Training Loss 4.40673828125, Validation Loss 4.40673828125, Validation Accuracy 0.0000\n",
      "Epoch 994: Training Loss 4.412293910980225, Validation Loss 4.412293910980225, Validation Accuracy 0.0000\n",
      "Epoch 995: Training Loss 4.416900634765625, Validation Loss 4.416900634765625, Validation Accuracy 0.0000\n",
      "Epoch 996: Training Loss 4.421435356140137, Validation Loss 4.421435356140137, Validation Accuracy 0.0000\n",
      "Epoch 997: Training Loss 4.425901889801025, Validation Loss 4.425901889801025, Validation Accuracy 0.0000\n",
      "Epoch 998: Training Loss 4.42119026184082, Validation Loss 4.42119026184082, Validation Accuracy 0.0000\n",
      "Epoch 999: Training Loss 4.416625499725342, Validation Loss 4.416625499725342, Validation Accuracy 0.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# 定义数据集类\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "# 定义神经网络模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.n_h = n_h\n",
    "        self.linear1 = nn.Linear(n_x, n_h)\n",
    "        self.linear2 = nn.Linear(n_h, n_y)\n",
    "\n",
    "    def forward(self, X):\n",
    "        A1 = torch.sigmoid(self.linear1(X))\n",
    "        A2 = torch.sigmoid(self.linear2(A1))\n",
    "        return A2\n",
    "\n",
    "# 初始化模型\n",
    "def initialize_model(n_x, n_h, n_y):\n",
    "    model = SimpleNN(n_x, n_h, n_y)\n",
    "    model.apply(initialize_weights)  # 应用权重初始化\n",
    "    return model\n",
    "\n",
    "# 初始化权重的函数\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "# 计算损失\n",
    "def compute_loss(output, Y):\n",
    "    loss = F.binary_cross_entropy(output, Y)\n",
    "    return loss\n",
    "\n",
    "# 训练模型\n",
    "def train(model, train_loader, val_loader, learning_rate, epochs):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # 设置模型为训练模式\n",
    "        for X_batch, Y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = compute_loss(output, Y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 在每个epoch结束时进行验证\n",
    "        model.eval()  # 设置模型为评估模式\n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        with torch.no_grad():  # 在验证阶段不计算梯度\n",
    "            for X_batch, Y_batch in val_loader:\n",
    "                output = model(X_batch)\n",
    "                val_loss += compute_loss(output, Y_batch).item()\n",
    "                _, predictions = torch.max(output.data, 1)\n",
    "                correct_predictions += (predictions == Y_batch).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = correct_predictions / len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch}: Training Loss {val_loss}, Validation Loss {val_loss}, Validation Accuracy {accuracy:.4f}\")\n",
    "\n",
    "# 示例数据\n",
    "X = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float32)\n",
    "Y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# 创建数据集\n",
    "dataset = SimpleDataset(X, Y)\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 2  # 可以调整批量大小\n",
    "train_size = int(0.8 * len(dataset))  # 80%的数据用于训练\n",
    "val_size = len(dataset) - train_size  # 剩余20%的数据用于验证\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# 初始化模型\n",
    "n_x = X.shape[1]\n",
    "n_h = 2\n",
    "n_y = 1\n",
    "model = initialize_model(n_x, n_h, n_y)\n",
    "\n",
    "# 训练模型\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "train(model, train_loader, val_loader, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建两个函数：fit()用于训练模型，get_data()用于加载和预处理数据\n",
    "\n",
    "现在，我们有了两个独立的函数：get_data()和fit()。get_data()函数接受原始数据集的特征矩阵X和标签向量Y，然后创建并返回训练和验证数据加载器。fit()函数接受模型实例、训练和验证数据加载器、学习率和训练轮数，然后执行训练和验证过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# 定义数据集类\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "# 定义神经网络模型\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, n_x, n_h, n_y):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.n_h = n_h\n",
    "        self.linear1 = nn.Linear(n_x, n_h)\n",
    "        self.linear2 = nn.Linear(n_h, n_y)\n",
    "\n",
    "    def forward(self, X):\n",
    "        A1 = torch.sigmoid(self.linear1(X))\n",
    "        A2 = torch.sigmoid(self.linear2(A1))\n",
    "        return A2\n",
    "\n",
    "# 初始化权重的函数\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "# 计算损失\n",
    "def compute_loss(output, Y):\n",
    "    loss = F.binary_cross_entropy(output, Y)\n",
    "    return loss\n",
    "\n",
    "# 加载和预处理数据\n",
    "def get_data(X, Y):\n",
    "    dataset = SimpleDataset(X, Y)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    batch_size = 2\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# 训练模型\n",
    "def fit(model, train_loader, val_loader, learning_rate, epochs):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # 设置模型为训练模式\n",
    "        for X_batch, Y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = compute_loss(output, Y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 在每个epoch结束时进行验证\n",
    "        model.eval()  # 设置模型为评估模式\n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        with torch.no_grad():  # 在验证阶段不计算梯度\n",
    "            for X_batch, Y_batch in val_loader:\n",
    "                output = model(X_batch)\n",
    "                val_loss += compute_loss(output, Y_batch).item()\n",
    "                _, predictions = torch.max(output.data, 1)\n",
    "                correct_predictions += (predictions == Y_batch).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = correct_predictions / len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch}: Training Loss {val_loss}, Validation Loss {val_loss}, Validation Accuracy {accuracy:.4f}\")\n",
    "\n",
    "# 示例数据\n",
    "X = torch.tensor([[0,0], [0,1], [1,0], [1,1]], dtype=torch.float32)\n",
    "Y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# 初始化模型\n",
    "n_x = X.shape[1]\n",
    "n_h = 2\n",
    "n_y = 1\n",
    "model = SimpleNN(n_x, n_h, n_y)\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "# 加载数据\n",
    "train_loader, val_loader = get_data(X, Y)\n",
    "\n",
    "# 训练模型\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "fit(model, train_loader, val_loader, learning_rate, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn3.10",
   "language": "python",
   "name": "nn3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
